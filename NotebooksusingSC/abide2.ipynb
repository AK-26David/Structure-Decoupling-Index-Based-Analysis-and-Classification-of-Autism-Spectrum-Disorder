{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fabb84e1",
   "metadata": {},
   "source": [
    "# Classification of ABIDE 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85aace8",
   "metadata": {},
   "source": [
    "### To check size of connectomes and files ----> Check for each site individually by changing file paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6631f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "sc_dir = '/Users/arnavkarnik/Documents/Classification/SC_Connectomes_ABIDE2/BNI_1_connectomes'\n",
    "csv_files = [f for f in os.listdir(sc_dir) if f.endswith('.csv')]\n",
    "print(f\"Number of CSV files in SC directory: {len(csv_files)}\\n\")\n",
    "\n",
    "for filename in csv_files:\n",
    "    filepath = os.path.join(sc_dir, filename)\n",
    "    try:\n",
    "        matrix = np.loadtxt(filepath, delimiter=',')\n",
    "        print(f\"{filename}: shape = {matrix.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158ae82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "fmri_dir = '/Users/arnavkarnik/Documents/Classification/Time_Series_ABIDE2/bni_time_series/schaefer_400'\n",
    "\n",
    "csv_files = []\n",
    "for root, dirs, files in os.walk(fmri_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            csv_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Number of CSV files in fMRI directory (including subfolders): {len(csv_files)}\\n\")\n",
    "\n",
    "for filepath in csv_files:\n",
    "    try:\n",
    "        matrix = np.loadtxt(filepath, delimiter=',')\n",
    "        print(f\"{os.path.basename(filepath)}: shape = {matrix.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read {os.path.basename(filepath)}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc15007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "sc_dir = '/Users/arnavkarnik/Documents/Classification/SC_Connectomes_ABIDE2/BNI_1_connectomes'\n",
    "fmri_dir = '/Users/arnavkarnik/Documents/Classification/Time_Series_ABIDE2/bni_time_series/schaefer_400'\n",
    "\n",
    "# Get patient IDs from SC files (remove extension)\n",
    "sc_files = [f for f in os.listdir(sc_dir) if f.endswith('.csv')]\n",
    "sc_ids = set([os.path.splitext(f)[0] for f in sc_files])\n",
    "\n",
    "# Get patient IDs from fMRI files recursively (remove extension)\n",
    "fmri_ids = set()\n",
    "for root, dirs, files in os.walk(fmri_dir):\n",
    "    for f in files:\n",
    "        if f.endswith('.csv'):\n",
    "            fmri_ids.add(os.path.splitext(f)[0])\n",
    "\n",
    "missing_in_fmri = sc_ids - fmri_ids\n",
    "missing_in_sc = fmri_ids - sc_ids\n",
    "\n",
    "print(f\"Patients missing in fMRI data: {missing_in_fmri}\")\n",
    "print(f\"Patients missing in SC data: {missing_in_sc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ee6b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "sc_dir = '/Users/arnavkarnik/Documents/Classification/SC_Connectomes_ABIDE2/BNI_1_connectomes'\n",
    "fmri_dir = '/Users/arnavkarnik/Documents/Classification/Time_Series_ABIDE2/bni_time_series/schaefer_400'\n",
    "\n",
    "def extract_id_sc(filename):\n",
    "    # Example: '29006_parcels.csv' -> '29006'\n",
    "    return filename.split('_')[0]\n",
    "\n",
    "def extract_id_fmri(filename):\n",
    "    # Example: 'sub-29006_ses-1_task-rest_cleaned-1_bold.csv' -> '29006'\n",
    "    match = re.search(r'sub-(\\d+)', filename)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "sc_files = [f for f in os.listdir(sc_dir) if f.endswith('.csv')]\n",
    "sc_ids = set(extract_id_sc(f) for f in sc_files)\n",
    "\n",
    "fmri_ids = set()\n",
    "for root, dirs, files in os.walk(fmri_dir):\n",
    "    for f in files:\n",
    "        if f.endswith('.csv'):\n",
    "            pid = extract_id_fmri(f)\n",
    "            if pid:\n",
    "                fmri_ids.add(pid)\n",
    "\n",
    "missing_in_fmri = sc_ids - fmri_ids\n",
    "missing_in_sc = fmri_ids - sc_ids\n",
    "\n",
    "print(f\"Patients missing in fMRI data: {missing_in_fmri}\")\n",
    "print(f\"Patients missing in SC data: {missing_in_sc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a2a394",
   "metadata": {},
   "source": [
    "### SDI Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ace2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.linalg import eigh\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def compute_structural_laplacian(A):\n",
    "    D = np.diag(np.sum(A, axis=1))\n",
    "    with np.errstate(divide='ignore'):\n",
    "        D_inv_sqrt = np.diag(1.0 / np.sqrt(np.sum(A, axis=1)))\n",
    "    D_inv_sqrt[np.isinf(D_inv_sqrt)] = 0\n",
    "    L = np.eye(A.shape[0]) - D_inv_sqrt @ A @ D_inv_sqrt\n",
    "    return L\n",
    "\n",
    "def graph_spectral_phase_randomize(X, eigvecs, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    X_hat = X @ eigvecs\n",
    "    T, N = X_hat.shape\n",
    "    X_surr = np.zeros_like(X_hat)\n",
    "\n",
    "    for i in range(N):\n",
    "        fft_coeff = np.fft.fft(X_hat[:, i])\n",
    "        mag = np.abs(fft_coeff)\n",
    "        phase = np.angle(fft_coeff)\n",
    "        num_phases = len(fft_coeff)\n",
    "\n",
    "        random_phases = np.random.uniform(0, 2*np.pi, num_phases // 2 - 1)\n",
    "        new_phase = np.copy(phase)\n",
    "        new_phase[1:num_phases//2] = random_phases\n",
    "        new_phase[-(num_phases//2)+1:] = -random_phases[::-1]\n",
    "\n",
    "        new_fft = mag * np.exp(1j * new_phase)\n",
    "        X_surr[:, i] = np.fft.ifft(new_fft).real\n",
    "\n",
    "    return X_surr @ eigvecs.T\n",
    "\n",
    "def compute_SDI_informed_energy_split(A, X, pid, num_surrogates=100, seed=None):\n",
    "    X = X - X.mean(axis=0)\n",
    "    X = X / (X.std(axis=0) + 1e-10)\n",
    "\n",
    "    T, N = X.shape\n",
    "    L = compute_structural_laplacian(A)\n",
    "    eigvals, eigvecs = eigh(L)\n",
    "\n",
    "    energy = np.sum((X @ eigvecs)**2, axis=0)\n",
    "    total_energy = np.sum(energy)\n",
    "    cum_energy = np.cumsum(energy)\n",
    "    cutoff_index = np.searchsorted(cum_energy, 0.5 * total_energy)\n",
    "    if cutoff_index <= 0 or cutoff_index >= N:\n",
    "        cutoff_index = N // 2\n",
    "\n",
    "    Vlow, Vhigh = eigvecs[:, :cutoff_index], eigvecs[:, cutoff_index:]\n",
    "\n",
    "    N_c_surr, N_d_surr = np.empty((N, num_surrogates)), np.empty((N, num_surrogates))\n",
    "\n",
    "    for s in range(num_surrogates):\n",
    "        X_surr = graph_spectral_phase_randomize(X, eigvecs, seed=seed+s if seed is not None else None)\n",
    "        X_hat = X_surr @ eigvecs\n",
    "\n",
    "        X_c = X_hat[:, :cutoff_index] @ Vlow.T\n",
    "        X_d = X_hat[:, cutoff_index:] @ Vhigh.T\n",
    "\n",
    "        for r in range(N):\n",
    "            N_c_surr[r, s] = np.linalg.norm(X_c[:, r])\n",
    "            N_d_surr[r, s] = np.linalg.norm(X_d[:, r])\n",
    "\n",
    "    SDI = N_d_surr / (N_c_surr + 1e-10)\n",
    "    mean_SDI = np.mean(SDI, axis=1)\n",
    "    print(f\"Patient {pid}: SDI mean={mean_SDI.mean():.4f}, cutoff={cutoff_index}\")\n",
    "    return mean_SDI, cutoff_index\n",
    "\n",
    "def extract_patient_id_structural(filename):\n",
    "    match = re.match(r'(\\d+)_parcels', filename)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def extract_patient_id_functional(filepath):\n",
    "    filename = os.path.basename(filepath)\n",
    "    match = re.search(r'sub-(\\d+)_', filename)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def get_structural_files_map(structural_dir):\n",
    "    files_map = {}\n",
    "    for f in os.listdir(structural_dir):\n",
    "        if f.endswith('.csv'):\n",
    "            pid = extract_patient_id_structural(f)\n",
    "            if pid:\n",
    "                files_map[pid] = os.path.join(structural_dir, f)\n",
    "    return files_map\n",
    "\n",
    "def get_functional_files_map(functional_dir):\n",
    "    files_map = {}\n",
    "    for root, _, files in os.walk(functional_dir):\n",
    "        for f in files:\n",
    "            if f.endswith('.csv'):\n",
    "                full_path = os.path.join(root, f)\n",
    "                pid = extract_patient_id_functional(full_path)\n",
    "                if pid:\n",
    "                    files_map[pid] = full_path\n",
    "    return files_map\n",
    "\n",
    "def plot_fmri_sc(X, A, pid):\n",
    "    fc_mat = np.corrcoef(X.T)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    sns.heatmap(fc_mat, ax=axes[0], cmap='coolwarm', center=0, square=True, cbar=True)\n",
    "    axes[0].set_title(f'Patient {pid}: fMRI Functional Connectivity')\n",
    "\n",
    "    sns.heatmap(A, ax=axes[1], cmap='coolwarm', center=0, square=True, cbar=True)\n",
    "    axes[1].set_title(f'Patient {pid}: Structural Connectivity')\n",
    "\n",
    "    diff = fc_mat - A\n",
    "    sns.heatmap(diff, ax=axes[2], cmap='bwr', center=0, square=True, cbar=True)\n",
    "    axes[2].set_title(f'Patient {pid}: Difference (FC - SC)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def main(functional_dir, structural_dir, output_csv_path, num_surrogates=100, seed=42):\n",
    "    func_files = get_functional_files_map(functional_dir)\n",
    "    struct_files = get_structural_files_map(structural_dir)\n",
    "\n",
    "    common_patients = set(func_files.keys()) & set(struct_files.keys())\n",
    "    print(f\"Found {len(common_patients)} patients with matching data.\")\n",
    "\n",
    "    results = []\n",
    "    cutoff_indices = []\n",
    "    expected_nodes = 400\n",
    "\n",
    "    for pid in sorted(common_patients):\n",
    "        func_path = func_files[pid]\n",
    "        struct_path = struct_files[pid]\n",
    "\n",
    "        try:\n",
    "            X = np.loadtxt(func_path, delimiter=',')\n",
    "            A = np.loadtxt(struct_path, delimiter=',')\n",
    "\n",
    "            if X.size == 0 or A.size == 0:\n",
    "                print(f\"Skipping {pid}: empty data\")\n",
    "                continue\n",
    "\n",
    "            if len(A.shape) != 2 or A.shape[0] != A.shape[1]:\n",
    "                print(f\"Skipping {pid}: Structural matrix not square {A.shape}\")\n",
    "                continue\n",
    "\n",
    "            if len(X.shape) != 2:\n",
    "                print(f\"Skipping {pid}: Functional data not 2D {X.shape}\")\n",
    "                continue\n",
    "\n",
    "            T, N_f = X.shape\n",
    "            N_s = A.shape[0]\n",
    "\n",
    "            if N_f != N_s:\n",
    "                print(f\"Skipping {pid}: functional nodes != structural nodes ({N_f} vs {N_s})\")\n",
    "                continue\n",
    "\n",
    "            if N_f != expected_nodes:\n",
    "                print(f\"Skipping {pid}: expected {expected_nodes} nodes, got {N_f}\")\n",
    "                continue\n",
    "\n",
    "            A = A / (np.max(A) + 1e-10)\n",
    "            plot_fmri_sc(X, A, pid)\n",
    "\n",
    "            sdi, cutoff_index = compute_SDI_informed_energy_split(A, X, pid, num_surrogates=num_surrogates, seed=seed)\n",
    "\n",
    "            # Log transform and normalize to (0,1)\n",
    "            sdi = np.log2(sdi + 1e-10)\n",
    "            sdi_min, sdi_max = sdi.min(), sdi.max()\n",
    "            sdi = (sdi - sdi_min) / (sdi_max - sdi_min + 1e-10)\n",
    "\n",
    "            if len(sdi) != expected_nodes:\n",
    "                print(f\"Skipping {pid}: SDI length is {len(sdi)}, expected {expected_nodes}.\")\n",
    "                continue\n",
    "\n",
    "            results.append((pid, sdi))\n",
    "            cutoff_indices.append(cutoff_index)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pid}: {e}\")\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)\n",
    "    with open(output_csv_path, 'w', newline='') as f_out:\n",
    "        writer = csv.writer(f_out)\n",
    "        header = ['PatientID'] + [f'SDI_Node_{i+1}' for i in range(expected_nodes)]\n",
    "        writer.writerow(header)\n",
    "\n",
    "        for pid, sdi in results:\n",
    "            writer.writerow([pid] + list(sdi))\n",
    "\n",
    "    print(f\"Saved all SDI results to {output_csv_path}\")\n",
    "\n",
    "    # Plot histogram of energy cutoff indices\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.histplot(cutoff_indices, bins=20, kde=False)\n",
    "    plt.xlabel('Energy-Based Cutoff Index')\n",
    "    plt.ylabel('Number of Subjects')\n",
    "    plt.title('Distribution of Graph Frequency Split Points')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sites_config = [\n",
    "        {\n",
    "            \"site\": \"bni\",\n",
    "            \"functional_dir\": \"/Users/arnavkarnik/Documents/Classification/Time_Series_ABIDE2/bni_time_series/schaefer_400/cleaned-1\",\n",
    "            \"structural_dir\": \"/Users/arnavkarnik/Documents/Classification/SC_Connectomes_ABIDE2/BNI_1_connectomes\",\n",
    "            \"output_csv_path\": \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_bni.csv\"\n",
    "        },\n",
    "        {\n",
    "            \"site\": \"ip\",\n",
    "            \"functional_dir\": \"/Users/arnavkarnik/Documents/Classification/Time_Series_ABIDE2/ip_time_series/schaefer_400/cleaned-1\",\n",
    "            \"structural_dir\": \"/Users/arnavkarnik/Documents/Classification/SC_Connectomes_ABIDE2/IP_1_connectomes\",\n",
    "            \"output_csv_path\": \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_ip.csv\"\n",
    "        },\n",
    "        {\n",
    "            \"site\": \"nyu1\",\n",
    "            \"functional_dir\": \"/Users/arnavkarnik/Documents/Classification/Time_Series_ABIDE2/nyu1_time_series/schaefer_400/cleaned-1\",\n",
    "            \"structural_dir\": \"/Users/arnavkarnik/Documents/Classification/SC_Connectomes_ABIDE2/NYU_1_connectomes\",\n",
    "            \"output_csv_path\": \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_nyu1.csv\"\n",
    "        },\n",
    "        {\n",
    "            \"site\": \"nyu2\",\n",
    "            \"functional_dir\": \"/Users/arnavkarnik/Documents/Classification/Time_Series_ABIDE2/nyu2_time_series/schaefer_400/cleaned-1\",\n",
    "            \"structural_dir\": \"/Users/arnavkarnik/Documents/Classification/SC_Connectomes_ABIDE2/NYU_2_connectomes\",\n",
    "            \"output_csv_path\": \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_nyu2.csv\"\n",
    "        },\n",
    "        {\n",
    "            \"site\": \"sdsu\",\n",
    "            \"functional_dir\": \"/Users/arnavkarnik/Documents/Classification/Time_Series_ABIDE2/sdsu_time_series/schaefer_400/cleaned-1\",\n",
    "            \"structural_dir\": \"/Users/arnavkarnik/Documents/Classification/SC_Connectomes_ABIDE2/SDSU_1_connectomes\",\n",
    "            \"output_csv_path\": \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_sdsu.csv\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    for config in sites_config:\n",
    "        print(f\"\\n=== Processing site: {config['site'].upper()} ===\")\n",
    "        main(\n",
    "            functional_dir=config[\"functional_dir\"],\n",
    "            structural_dir=config[\"structural_dir\"],\n",
    "            output_csv_path=config[\"output_csv_path\"],\n",
    "            num_surrogates=100,\n",
    "            seed=42\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52266928",
   "metadata": {},
   "source": [
    "### Visualization of SDI data for each individual site ---> Plug in file path to see "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe6d1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "\n",
    "# Path to your SDI results CSV\n",
    "csv_path = \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_bni.csv\"\n",
    "\n",
    "# Load SDI data from CSV (skip header)\n",
    "patient_ids = []\n",
    "sdi_data = []\n",
    "\n",
    "with open(csv_path, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)  # Skip header\n",
    "    for row in reader:\n",
    "        patient_ids.append(row[0])\n",
    "        sdi_vals = [float(x) if x != '' else np.nan for x in row[1:]]\n",
    "        sdi_data.append(sdi_vals)\n",
    "\n",
    "sdi_array = np.array(sdi_data)  # shape: (num_patients, num_nodes)\n",
    "\n",
    "# Replace empty/nan with np.nan explicitly\n",
    "sdi_array = np.where(np.isnan(sdi_array), np.nan, sdi_array)\n",
    "\n",
    "# Plot 1: Histogram of all SDI values across all nodes and patients\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.hist(sdi_array[~np.isnan(sdi_array)].flatten(), bins=90, color='c', alpha=0.7)\n",
    "plt.title(\"Histogram of all SDI values across all nodes and patients\")\n",
    "plt.xlabel(\"SDI value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Mean ± Std Dev of SDI per brain node across patients\n",
    "mean_sdi = np.nanmean(sdi_array, axis=0)\n",
    "std_sdi = np.nanstd(sdi_array, axis=0)\n",
    "nodes = np.arange(1, len(mean_sdi)+1)\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.errorbar(nodes, mean_sdi, yerr=std_sdi, fmt='-o', ecolor='r', capsize=5)\n",
    "plt.title(\"Mean ± Std Dev of SDI per brain node across patients\")\n",
    "plt.xlabel(\"Brain node\")\n",
    "plt.ylabel(\"SDI\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Distribution of total SDI per patient\n",
    "total_sdi_per_patient = np.nansum(sdi_array, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(total_sdi_per_patient, bins=30, color='m', alpha=0.7)\n",
    "plt.title(\"Distribution of total SDI scores across patients\")\n",
    "plt.xlabel(\"Total SDI\")\n",
    "plt.ylabel(\"Number of patients\")\n",
    "plt.show()\n",
    "\n",
    "# Plot 4: Heatmap of SDI for first 5 patients (or less if fewer patients)\n",
    "# Show only first 5 patients and first 50 nodes\n",
    "num_patients_to_show = min(5, sdi_array.shape[0])\n",
    "num_nodes_to_show = min(50, sdi_array.shape[1])\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "sns.heatmap(\n",
    "    sdi_array[:num_patients_to_show, :num_nodes_to_show],\n",
    "    cmap='viridis',\n",
    "    xticklabels=np.arange(1, num_nodes_to_show + 1),\n",
    "    yticklabels=patient_ids[:num_patients_to_show]\n",
    ")\n",
    "plt.title(\"Zoomed-in SDI heatmap (first 5 patients × first 50 brain nodes)\")\n",
    "plt.xlabel(\"Brain node\")\n",
    "plt.ylabel(\"Patient ID\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "start_node = 50\n",
    "end_node = 150\n",
    "num_patients_to_show = min(5, sdi_array.shape[0])\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "sns.heatmap(\n",
    "    sdi_array[:num_patients_to_show, start_node:end_node],\n",
    "    cmap='viridis',\n",
    "    xticklabels=np.arange(start_node + 1, end_node + 1),\n",
    "    yticklabels=patient_ids[:num_patients_to_show]\n",
    ")\n",
    "plt.title(f\"Zoomed-in SDI heatmap (first 5 patients × nodes {start_node+1}-{end_node})\")\n",
    "plt.xlabel(\"Brain node\")\n",
    "plt.ylabel(\"Patient ID\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9778b55",
   "metadata": {},
   "source": [
    "### Visualization of SDI data for all sites together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c5ffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# List of all CSV paths\n",
    "csv_paths = [\n",
    "    \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_bni.csv\",\n",
    "    \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_ip.csv\",\n",
    "    \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_nyu1.csv\",\n",
    "    \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_nyu2.csv\",\n",
    "    \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_sdsu.csv\"\n",
    "]\n",
    "\n",
    "all_patient_ids = []\n",
    "all_sdi_data = []\n",
    "\n",
    "# Load data from all CSVs\n",
    "for path in csv_paths:\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"File not found: {path}\")\n",
    "        continue\n",
    "    with open(path, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)\n",
    "        for row in reader:\n",
    "            all_patient_ids.append(row[0])\n",
    "            sdi_vals = [float(x) if x != '' else np.nan for x in row[1:]]\n",
    "            all_sdi_data.append(sdi_vals)\n",
    "\n",
    "sdi_array = np.array(all_sdi_data)  # shape: (num_patients, num_nodes)\n",
    "\n",
    "# Replace nan explicitly\n",
    "sdi_array = np.where(np.isnan(sdi_array), np.nan, sdi_array)\n",
    "\n",
    "# --- Plot 1: Histogram of all SDI values ---\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.hist(sdi_array[~np.isnan(sdi_array)].flatten(), bins=90, color='c', alpha=0.7)\n",
    "plt.title(\"Histogram of all SDI values across all ABIDE II patients and nodes\")\n",
    "plt.xlabel(\"SDI value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Plot 2: Mean ± Std Dev per brain node ---\n",
    "mean_sdi = np.nanmean(sdi_array, axis=0)\n",
    "std_sdi = np.nanstd(sdi_array, axis=0)\n",
    "nodes = np.arange(1, len(mean_sdi)+1)\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.errorbar(nodes, mean_sdi, yerr=std_sdi, fmt='-o', ecolor='r', capsize=5)\n",
    "plt.title(\"Mean ± Std Dev of SDI per brain node across ABIDE II patients\")\n",
    "plt.xlabel(\"Brain node\")\n",
    "plt.ylabel(\"SDI\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Plot 3: Distribution of total SDI per patient ---\n",
    "total_sdi_per_patient = np.nansum(sdi_array, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(total_sdi_per_patient, bins=30, color='m', alpha=0.7)\n",
    "plt.title(\"Distribution of total SDI scores across ABIDE II patients\")\n",
    "plt.xlabel(\"Total SDI\")\n",
    "plt.ylabel(\"Number of patients\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Plot 4a: Zoomed-in heatmap (first 5 patients × first 50 brain nodes) ---\n",
    "num_patients_to_show = min(5, sdi_array.shape[0])\n",
    "num_nodes_to_show = min(50, sdi_array.shape[1])\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "sns.heatmap(\n",
    "    sdi_array[:num_patients_to_show, :num_nodes_to_show],\n",
    "    cmap='viridis',\n",
    "    xticklabels=np.arange(1, num_nodes_to_show + 1),\n",
    "    yticklabels=all_patient_ids[:num_patients_to_show]\n",
    ")\n",
    "plt.title(\"Zoomed-in SDI heatmap (first 5 patients × first 50 brain nodes)\")\n",
    "plt.xlabel(\"Brain node\")\n",
    "plt.ylabel(\"Patient ID\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Plot 4b: Zoomed-in heatmap (first 5 patients × nodes 51–150) ---\n",
    "start_node = 50\n",
    "end_node = 150\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "sns.heatmap(\n",
    "    sdi_array[:num_patients_to_show, start_node:end_node],\n",
    "    cmap='viridis',\n",
    "    xticklabels=np.arange(start_node + 1, end_node + 1),\n",
    "    yticklabels=all_patient_ids[:num_patients_to_show]\n",
    ")\n",
    "plt.title(f\"Zoomed-in SDI heatmap (first 5 patients × nodes {start_node+1}-{end_node})\")\n",
    "plt.xlabel(\"Brain node\")\n",
    "plt.ylabel(\"Patient ID\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d6a415",
   "metadata": {},
   "source": [
    "### Visualization of Brain Atlas for each patient in individual site using Schaffer 400 ----> Keep changing patient number and site "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad0a089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn import plotting, datasets\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Load SDI from CSV\n",
    "# -----------------------------\n",
    "csv_path = \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_bni.csv\"\n",
    "patient_id = \"29006\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "df[\"PatientID\"] = df[\"PatientID\"].astype(str)\n",
    "row_match = df[df[\"PatientID\"] == patient_id]\n",
    "\n",
    "if row_match.empty:\n",
    "    raise ValueError(f\"Patient ID {patient_id} not found in CSV.\")\n",
    "\n",
    "sdi_row = row_match.iloc[0]\n",
    "sdi_values = sdi_row.iloc[1:401].to_numpy(dtype=float)  # 400 parcels\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Load Schaefer 400 atlas volume\n",
    "# -----------------------------\n",
    "atlas_path = \"/Users/arnavkarnik/Documents/Classification/Schaefer2018_400Parcels_7Networks_order_FSLMNI152_1mm.nii\"\n",
    "atlas_img = nib.load(atlas_path)\n",
    "atlas_data = atlas_img.get_fdata()\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Build SDI volume\n",
    "# -----------------------------\n",
    "sdi_volume = np.zeros_like(atlas_data)\n",
    "\n",
    "for i in range(400):\n",
    "    region_label = i + 1  # Labels are from 1 to 400\n",
    "    sdi_volume[atlas_data == region_label] = sdi_values[i]\n",
    "\n",
    "sdi_img = nib.Nifti1Image(sdi_volume, affine=atlas_img.affine)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Plot on MNI template\n",
    "# -----------------------------\n",
    "template = datasets.load_mni152_template()\n",
    "\n",
    "display = plotting.plot_stat_map(\n",
    "    sdi_img,\n",
    "    bg_img=template,\n",
    "    title=f\"SDI Map - Patient {patient_id}\",\n",
    "    display_mode=\"ortho\",\n",
    "    threshold=np.percentile(sdi_values, 20),  # show top 80%\n",
    "    cmap=\"viridis\",\n",
    ")\n",
    "plotting.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8686dfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "# -------- Step 1: Load SDI values from CSV --------\n",
    "csv_path = \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_bni.csv\"\n",
    "patient_id = \"29006\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "df[\"PatientID\"] = df[\"PatientID\"].astype(str)\n",
    "row = df[df[\"PatientID\"] == patient_id]\n",
    "\n",
    "if row.empty:\n",
    "    raise ValueError(f\"Patient {patient_id} not found in CSV\")\n",
    "\n",
    "sdi_values = row.iloc[0, 1:401].to_numpy(dtype=float)  # 400 regions\n",
    "\n",
    "# -------- Step 2: Load the Schaefer400 atlas --------\n",
    "atlas_path = \"/Users/arnavkarnik/Documents/Classification/Schaefer2018_400Parcels_7Networks_order_FSLMNI152_1mm.nii\"\n",
    "atlas_img = nib.load(atlas_path)\n",
    "atlas_data = atlas_img.get_fdata().astype(int)\n",
    "\n",
    "# -------- Step 3: Identify top N coupled/decoupled regions --------\n",
    "N = 10\n",
    "\n",
    "# Indices of highest/lowest SDI values\n",
    "top_indices = np.argsort(sdi_values)[-N:][::-1]     # top N\n",
    "bottom_indices = np.argsort(sdi_values)[:N]         # bottom N\n",
    "\n",
    "print(\"Top Coupled Regions (High SDI):\")\n",
    "for i in top_indices:\n",
    "    print(f\"Region {i+1} — SDI: {sdi_values[i]:.4f}\")\n",
    "\n",
    "print(\"\\nTop Decoupled Regions (Low SDI):\")\n",
    "for i in bottom_indices:\n",
    "    print(f\"Region {i+1} — SDI: {sdi_values[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c6fb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "\n",
    "# Fetch the Schaefer 2018 atlas with 400 regions and 7-network solution\n",
    "schaefer = datasets.fetch_atlas_schaefer_2018(n_rois=400, yeo_networks=7, resolution_mm=1)\n",
    "\n",
    "# Extract the labels\n",
    "region_labels = schaefer['labels']  # This is a list of region names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6203e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "# Build Schaefer400 parcel names dictionary by network and parcel number\n",
    "network_names = [\n",
    "    (\"Vis\", 75),         # Visual\n",
    "    (\"SomMot\", 60),      # Somatomotor\n",
    "    (\"DorsAttn\", 52),    # Dorsal Attention\n",
    "    (\"SalVentAttn\", 50), # Ventral Attention\n",
    "    (\"Limbic\", 25),      # Limbic\n",
    "    (\"Cont\", 58),        # Frontoparietal Control\n",
    "    (\"Default\", 80)      # Default Mode\n",
    "]\n",
    "\n",
    "parcel_names = {}\n",
    "current_index = 1\n",
    "for network, count in network_names:\n",
    "    for i in range(1, count + 1):\n",
    "        parcel_names[current_index] = f\"{network}_{i}\"\n",
    "        current_index += 1\n",
    "\n",
    "# === New code to verify and save all 400 parcel names ===\n",
    "print(f\"Total parcels: {len(parcel_names)}\")\n",
    "\n",
    "for i in range(1, 401):\n",
    "    print(f\"Region {i}: {parcel_names[i]}\")\n",
    "\n",
    "with open(\"schaefer400_parcel_names.txt\", \"w\") as f:\n",
    "    for i in range(1, 401):\n",
    "        f.write(f\"Region {i}: {parcel_names[i]}\\n\")\n",
    "\n",
    "print(\"Saved all parcel names to schaefer400_parcel_names.txt\")\n",
    "\n",
    "# Load SDI values\n",
    "csv_path = \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_bni.csv\"\n",
    "patient_id = \"29006\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "df[\"PatientID\"] = df[\"PatientID\"].astype(str)\n",
    "row = df[df[\"PatientID\"] == patient_id]\n",
    "\n",
    "if row.empty:\n",
    "    raise ValueError(f\"Patient {patient_id} not found in CSV\")\n",
    "\n",
    "sdi_values = row.iloc[0, 1:401].to_numpy(dtype=float)  # 400 regions\n",
    "\n",
    "# Load Schaefer400 atlas (not used directly here but useful if needed)\n",
    "atlas_path = \"/Users/arnavkarnik/Documents/Classification/Schaefer2018_400Parcels_7Networks_order_FSLMNI152_1mm.nii\"\n",
    "atlas_img = nib.load(atlas_path)\n",
    "atlas_data = atlas_img.get_fdata().astype(int)\n",
    "\n",
    "N = 10\n",
    "top_indices = np.argsort(sdi_values)[-N:][::-1]  # Top 10\n",
    "bottom_indices = np.argsort(sdi_values)[:N]      # Bottom 10\n",
    "\n",
    "print(\"Top Coupled Regions (High SDI):\")\n",
    "for i in top_indices:\n",
    "    region_index = i + 1  # Convert zero-based to 1-based indexing\n",
    "    region_name = parcel_names.get(region_index, f\"Region_{region_index}\")\n",
    "    print(f\"{region_name} (Region {region_index}) — SDI: {sdi_values[i]:.4f}\")\n",
    "\n",
    "print(\"\\nTop Decoupled Regions (Low SDI):\")\n",
    "for i in bottom_indices:\n",
    "    region_index = i + 1\n",
    "    region_name = parcel_names.get(region_index, f\"Region_{region_index}\")\n",
    "    print(f\"{region_name} (Region {region_index}) — SDI: {sdi_values[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dad0854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn import plotting\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Paths\n",
    "atlas_path = \"/Users/arnavkarnik/Documents/Classification/Schaefer2018_400Parcels_7Networks_order_FSLMNI152_1mm.nii\"\n",
    "\n",
    "# Your top coupled/decoupled region indices (1-based)\n",
    "top_coupled = [90, 70, 373, 79, 133, 60, 255, 239, 107, 154]\n",
    "top_decoupled = [15, 168, 27, 399, 254, 249, 252, 217, 128, 18]\n",
    "\n",
    "# Load atlas\n",
    "atlas_img = nib.load(atlas_path)\n",
    "atlas_data = atlas_img.get_fdata()\n",
    "\n",
    "# Create masks for top coupled and decoupled parcels\n",
    "top_coupled_mask = np.isin(atlas_data, top_coupled).astype(int)\n",
    "top_decoupled_mask = np.isin(atlas_data, top_decoupled).astype(int)\n",
    "\n",
    "# Combine masks: 0=background, 1=top coupled, 2=top decoupled\n",
    "combined_mask = top_coupled_mask + (top_decoupled_mask * 2)\n",
    "\n",
    "# Convert to uint8 to avoid nibabel dtype error\n",
    "combined_img = nib.Nifti1Image(combined_mask.astype(np.uint8), affine=atlas_img.affine)\n",
    "\n",
    "# Plotting combined mask\n",
    "display = plotting.plot_roi(\n",
    "    combined_img,\n",
    "    title=\"Top Coupled (red) & Decoupled (blue) Regions\",\n",
    "    cmap=plt.cm.get_cmap('coolwarm', 3),\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "# Add legend for color meaning\n",
    "red_patch = mpatches.Patch(color='red', label='Top Coupled')\n",
    "blue_patch = mpatches.Patch(color='blue', label='Top Decoupled')\n",
    "plt.legend(handles=[red_patch, blue_patch], loc='lower left')\n",
    "\n",
    "plotting.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d589d1",
   "metadata": {},
   "source": [
    "### Visualization of Brain Atlas for all patients in all sites using Schaffer 400 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c54e505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn import plotting, datasets\n",
    "import os\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: File Paths\n",
    "# -----------------------------\n",
    "atlas_path = \"/Users/arnavkarnik/Documents/Classification/Schaefer2018_400Parcels_7Networks_order_FSLMNI152_1mm.nii\"\n",
    "template = datasets.load_mni152_template()\n",
    "\n",
    "site_csvs = {\n",
    "    \"BNI\": \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_bni.csv\",\n",
    "    \"IP\": \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_ip.csv\",\n",
    "    \"NYU1\": \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_nyu1.csv\",\n",
    "    \"NYU2\": \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_nyu2.csv\",\n",
    "    \"SDSU\": \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_sdsu.csv\",\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Aggregate SDI from All Sites\n",
    "# -----------------------------\n",
    "all_sdi = []\n",
    "\n",
    "for site_name, csv_path in site_csvs.items():\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if df.empty:\n",
    "        continue\n",
    "    sdi_values = df.iloc[:, 1:401].to_numpy(dtype=float)\n",
    "    all_sdi.append(sdi_values)\n",
    "\n",
    "# Concatenate all patients\n",
    "all_sdi_array = np.vstack(all_sdi)  # shape: (total_patients, 400)\n",
    "mean_sdi = np.nanmean(all_sdi_array, axis=0)  # shape: (400,)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Load Atlas and Create Volume\n",
    "# -----------------------------\n",
    "atlas_img = nib.load(atlas_path)\n",
    "atlas_data = atlas_img.get_fdata()\n",
    "sdi_volume = np.zeros_like(atlas_data)\n",
    "\n",
    "for i in range(400):\n",
    "    region_label = i + 1\n",
    "    sdi_volume[atlas_data == region_label] = mean_sdi[i]\n",
    "\n",
    "sdi_img = nib.Nifti1Image(sdi_volume, affine=atlas_img.affine)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Plot the Common SDI Atlas\n",
    "# -----------------------------\n",
    "plotting.plot_stat_map(\n",
    "    sdi_img,\n",
    "    bg_img=template,\n",
    "    title=\"Mean SDI Atlas (All ABIDE II Sites)\",\n",
    "    display_mode=\"ortho\",\n",
    "    threshold=np.percentile(mean_sdi, 20),\n",
    "    cmap=\"viridis\"\n",
    ")\n",
    "plotting.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842cf072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- List all your ABIDE II SDI CSVs across sites here ---\n",
    "csv_files = [\n",
    "    \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_ip.csv\",\n",
    "    \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_nyu1.csv\",\n",
    "    \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_nyu2.csv\",\n",
    "    \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_sdsu.csv\",\n",
    "    \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_bni.csv\"\n",
    "]\n",
    "\n",
    "all_sdi = []\n",
    "\n",
    "# --- Aggregate SDI data from all patients ---\n",
    "for csv_path in csv_files:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    sdi_data = df.iloc[:, 1:401].to_numpy(dtype=float)  # Only SDI columns\n",
    "    all_sdi.append(sdi_data)\n",
    "\n",
    "# Combine all patients across all sites\n",
    "sdi_matrix = np.vstack(all_sdi)  # Shape: (total_patients, 400)\n",
    "\n",
    "# --- Compute average SDI across patients for each region ---\n",
    "mean_sdi_per_region = np.nanmean(sdi_matrix, axis=0)  # shape (400,)\n",
    "\n",
    "# --- Identify top/bottom N regions ---\n",
    "N = 10\n",
    "top_indices = np.argsort(mean_sdi_per_region)[-N:][::-1]\n",
    "bottom_indices = np.argsort(mean_sdi_per_region)[:N]\n",
    "\n",
    "# --- Print results ---\n",
    "print(f\"\\nTop {N} Coupled Regions (Highest Mean SDI):\")\n",
    "for i in top_indices:\n",
    "    print(f\"Region {i+1} — Mean SDI: {mean_sdi_per_region[i]:.4f}\")\n",
    "\n",
    "print(f\"\\nTop {N} Decoupled Regions (Lowest Mean SDI):\")\n",
    "for i in bottom_indices:\n",
    "    print(f\"Region {i+1} — Mean SDI: {mean_sdi_per_region[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d4f805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "\n",
    "# Fetch the Schaefer 2018 atlas with 400 regions and 7-network solution\n",
    "schaefer = datasets.fetch_atlas_schaefer_2018(n_rois=400, yeo_networks=7, resolution_mm=1)\n",
    "\n",
    "# Extract the labels\n",
    "region_labels = schaefer['labels']  # This is a list of region names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a1bb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# -------- Define Schaefer-400 Parcel Names --------\n",
    "network_names = [\n",
    "    (\"Vis\", 75),\n",
    "    (\"SomMot\", 60),\n",
    "    (\"DorsAttn\", 52),\n",
    "    (\"SalVentAttn\", 50),\n",
    "    (\"Limbic\", 25),\n",
    "    (\"Cont\", 58),\n",
    "    (\"Default\", 80)\n",
    "]\n",
    "\n",
    "parcel_names = {}\n",
    "idx = 1\n",
    "for net, count in network_names:\n",
    "    for i in range(1, count + 1):\n",
    "        parcel_names[idx] = f\"{net}_{i}\"\n",
    "        idx += 1\n",
    "\n",
    "# -------- All CSV Files Across Sites --------\n",
    "csv_paths = [\n",
    "    \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_ip.csv\",\n",
    "    \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_nyu1.csv\",\n",
    "    \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_nyu2.csv\",\n",
    "    \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_sdsu.csv\",\n",
    "    \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_bni.csv\"\n",
    "]\n",
    "\n",
    "all_sdi = []\n",
    "\n",
    "# -------- Load and Stack SDI Data from All Sites --------\n",
    "for path in csv_paths:\n",
    "    df = pd.read_csv(path)\n",
    "    sdi_vals = df.iloc[:, 1:401].to_numpy(dtype=float)\n",
    "    all_sdi.append(sdi_vals)\n",
    "\n",
    "sdi_matrix = np.vstack(all_sdi)  # shape: (total_patients, 400)\n",
    "\n",
    "# -------- Compute Mean SDI Per Region Across Patients --------\n",
    "mean_sdi = np.nanmean(sdi_matrix, axis=0)\n",
    "\n",
    "# -------- Identify Top Coupled and Decoupled Regions --------\n",
    "N = 10\n",
    "top_indices = np.argsort(mean_sdi)[-N:][::-1]\n",
    "bottom_indices = np.argsort(mean_sdi)[:N]\n",
    "\n",
    "# -------- Print Results with Parcel Names --------\n",
    "print(f\"Top {N} Coupled Regions (Highest Mean SDI):\")\n",
    "for i in top_indices:\n",
    "    parcel_id = i + 1\n",
    "    name = parcel_names.get(parcel_id, f\"Region_{parcel_id}\")\n",
    "    print(f\"{name} (Region {parcel_id}) — Mean SDI: {mean_sdi[i]:.4f}\")\n",
    "\n",
    "print(f\"\\nTop {N} Decoupled Regions (Lowest Mean SDI):\")\n",
    "for i in bottom_indices:\n",
    "    parcel_id = i + 1\n",
    "    name = parcel_names.get(parcel_id, f\"Region_{parcel_id}\")\n",
    "    print(f\"{name} (Region {parcel_id}) — Mean SDI: {mean_sdi[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9507e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn import plotting\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# --- Step 1: Define Paths ---\n",
    "atlas_path = \"/Users/arnavkarnik/Documents/Classification/Schaefer2018_400Parcels_7Networks_order_FSLMNI152_1mm.nii\"\n",
    "csv_paths = [\n",
    "    \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_ip.csv\",\n",
    "    \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_nyu1.csv\",\n",
    "    \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_nyu2.csv\",\n",
    "    \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_sdsu.csv\",\n",
    "    \"/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_bni.csv\"\n",
    "]\n",
    "\n",
    "# --- Step 2: Load and Combine SDI Data ---\n",
    "all_sdi = []\n",
    "for path in csv_paths:\n",
    "    df = pd.read_csv(path)\n",
    "    sdi_vals = df.iloc[:, 1:401].to_numpy(dtype=float)\n",
    "    all_sdi.append(sdi_vals)\n",
    "\n",
    "sdi_matrix = np.vstack(all_sdi)  # (total_patients, 400 regions)\n",
    "\n",
    "# --- Step 3: Compute Mean SDI and Identify Top/Bottom Regions ---\n",
    "mean_sdi = np.nanmean(sdi_matrix, axis=0)\n",
    "top_coupled = np.argsort(mean_sdi)[-10:][::-1] + 1     # Convert to 1-based\n",
    "top_decoupled = np.argsort(mean_sdi)[:10] + 1\n",
    "\n",
    "# --- Step 4: Load Atlas and Generate Region Masks ---\n",
    "atlas_img = nib.load(atlas_path)\n",
    "atlas_data = atlas_img.get_fdata()\n",
    "\n",
    "top_coupled_mask = np.isin(atlas_data, top_coupled).astype(int)\n",
    "top_decoupled_mask = np.isin(atlas_data, top_decoupled).astype(int)\n",
    "\n",
    "# 0: background, 1: coupled (red), 2: decoupled (blue)\n",
    "combined_mask = top_coupled_mask + (top_decoupled_mask * 2)\n",
    "combined_img = nib.Nifti1Image(combined_mask.astype(np.uint8), affine=atlas_img.affine)\n",
    "\n",
    "# --- Step 5: Plotting ---\n",
    "display = plotting.plot_roi(\n",
    "    combined_img,\n",
    "    title=\"Top Coupled (red) & Decoupled (blue) Regions (All Sites)\",\n",
    "    cmap=plt.cm.get_cmap('coolwarm', 3),\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "# Add legend\n",
    "red_patch = mpatches.Patch(color='red', label='Top Coupled')\n",
    "blue_patch = mpatches.Patch(color='blue', label='Top Decoupled')\n",
    "plt.legend(handles=[red_patch, blue_patch], loc='lower left')\n",
    "\n",
    "plotting.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c19b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from nilearn import plotting, image, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define sites\n",
    "sites = {\n",
    "    'IP': {\n",
    "        'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_ip.csv',\n",
    "        'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/IP_1_phenotypes.csv'\n",
    "    },\n",
    "    'BNI': {\n",
    "        'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_bni.csv',\n",
    "        'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/BNI_1_phenotypes.csv'\n",
    "    },\n",
    "    'NYU1': {\n",
    "        'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_nyu1.csv',\n",
    "        'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/NYU_1_phenotypes.csv'\n",
    "    },\n",
    "    'NYU2': {\n",
    "        'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_nyu2.csv',\n",
    "        'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/NYU_2_phenotypes.csv'\n",
    "    },\n",
    "    'SDSU': {\n",
    "        'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_sdsu.csv',\n",
    "        'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/SDSU_1_phenotypes.csv'\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- Step 1: Load atlas and compute coordinates (do this once) ---\n",
    "atlas_path = \"/Users/arnavkarnik/Documents/Classification/Schaefer2018_400Parcels_7Networks_order_FSLMNI152_1mm.nii\"\n",
    "atlas_img = nib.load(atlas_path)\n",
    "atlas_data = atlas_img.get_fdata()\n",
    "affine = atlas_img.affine\n",
    "\n",
    "# Compute center-of-mass (coordinates) for each region\n",
    "coords = []\n",
    "for label in range(0, 400):  # 0-based indexing (0-399)\n",
    "    mask = atlas_data == (label + 1)  # Atlas labels are still 1-based (1-400)\n",
    "    if np.any(mask):\n",
    "        indices = np.argwhere(mask)\n",
    "        center_voxel = np.mean(indices, axis=0)\n",
    "        center_mm = nib.affines.apply_affine(affine, center_voxel)\n",
    "        coords.append(center_mm)\n",
    "coords = np.array(coords)  # shape (400, 3)\n",
    "\n",
    "# --- Step 2: Process each site ---\n",
    "for site_name, paths in sites.items():\n",
    "    print(f\"Processing site: {site_name}\")\n",
    "    \n",
    "    # Load SDI values for this site\n",
    "    df = pd.read_csv(paths['sdi'])\n",
    "    sdi_matrix = df.iloc[:, 1:401].to_numpy(dtype=float)  # (subjects, 400 ROIs)\n",
    "    mean_sdi = np.nanmean(sdi_matrix, axis=0)\n",
    "    \n",
    "    # Normalize SDI values to 0-1 range\n",
    "    sdi_normalized = (mean_sdi - np.min(mean_sdi)) / (np.max(mean_sdi) - np.min(mean_sdi))\n",
    "    sdi_scaled = 20 + sdi_normalized * 80  # marker size from 20 to 100\n",
    "    \n",
    "    # Create visualization for this site\n",
    "    fig = plotting.plot_markers(\n",
    "        node_coords=coords,\n",
    "        node_values=sdi_normalized,\n",
    "        node_size=sdi_scaled,\n",
    "        display_mode='lyrz',\n",
    "        title=f\"Schaefer-400 ROIs – Mean SDI for {site_name}\"\n",
    "    )\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Optional: Print some statistics\n",
    "    print(f\"  Mean SDI range: {np.min(mean_sdi):.4f} to {np.max(mean_sdi):.4f}\")\n",
    "    print(f\"  Number of subjects: {sdi_matrix.shape[0]}\")\n",
    "    print(f\"  Number of ROIs with valid data: {np.sum(~np.isnan(mean_sdi))}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb8cfdb",
   "metadata": {},
   "source": [
    "### Classification of Healthy vs Autistic patients in ABIDE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eec662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import (train_test_split, cross_val_score, \n",
    "                                   StratifiedKFold, GridSearchCV)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# File paths - Configure to include/exclude sites as needed\n",
    "sites = {\n",
    "    'IP': {\n",
    "        'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_ip.csv',\n",
    "        'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/IP_1_phenotypes.csv'\n",
    "    },\n",
    "    'BNI': {\n",
    "        'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_bni.csv',\n",
    "        'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/BNI_1_phenotypes.csv'\n",
    "    },\n",
    "    'NYU1': {\n",
    "        'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_nyu1.csv',\n",
    "        'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/NYU_1_phenotypes.csv'\n",
    "    },\n",
    "    'NYU2': {\n",
    "        'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_nyu2.csv',\n",
    "        'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/NYU_2_phenotypes.csv'\n",
    "    },\n",
    "    'SDSU': {\n",
    "        'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_sdsu.csv',\n",
    "        'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/SDSU_1_phenotypes.csv'\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_site_data(sites, exclude_sites=None):\n",
    "    \"\"\"Load data from each site separately.\"\"\"\n",
    "    if exclude_sites is None:\n",
    "        exclude_sites = []\n",
    "    \n",
    "    site_data = {}\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(\"Loading data from sites:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for site, paths in sites.items():\n",
    "        if site in exclude_sites:\n",
    "            print(f\"{site}: EXCLUDED\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            sdi_df = pd.read_csv(paths['sdi'])\n",
    "            phen_df = pd.read_csv(paths['phenotype'])\n",
    "            \n",
    "            # Normalize patient IDs\n",
    "            sdi_df['PatientID'] = sdi_df['PatientID'].astype(str)\n",
    "            phen_df['SUB_ID'] = phen_df['SUB_ID'].astype(str)\n",
    "            \n",
    "            # Merge on patient ID\n",
    "            merged = pd.merge(sdi_df, phen_df, left_on='PatientID', right_on='SUB_ID')\n",
    "            \n",
    "            # Extract features and labels\n",
    "            features = merged.filter(like='SDI_Node').values\n",
    "            labels = merged['DX_GROUP'].values  # 1 = TD, 2 = ASD\n",
    "            \n",
    "            site_data[site] = {\n",
    "                'features': features,\n",
    "                'labels': labels\n",
    "            }\n",
    "            \n",
    "            # For combined analysis\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "            \n",
    "            # Print basic stats\n",
    "            label_counts = Counter(labels)\n",
    "            print(f\"{site}: {len(labels)} subjects (TD: {label_counts.get(1, 0)}, ASD: {label_counts.get(2, 0)}) - {features.shape[1]} features\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{site}: Failed to load - {e}\")\n",
    "    \n",
    "    # Combined dataset\n",
    "    if all_features:\n",
    "        X_combined = np.vstack(all_features)\n",
    "        y_combined = np.concatenate(all_labels)\n",
    "        \n",
    "        print(f\"\\nCombined dataset: {X_combined.shape}\")\n",
    "        print(f\"Label distribution: TD={np.sum(y_combined==1)}, ASD={np.sum(y_combined==2)}\")\n",
    "    else:\n",
    "        X_combined, y_combined = None, None\n",
    "    \n",
    "    return site_data, X_combined, y_combined\n",
    "\n",
    "def basic_cross_validation(X, y, n_splits=5):\n",
    "    \"\"\"Perform basic stratified cross-validation.\"\"\"\n",
    "    print(f\"\\nBasic {n_splits}-Fold Cross-Validation:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    clf = SVC(kernel='rbf', C=1, gamma='scale', random_state=42)\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(clf, X, y, cv=skf)\n",
    "    \n",
    "    print(f\"Cross-validated Accuracy Scores: {scores}\")\n",
    "    print(f\"Mean Accuracy: {np.mean(scores):.4f} ± {np.std(scores):.4f}\")\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def train_test_evaluation(X, y, test_size=0.2):\n",
    "    \"\"\"Train-test split evaluation with confusion matrix.\"\"\"\n",
    "    print(f\"\\nTrain-Test Split Evaluation (test_size={test_size}):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=42\n",
    "    )\n",
    "    \n",
    "    clf = SVC(kernel='rbf', C=1, gamma='scale', random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    final_accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Training set: {len(y_train)} samples\")\n",
    "    print(f\"Test set: {len(y_test)} samples\")\n",
    "    print(f\"\\n=== FINAL TEST ACCURACY: {final_accuracy:.4f} ({final_accuracy*100:.2f}%) ===\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['TD', 'ASD']))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    return y_test, y_pred, final_accuracy\n",
    "\n",
    "def leave_one_site_out_cv(site_data):\n",
    "    \"\"\"Perform Leave-One-Site-Out Cross-Validation.\"\"\"\n",
    "    \n",
    "    site_names = list(site_data.keys())\n",
    "    if len(site_names) < 2:\n",
    "        print(\"Need at least 2 sites for LOSO-CV\")\n",
    "        return None\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(f\"\\nLeave-One-Site-Out CV ({len(site_names)} folds):\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for test_site in site_names:\n",
    "        # Get training sites\n",
    "        train_sites = [s for s in site_names if s != test_site]\n",
    "        \n",
    "        # Combine training data\n",
    "        train_features = []\n",
    "        train_labels = []\n",
    "        for train_site in train_sites:\n",
    "            train_features.append(site_data[train_site]['features'])\n",
    "            train_labels.append(site_data[train_site]['labels'])\n",
    "        \n",
    "        X_train = np.vstack(train_features)\n",
    "        y_train = np.concatenate(train_labels)\n",
    "        \n",
    "        # Test data\n",
    "        X_test = site_data[test_site]['features']\n",
    "        y_test = site_data[test_site]['labels']\n",
    "        \n",
    "        # Preprocessing\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Hyperparameter tuning\n",
    "        param_grid = {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'gamma': ['scale', 0.01, 0.1],\n",
    "            'kernel': ['rbf', 'linear']\n",
    "        }\n",
    "        \n",
    "        svm = SVC(probability=True, random_state=42)\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        grid_search = GridSearchCV(svm, param_grid, cv=cv, scoring='accuracy')\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Train best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = best_model.predict(X_test_scaled)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'test_site': test_site,\n",
    "            'train_sites': train_sites,\n",
    "            'accuracy': accuracy,\n",
    "            'n_train': len(y_train),\n",
    "            'n_test': len(y_test),\n",
    "            'y_true': y_test,\n",
    "            'y_pred': y_pred,\n",
    "            'best_params': grid_search.best_params_\n",
    "        })\n",
    "        \n",
    "        print(f\"Test site: {test_site:5} | Accuracy: {accuracy:.3f} | Train: {len(y_train)} | Test: {len(y_test)} | Best params: {grid_search.best_params_}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def summarize_loso_results(results):\n",
    "    \"\"\"Summarize LOSO-CV results.\"\"\"\n",
    "    \n",
    "    accuracies = [r['accuracy'] for r in results]\n",
    "    mean_acc = np.mean(accuracies)\n",
    "    std_acc = np.std(accuracies)\n",
    "    \n",
    "    print(f\"\\nLOSO-CV Summary:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Mean accuracy: {mean_acc:.3f} ± {std_acc:.3f}\")\n",
    "    print(f\"Range: {np.min(accuracies):.3f} - {np.max(accuracies):.3f}\")\n",
    "    \n",
    "    # Per-site results\n",
    "    print(f\"\\nPer-site results:\")\n",
    "    for r in results:\n",
    "        print(f\"{r['test_site']}: {r['accuracy']:.3f}\")\n",
    "    \n",
    "    # Overall confusion matrix\n",
    "    all_true = np.concatenate([r['y_true'] for r in results])\n",
    "    all_pred = np.concatenate([r['y_pred'] for r in results])\n",
    "    overall_accuracy = accuracy_score(all_true, all_pred)\n",
    "    \n",
    "    print(f\"\\nOverall LOSO accuracy: {overall_accuracy:.3f}\")\n",
    "    print(\"\\nOverall LOSO classification report:\")\n",
    "    print(classification_report(all_true, all_pred, target_names=['TD', 'ASD']))\n",
    "    \n",
    "    # Interpretation\n",
    "    if mean_acc >= 0.75:\n",
    "        print(f\"\\n✓ Good cross-site generalization (accuracy: {mean_acc:.1%})\")\n",
    "    elif mean_acc >= 0.65:\n",
    "        print(f\"\\n~ Moderate cross-site generalization (accuracy: {mean_acc:.1%})\")\n",
    "        print(\"  Consider site harmonization techniques\")\n",
    "    else:\n",
    "        print(f\"\\n✗ Poor cross-site generalization (accuracy: {mean_acc:.1%})\")\n",
    "        print(\"  Strong site effects detected\")\n",
    "    \n",
    "    return all_true, all_pred, mean_acc\n",
    "\n",
    "def create_visualizations(basic_scores=None, y_test=None, y_pred=None, \n",
    "                         loso_results=None, all_true_loso=None, all_pred_loso=None):\n",
    "    \"\"\"Create comprehensive visualization plots.\"\"\"\n",
    "    \n",
    "    # Determine number of subplots needed\n",
    "    n_plots = 0\n",
    "    if basic_scores is not None:\n",
    "        n_plots += 1\n",
    "    if y_test is not None and y_pred is not None:\n",
    "        n_plots += 1\n",
    "    if loso_results is not None:\n",
    "        n_plots += 2  # accuracy per site + sample sizes\n",
    "    if all_true_loso is not None and all_pred_loso is not None:\n",
    "        n_plots += 1\n",
    "    \n",
    "    if n_plots == 0:\n",
    "        print(\"No data available for visualization\")\n",
    "        return\n",
    "    \n",
    "    # Create subplots\n",
    "    cols = min(3, n_plots)\n",
    "    rows = (n_plots + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 4*rows))\n",
    "    if n_plots == 1:\n",
    "        axes = [axes]\n",
    "    elif rows == 1:\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    plot_idx = 0\n",
    "    \n",
    "    # 1. Basic CV scores\n",
    "    if basic_scores is not None:\n",
    "        ax = axes[plot_idx]\n",
    "        ax.bar(range(len(basic_scores)), basic_scores, alpha=0.7)\n",
    "        ax.set_title('Basic Cross-Validation Scores')\n",
    "        ax.set_xlabel('Fold')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.set_ylim([0, 1])\n",
    "        ax.axhline(y=np.mean(basic_scores), color='red', linestyle='--', \n",
    "                  label=f'Mean: {np.mean(basic_scores):.3f}')\n",
    "        ax.legend()\n",
    "        plot_idx += 1\n",
    "    \n",
    "    # 2. Train-test confusion matrix\n",
    "    if y_test is not None and y_pred is not None:\n",
    "        ax = axes[plot_idx]\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=['TD', 'ASD'], yticklabels=['TD', 'ASD'], ax=ax)\n",
    "        ax.set_title('Train-Test Confusion Matrix')\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('Actual')\n",
    "        plot_idx += 1\n",
    "    \n",
    "    # 3. LOSO accuracy per site\n",
    "    if loso_results is not None:\n",
    "        ax = axes[plot_idx]\n",
    "        sites = [r['test_site'] for r in loso_results]\n",
    "        accuracies = [r['accuracy'] for r in loso_results]\n",
    "        \n",
    "        ax.bar(sites, accuracies, alpha=0.7)\n",
    "        ax.set_title('LOSO-CV: Accuracy per Test Site')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.set_ylim([0, 1])\n",
    "        ax.axhline(y=np.mean(accuracies), color='red', linestyle='--', \n",
    "                  label=f'Mean: {np.mean(accuracies):.3f}')\n",
    "        ax.legend()\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, acc in enumerate(accuracies):\n",
    "            ax.text(i, acc + 0.02, f'{acc:.3f}', ha='center', va='bottom')\n",
    "        plot_idx += 1\n",
    "        \n",
    "        # 4. LOSO sample sizes\n",
    "        if plot_idx < len(axes):\n",
    "            ax = axes[plot_idx]\n",
    "            n_train = [r['n_train'] for r in loso_results]\n",
    "            n_test = [r['n_test'] for r in loso_results]\n",
    "            \n",
    "            x = np.arange(len(sites))\n",
    "            width = 0.35\n",
    "            \n",
    "            ax.bar(x - width/2, n_train, width, label='Train', alpha=0.7)\n",
    "            ax.bar(x + width/2, n_test, width, label='Test', alpha=0.7)\n",
    "            ax.set_title('LOSO-CV: Sample Sizes')\n",
    "            ax.set_ylabel('Number of Subjects')\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(sites)\n",
    "            ax.legend()\n",
    "            plot_idx += 1\n",
    "    \n",
    "    # 5. LOSO overall confusion matrix\n",
    "    if all_true_loso is not None and all_pred_loso is not None and plot_idx < len(axes):\n",
    "        ax = axes[plot_idx]\n",
    "        cm = confusion_matrix(all_true_loso, all_pred_loso)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=['TD', 'ASD'], yticklabels=['TD', 'ASD'], ax=ax)\n",
    "        ax.set_title('LOSO-CV: Overall Confusion Matrix')\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('Actual')\n",
    "        plot_idx += 1\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(plot_idx, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    print(\"ASD Classification Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Configuration\n",
    "    EXCLUDE_SITES = []  # Add site names here to exclude them, e.g., ['NYU2']\n",
    "    RUN_BASIC_CV = False\n",
    "    RUN_TRAIN_TEST = True\n",
    "    RUN_LOSO_CV = False\n",
    "    CREATE_PLOTS = False\n",
    "    \n",
    "    # Load data\n",
    "    site_data, X_combined, y_combined = load_site_data(sites, exclude_sites=EXCLUDE_SITES)\n",
    "    \n",
    "    if X_combined is None or len(site_data) == 0:\n",
    "        print(\"No data loaded successfully. Check file paths.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize variables for plotting\n",
    "    basic_scores = None\n",
    "    y_test, y_pred = None, None\n",
    "    loso_results = None\n",
    "    all_true_loso, all_pred_loso = None, None\n",
    "    \n",
    "    # 2. Train-Test Split Evaluation only\n",
    "    if RUN_TRAIN_TEST:\n",
    "        y_test, y_pred, final_accuracy = train_test_evaluation(X_combined, y_combined)\n",
    "    \n",
    "    print(\"\\nAnalysis complete!\")\n",
    "    if EXCLUDE_SITES:\n",
    "        print(f\"Excluded sites: {EXCLUDE_SITES}\")\n",
    "    print(f\"Included sites: {list(site_data.keys())}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
