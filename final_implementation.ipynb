{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4071e8de",
   "metadata": {},
   "source": [
    "# Classification of Healthy vs Autistic Patients "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eade14ed",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM) Classifier using FC for ABIDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee7c4883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASD Classification Pipeline\n",
      "==================================================\n",
      "Loading data from sites:\n",
      "----------------------------------------\n",
      "IP: 35 subjects (TD: 13, ASD: 22) - 400 features\n",
      "BNI: 56 subjects (TD: 27, ASD: 29) - 400 features\n",
      "NYU1: 46 subjects (TD: 27, ASD: 19) - 400 features\n",
      "NYU2: 15 subjects (TD: 15, ASD: 0) - 400 features\n",
      "SDSU: 51 subjects (TD: 29, ASD: 22) - 400 features\n",
      "ABIDE1: 867 subjects (TD: 402, ASD: 465) - 400 features\n",
      "\n",
      "Combined dataset: (1070, 400)\n",
      "Label distribution: TD=513, ASD=557\n",
      "\n",
      "Train-Test Split Evaluation (test_size=0.2):\n",
      "----------------------------------------\n",
      "Training set: 856 samples\n",
      "Test set: 214 samples\n",
      "\n",
      "=== FINAL TEST ACCURACY: 0.6449 (64.49%) ===\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          TD       0.62      0.68      0.65       103\n",
      "         ASD       0.67      0.61      0.64       111\n",
      "\n",
      "    accuracy                           0.64       214\n",
      "   macro avg       0.65      0.65      0.64       214\n",
      "weighted avg       0.65      0.64      0.64       214\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[70 33]\n",
      " [43 68]]\n",
      "\n",
      "Analysis complete!\n",
      "Included sites: ['IP', 'BNI', 'NYU1', 'NYU2', 'SDSU', 'ABIDE1']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import (train_test_split, cross_val_score, \n",
    "                                   StratifiedKFold, GridSearchCV)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# File paths - Configure to include/exclude sites as needed\n",
    "sites = {\n",
    "    'IP': {\n",
    "        'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_ip.csv',\n",
    "        'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/IP_1_phenotypes.csv'\n",
    "    },\n",
    "    'BNI': {\n",
    "        'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_bni.csv',\n",
    "        'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/BNI_1_phenotypes.csv'\n",
    "    },\n",
    "    'NYU1': {\n",
    "        'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_nyu1.csv',\n",
    "        'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/NYU_1_phenotypes.csv'\n",
    "    },\n",
    "    'NYU2': {\n",
    "        'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_nyu2.csv',\n",
    "        'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/NYU_2_phenotypes.csv'\n",
    "    },\n",
    "    'SDSU': {\n",
    "        'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_sdsu.csv',\n",
    "        'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/SDSU_1_phenotypes.csv'\n",
    "    },\n",
    "    'ABIDE1': {\n",
    "        'sdi':'/Users/arnavkarnik/Documents/Classification/results_ABIDE1FC/sdi_informed_energy_normalized_abide1.csv',\n",
    "        'phenotype' : '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE1/Phenotypic_V1_0b_preprocessed1.csv'\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_site_data(sites, exclude_sites=None):\n",
    "    \"\"\"Load data from each site separately.\"\"\"\n",
    "    if exclude_sites is None:\n",
    "        exclude_sites = []\n",
    "    \n",
    "    site_data = {}\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(\"Loading data from sites:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for site, paths in sites.items():\n",
    "        if site in exclude_sites:\n",
    "            print(f\"{site}: EXCLUDED\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            sdi_df = pd.read_csv(paths['sdi'])\n",
    "            phen_df = pd.read_csv(paths['phenotype'])\n",
    "            \n",
    "            # Normalize patient IDs\n",
    "            sdi_df['PatientID'] = sdi_df['PatientID'].astype(str)\n",
    "            phen_df['SUB_ID'] = phen_df['SUB_ID'].astype(str)\n",
    "            \n",
    "            # Merge on patient ID\n",
    "            merged = pd.merge(sdi_df, phen_df, left_on='PatientID', right_on='SUB_ID')\n",
    "            \n",
    "            # Extract features and labels\n",
    "            features = merged.filter(like='SDI_Node').values\n",
    "            labels = merged['DX_GROUP'].values  # 1 = TD, 2 = ASD\n",
    "            \n",
    "            site_data[site] = {\n",
    "                'features': features,\n",
    "                'labels': labels\n",
    "            }\n",
    "            \n",
    "            # For combined analysis\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "            \n",
    "            # Print basic stats\n",
    "            label_counts = Counter(labels)\n",
    "            print(f\"{site}: {len(labels)} subjects (TD: {label_counts.get(1, 0)}, ASD: {label_counts.get(2, 0)}) - {features.shape[1]} features\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{site}: Failed to load - {e}\")\n",
    "    \n",
    "    # Combined dataset\n",
    "    if all_features:\n",
    "        X_combined = np.vstack(all_features)\n",
    "        y_combined = np.concatenate(all_labels)\n",
    "        \n",
    "        print(f\"\\nCombined dataset: {X_combined.shape}\")\n",
    "        print(f\"Label distribution: TD={np.sum(y_combined==1)}, ASD={np.sum(y_combined==2)}\")\n",
    "    else:\n",
    "        X_combined, y_combined = None, None\n",
    "    \n",
    "    return site_data, X_combined, y_combined\n",
    "\n",
    "def basic_cross_validation(X, y, n_splits=5):\n",
    "    \"\"\"Perform basic stratified cross-validation.\"\"\"\n",
    "    print(f\"\\nBasic {n_splits}-Fold Cross-Validation:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    clf = SVC(kernel='rbf', C=1, gamma='scale', random_state=42)\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(clf, X, y, cv=skf)\n",
    "    \n",
    "    print(f\"Cross-validated Accuracy Scores: {scores}\")\n",
    "    print(f\"Mean Accuracy: {np.mean(scores):.4f} Â± {np.std(scores):.4f}\")\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def train_test_evaluation(X, y, test_size=0.2):\n",
    "    \"\"\"Train-test split evaluation with confusion matrix.\"\"\"\n",
    "    print(f\"\\nTrain-Test Split Evaluation (test_size={test_size}):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=42\n",
    "    )\n",
    "    \n",
    "    clf = SVC(kernel='rbf', C=1, gamma='scale', random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    final_accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Training set: {len(y_train)} samples\")\n",
    "    print(f\"Test set: {len(y_test)} samples\")\n",
    "    print(f\"\\n=== FINAL TEST ACCURACY: {final_accuracy:.4f} ({final_accuracy*100:.2f}%) ===\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['TD', 'ASD']))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    return y_test, y_pred, final_accuracy\n",
    "\n",
    "def leave_one_site_out_cv(site_data):\n",
    "    \"\"\"Perform Leave-One-Site-Out Cross-Validation.\"\"\"\n",
    "    \n",
    "    site_names = list(site_data.keys())\n",
    "    if len(site_names) < 2:\n",
    "        print(\"Need at least 2 sites for LOSO-CV\")\n",
    "        return None\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(f\"\\nLeave-One-Site-Out CV ({len(site_names)} folds):\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for test_site in site_names:\n",
    "        # Get training sites\n",
    "        train_sites = [s for s in site_names if s != test_site]\n",
    "        \n",
    "        # Combine training data\n",
    "        train_features = []\n",
    "        train_labels = []\n",
    "        for train_site in train_sites:\n",
    "            train_features.append(site_data[train_site]['features'])\n",
    "            train_labels.append(site_data[train_site]['labels'])\n",
    "        \n",
    "        X_train = np.vstack(train_features)\n",
    "        y_train = np.concatenate(train_labels)\n",
    "        \n",
    "        # Test data\n",
    "        X_test = site_data[test_site]['features']\n",
    "        y_test = site_data[test_site]['labels']\n",
    "        \n",
    "        # Preprocessing\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Hyperparameter tuning\n",
    "        param_grid = {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'gamma': ['scale', 0.01, 0.1],\n",
    "            'kernel': ['rbf', 'linear']\n",
    "        }\n",
    "        \n",
    "        svm = SVC(probability=True, random_state=42)\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        grid_search = GridSearchCV(svm, param_grid, cv=cv, scoring='accuracy')\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Train best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = best_model.predict(X_test_scaled)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'test_site': test_site,\n",
    "            'train_sites': train_sites,\n",
    "            'accuracy': accuracy,\n",
    "            'n_train': len(y_train),\n",
    "            'n_test': len(y_test),\n",
    "            'y_true': y_test,\n",
    "            'y_pred': y_pred,\n",
    "            'best_params': grid_search.best_params_\n",
    "        })\n",
    "        \n",
    "        print(f\"Test site: {test_site:5} | Accuracy: {accuracy:.3f} | Train: {len(y_train)} | Test: {len(y_test)} | Best params: {grid_search.best_params_}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def summarize_loso_results(results):\n",
    "    \"\"\"Summarize LOSO-CV results.\"\"\"\n",
    "    \n",
    "    accuracies = [r['accuracy'] for r in results]\n",
    "    mean_acc = np.mean(accuracies)\n",
    "    std_acc = np.std(accuracies)\n",
    "    \n",
    "    print(f\"\\nLOSO-CV Summary:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Mean accuracy: {mean_acc:.3f} Â± {std_acc:.3f}\")\n",
    "    print(f\"Range: {np.min(accuracies):.3f} - {np.max(accuracies):.3f}\")\n",
    "    \n",
    "    # Per-site results\n",
    "    print(f\"\\nPer-site results:\")\n",
    "    for r in results:\n",
    "        print(f\"{r['test_site']}: {r['accuracy']:.3f}\")\n",
    "    \n",
    "    # Overall confusion matrix\n",
    "    all_true = np.concatenate([r['y_true'] for r in results])\n",
    "    all_pred = np.concatenate([r['y_pred'] for r in results])\n",
    "    overall_accuracy = accuracy_score(all_true, all_pred)\n",
    "    \n",
    "    print(f\"\\nOverall LOSO accuracy: {overall_accuracy:.3f}\")\n",
    "    print(\"\\nOverall LOSO classification report:\")\n",
    "    print(classification_report(all_true, all_pred, target_names=['TD', 'ASD']))\n",
    "    \n",
    "    # Interpretation\n",
    "    if mean_acc >= 0.75:\n",
    "        print(f\"\\nâ Good cross-site generalization (accuracy: {mean_acc:.1%})\")\n",
    "    elif mean_acc >= 0.65:\n",
    "        print(f\"\\n~ Moderate cross-site generalization (accuracy: {mean_acc:.1%})\")\n",
    "        print(\"  Consider site harmonization techniques\")\n",
    "    else:\n",
    "        print(f\"\\nâ Poor cross-site generalization (accuracy: {mean_acc:.1%})\")\n",
    "        print(\"  Strong site effects detected\")\n",
    "    \n",
    "    return all_true, all_pred, mean_acc\n",
    "\n",
    "def create_visualizations(basic_scores=None, y_test=None, y_pred=None, \n",
    "                         loso_results=None, all_true_loso=None, all_pred_loso=None):\n",
    "    \"\"\"Create comprehensive visualization plots.\"\"\"\n",
    "    \n",
    "    # Determine number of subplots needed\n",
    "    n_plots = 0\n",
    "    if basic_scores is not None:\n",
    "        n_plots += 1\n",
    "    if y_test is not None and y_pred is not None:\n",
    "        n_plots += 1\n",
    "    if loso_results is not None:\n",
    "        n_plots += 2  # accuracy per site + sample sizes\n",
    "    if all_true_loso is not None and all_pred_loso is not None:\n",
    "        n_plots += 1\n",
    "    \n",
    "    if n_plots == 0:\n",
    "        print(\"No data available for visualization\")\n",
    "        return\n",
    "    \n",
    "    # Create subplots\n",
    "    cols = min(3, n_plots)\n",
    "    rows = (n_plots + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 4*rows))\n",
    "    if n_plots == 1:\n",
    "        axes = [axes]\n",
    "    elif rows == 1:\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    plot_idx = 0\n",
    "    \n",
    "    # 1. Basic CV scores\n",
    "    if basic_scores is not None:\n",
    "        ax = axes[plot_idx]\n",
    "        ax.bar(range(len(basic_scores)), basic_scores, alpha=0.7)\n",
    "        ax.set_title('Basic Cross-Validation Scores')\n",
    "        ax.set_xlabel('Fold')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.set_ylim([0, 1])\n",
    "        ax.axhline(y=np.mean(basic_scores), color='red', linestyle='--', \n",
    "                  label=f'Mean: {np.mean(basic_scores):.3f}')\n",
    "        ax.legend()\n",
    "        plot_idx += 1\n",
    "    \n",
    "    # 2. Train-test confusion matrix\n",
    "    if y_test is not None and y_pred is not None:\n",
    "        ax = axes[plot_idx]\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=['TD', 'ASD'], yticklabels=['TD', 'ASD'], ax=ax)\n",
    "        ax.set_title('Train-Test Confusion Matrix')\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('Actual')\n",
    "        plot_idx += 1\n",
    "    \n",
    "    # 3. LOSO accuracy per site\n",
    "    if loso_results is not None:\n",
    "        ax = axes[plot_idx]\n",
    "        sites = [r['test_site'] for r in loso_results]\n",
    "        accuracies = [r['accuracy'] for r in loso_results]\n",
    "        \n",
    "        ax.bar(sites, accuracies, alpha=0.7)\n",
    "        ax.set_title('LOSO-CV: Accuracy per Test Site')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.set_ylim([0, 1])\n",
    "        ax.axhline(y=np.mean(accuracies), color='red', linestyle='--', \n",
    "                  label=f'Mean: {np.mean(accuracies):.3f}')\n",
    "        ax.legend()\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, acc in enumerate(accuracies):\n",
    "            ax.text(i, acc + 0.02, f'{acc:.3f}', ha='center', va='bottom')\n",
    "        plot_idx += 1\n",
    "        \n",
    "        # 4. LOSO sample sizes\n",
    "        if plot_idx < len(axes):\n",
    "            ax = axes[plot_idx]\n",
    "            n_train = [r['n_train'] for r in loso_results]\n",
    "            n_test = [r['n_test'] for r in loso_results]\n",
    "            \n",
    "            x = np.arange(len(sites))\n",
    "            width = 0.35\n",
    "            \n",
    "            ax.bar(x - width/2, n_train, width, label='Train', alpha=0.7)\n",
    "            ax.bar(x + width/2, n_test, width, label='Test', alpha=0.7)\n",
    "            ax.set_title('LOSO-CV: Sample Sizes')\n",
    "            ax.set_ylabel('Number of Subjects')\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(sites)\n",
    "            ax.legend()\n",
    "            plot_idx += 1\n",
    "    \n",
    "    # 5. LOSO overall confusion matrix\n",
    "    if all_true_loso is not None and all_pred_loso is not None and plot_idx < len(axes):\n",
    "        ax = axes[plot_idx]\n",
    "        cm = confusion_matrix(all_true_loso, all_pred_loso)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=['TD', 'ASD'], yticklabels=['TD', 'ASD'], ax=ax)\n",
    "        ax.set_title('LOSO-CV: Overall Confusion Matrix')\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('Actual')\n",
    "        plot_idx += 1\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(plot_idx, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    print(\"ASD Classification Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Configuration\n",
    "    EXCLUDE_SITES = []  # Add site names here to exclude them, e.g., ['NYU2']\n",
    "    RUN_BASIC_CV = False\n",
    "    RUN_TRAIN_TEST = True\n",
    "    RUN_LOSO_CV = False\n",
    "    CREATE_PLOTS = False\n",
    "    \n",
    "    # Load data\n",
    "    site_data, X_combined, y_combined = load_site_data(sites, exclude_sites=EXCLUDE_SITES)\n",
    "    \n",
    "    if X_combined is None or len(site_data) == 0:\n",
    "        print(\"No data loaded successfully. Check file paths.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize variables for plotting\n",
    "    basic_scores = None\n",
    "    y_test, y_pred = None, None\n",
    "    loso_results = None\n",
    "    all_true_loso, all_pred_loso = None, None\n",
    "    \n",
    "    # 2. Train-Test Split Evaluation only\n",
    "    if RUN_TRAIN_TEST:\n",
    "        y_test, y_pred, final_accuracy = train_test_evaluation(X_combined, y_combined)\n",
    "    \n",
    "    print(\"\\nAnalysis complete!\")\n",
    "    if EXCLUDE_SITES:\n",
    "        print(f\"Excluded sites: {EXCLUDE_SITES}\")\n",
    "    print(f\"Included sites: {list(site_data.keys())}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2591d264",
   "metadata": {},
   "source": [
    "### PCA - Logistic Regression using FC for ABIDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cd70334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "=== Site: ABIDE1 ===\n",
      "SDI shape: (867, 401), Phenotype shape: (1112, 106)\n",
      "Merged samples: 867\n",
      "DX_GROUP distribution: {2: 465, 1: 402}\n",
      "Added 867 samples with 400 features\n",
      "\n",
      "=== Site: IP ===\n",
      "SDI shape: (35, 401), Phenotype shape: (36, 349)\n",
      "Merged samples: 35\n",
      "DX_GROUP distribution: {2: 22, 1: 13}\n",
      "Added 35 samples with 400 features\n",
      "\n",
      "=== Site: BNI ===\n",
      "SDI shape: (56, 401), Phenotype shape: (57, 349)\n",
      "Merged samples: 56\n",
      "DX_GROUP distribution: {2: 29, 1: 27}\n",
      "Added 56 samples with 400 features\n",
      "\n",
      "=== Site: NYU1 ===\n",
      "SDI shape: (46, 401), Phenotype shape: (47, 349)\n",
      "Merged samples: 46\n",
      "DX_GROUP distribution: {1: 27, 2: 19}\n",
      "Added 46 samples with 400 features\n",
      "\n",
      "=== Site: NYU2 ===\n",
      "SDI shape: (15, 401), Phenotype shape: (15, 349)\n",
      "Merged samples: 15\n",
      "DX_GROUP distribution: {1: 15}\n",
      "Added 15 samples with 400 features\n",
      "\n",
      "=== Site: SDSU ===\n",
      "SDI shape: (51, 401), Phenotype shape: (54, 349)\n",
      "Merged samples: 51\n",
      "DX_GROUP distribution: {1: 29, 2: 22}\n",
      "Added 51 samples with 400 features\n",
      "\n",
      "==================================================\n",
      "DATA QUALITY ASSESSMENT\n",
      "==================================================\n",
      "Total samples: 1070\n",
      "Total features: 400\n",
      "Class distribution: Counter({np.int64(2): 557, np.int64(1): 513})\n",
      "Minimum class ratio: 0.479\n",
      "Site distribution: {'ABIDE1': 867, 'IP': 35, 'BNI': 56, 'NYU1': 46, 'NYU2': 15, 'SDSU': 51}\n",
      "Constant features: 0\n",
      "Low variance features (< 1e-6): 0\n",
      "\n",
      "==================================================\n",
      "COMPREHENSIVE MODEL EVALUATION\n",
      "==================================================\n",
      "Using 400 features after removing constants\n",
      "Train set: 856 samples\n",
      "Test set: 214 samples\n",
      "Train class distribution: Counter({np.int64(2): 446, np.int64(1): 410})\n",
      "Test class distribution: Counter({np.int64(2): 111, np.int64(1): 103})\n",
      "\n",
      "--- Preprocessing: PCA_50 ---\n",
      "After SMOTE: Counter({np.int64(2): 446, np.int64(1): 446})\n",
      "  Random Forest: 0.547\n",
      "  Gradient Boosting: 0.551\n",
      "  Logistic Regression: 0.626\n",
      "  Stacking: 0.593\n",
      "\n",
      "--- Preprocessing: SelectK_50 ---\n",
      "After SMOTE: Counter({np.int64(2): 446, np.int64(1): 446})\n",
      "  Random Forest: 0.551\n",
      "  Gradient Boosting: 0.547\n",
      "  Logistic Regression: 0.575\n",
      "  Stacking: 0.561\n",
      "\n",
      "--- Preprocessing: PCA_100 ---\n",
      "After SMOTE: Counter({np.int64(2): 446, np.int64(1): 446})\n",
      "  Random Forest: 0.565\n",
      "  Gradient Boosting: 0.575\n",
      "  Logistic Regression: 0.654\n",
      "  Stacking: 0.626\n",
      "\n",
      "--- Preprocessing: SelectK_PCA ---\n",
      "After SMOTE: Counter({np.int64(2): 446, np.int64(1): 446})\n",
      "  Random Forest: 0.584\n",
      "  Gradient Boosting: 0.556\n",
      "  Logistic Regression: 0.584\n",
      "  Stacking: 0.561\n",
      "\n",
      "==================================================\n",
      "RESULTS SUMMARY\n",
      "==================================================\n",
      "\n",
      "Top 5 configurations:\n",
      "1. PCA_100_Logistic Regression: 0.654\n",
      "2. PCA_50_Logistic Regression: 0.626\n",
      "3. PCA_100_Stacking: 0.626\n",
      "4. PCA_50_Stacking: 0.593\n",
      "5. SelectK_PCA_Random Forest: 0.584\n",
      "\n",
      "Best configuration: PCA_100_Logistic Regression\n",
      "Best test accuracy: 0.654 (65.4%)\n",
      "\n",
      "Detailed classification report for best model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.65      0.62      0.63       103\n",
      "           2       0.66      0.68      0.67       111\n",
      "\n",
      "    accuracy                           0.65       214\n",
      "   macro avg       0.65      0.65      0.65       214\n",
      "weighted avg       0.65      0.65      0.65       214\n",
      "\n",
      "\n",
      "==================================================\n",
      "CROSS-VALIDATION FOR BEST MODEL\n",
      "==================================================\n",
      "Best configuration: PCA_100 + Logistic Regression\n",
      "Cross-validation scores: [0.64018692 0.60747664 0.67757009 0.61214953 0.56542056]\n",
      "Mean CV accuracy: 0.621 (+/- 0.074)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ====== Step 1: Enhanced Data Loading with Quality Checks ======\n",
    "def load_data_with_demographics(sites):\n",
    "    X_combined, y_combined, site_labels = [], [], []\n",
    "    \n",
    "    for site, paths in sites.items():\n",
    "        sdi_file = paths['sdi']\n",
    "        phenotype_file = paths['phenotype']\n",
    "        \n",
    "        if not os.path.exists(sdi_file) or not os.path.exists(phenotype_file):\n",
    "            print(f\"[Warning] Missing files for site: {site}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            df_sdi = pd.read_csv(sdi_file)\n",
    "            df_pheno = pd.read_csv(phenotype_file)\n",
    "            \n",
    "            print(f\"\\n=== Site: {site} ===\")\n",
    "            print(f\"SDI shape: {df_sdi.shape}, Phenotype shape: {df_pheno.shape}\")\n",
    "            \n",
    "            # Find ID columns\n",
    "            id_column_sdi = next((col for col in df_sdi.columns if col.lower() in ['subject_id', 'patientid', 'sub_id']), None)\n",
    "            id_column_pheno = next((col for col in df_pheno.columns if col.lower() in ['subject_id', 'patientid', 'sub_id']), None)\n",
    "            \n",
    "            if not id_column_sdi or not id_column_pheno:\n",
    "                print(f\"[Error] ID column missing in site {site}\")\n",
    "                print(f\"SDI columns: {list(df_sdi.columns[:5])}\")  # Show first 5 columns\n",
    "                print(f\"Phenotype columns: {list(df_pheno.columns)}\")\n",
    "                continue\n",
    "            \n",
    "            # Standardize ID column names\n",
    "            df_sdi.rename(columns={id_column_sdi: 'Subject_ID'}, inplace=True)\n",
    "            df_pheno.rename(columns={id_column_pheno: 'Subject_ID'}, inplace=True)\n",
    "            \n",
    "            df_sdi['Subject_ID'] = df_sdi['Subject_ID'].astype(str)\n",
    "            df_pheno['Subject_ID'] = df_pheno['Subject_ID'].astype(str)\n",
    "            \n",
    "            if 'DX_GROUP' not in df_pheno.columns:\n",
    "                print(f\"[Error] 'DX_GROUP' column missing in {site}\")\n",
    "                print(f\"Available phenotype columns: {list(df_pheno.columns)}\")\n",
    "                continue\n",
    "            \n",
    "            # Remove duplicates\n",
    "            df_sdi = df_sdi.drop_duplicates(subset=['Subject_ID'])\n",
    "            df_pheno = df_pheno.drop_duplicates(subset=['Subject_ID'])\n",
    "            \n",
    "            # Merge data\n",
    "            merged = pd.merge(df_sdi, df_pheno[['Subject_ID', 'DX_GROUP']], on='Subject_ID')\n",
    "            print(f\"Merged samples: {len(merged)}\")\n",
    "            print(f\"DX_GROUP distribution: {merged['DX_GROUP'].value_counts().to_dict()}\")\n",
    "            \n",
    "            # Prepare features (only numeric columns)\n",
    "            feature_cols = merged.drop(columns=['Subject_ID', 'DX_GROUP'])\n",
    "            feature_cols = feature_cols.select_dtypes(include=[np.number])\n",
    "            \n",
    "            # Handle missing values\n",
    "            if feature_cols.isnull().any().any():\n",
    "                print(f\"Found missing values, filling with median\")\n",
    "                feature_cols = feature_cols.fillna(feature_cols.median())\n",
    "            \n",
    "            # Remove infinite values\n",
    "            inf_mask = np.isinf(feature_cols.values).any(axis=1)\n",
    "            if inf_mask.any():\n",
    "                print(f\"Removing {inf_mask.sum()} rows with infinite values\")\n",
    "                feature_cols = feature_cols[~inf_mask]\n",
    "                merged = merged[~inf_mask]\n",
    "            \n",
    "            X = feature_cols.values\n",
    "            y = merged['DX_GROUP'].values\n",
    "            \n",
    "            if X.shape[0] > 0 and X.shape[1] > 0:\n",
    "                X_combined.append(X)\n",
    "                y_combined.append(y)\n",
    "                site_labels.extend([site] * len(y))\n",
    "                print(f\"Added {len(y)} samples with {X.shape[1]} features\")\n",
    "            else:\n",
    "                print(f\"No valid data for site {site}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing site {site}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not X_combined:\n",
    "        return np.array([]), np.array([]), []\n",
    "    \n",
    "    return np.vstack(X_combined), np.concatenate(y_combined), site_labels\n",
    "\n",
    "# ====== Step 2: Data Quality Assessment ======\n",
    "def assess_data_quality(X, y, site_labels):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DATA QUALITY ASSESSMENT\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"Total samples: {len(y)}\")\n",
    "    print(f\"Total features: {X.shape[1]}\")\n",
    "    \n",
    "    # Class distribution\n",
    "    if y.dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        y_encoded = le.fit_transform(y)\n",
    "        print(f\"Label encoding: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "    else:\n",
    "        y_encoded = y\n",
    "    \n",
    "    class_counts = Counter(y_encoded)\n",
    "    print(f\"Class distribution: {class_counts}\")\n",
    "    \n",
    "    # Check for severe imbalance\n",
    "    total_samples = sum(class_counts.values())\n",
    "    min_class_ratio = min(class_counts.values()) / total_samples\n",
    "    print(f\"Minimum class ratio: {min_class_ratio:.3f}\")\n",
    "    \n",
    "    if min_class_ratio < 0.1:\n",
    "        print(\"â ï¸  WARNING: Severe class imbalance detected!\")\n",
    "    \n",
    "    # Site distribution\n",
    "    site_dist = Counter(site_labels)\n",
    "    print(f\"Site distribution: {dict(site_dist)}\")\n",
    "    \n",
    "    # Feature quality\n",
    "    feature_vars = np.var(X, axis=0)\n",
    "    constant_features = (feature_vars == 0).sum()\n",
    "    low_var_features = (feature_vars < 1e-6).sum()\n",
    "    \n",
    "    print(f\"Constant features: {constant_features}\")\n",
    "    print(f\"Low variance features (< 1e-6): {low_var_features}\")\n",
    "    \n",
    "    return y_encoded if y.dtype == 'object' else y, constant_features > 0\n",
    "\n",
    "# ====== Step 3: Multiple Preprocessing Approaches ======\n",
    "def create_preprocessing_pipelines(n_features):\n",
    "    \"\"\"Create different preprocessing approaches to test\"\"\"\n",
    "    \n",
    "    pipelines = {}\n",
    "    \n",
    "    # 1. PCA only\n",
    "    pipelines['PCA_50'] = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=min(50, n_features-1), random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # 2. Feature selection only\n",
    "    if n_features > 50:\n",
    "        pipelines['SelectK_50'] = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('selector', SelectKBest(f_classif, k=50))\n",
    "        ])\n",
    "    \n",
    "    # 3. PCA with more components\n",
    "    pipelines['PCA_100'] = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=min(100, n_features-1), random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # 4. Combined approach\n",
    "    if n_features > 100:\n",
    "        pipelines['SelectK_PCA'] = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('selector', SelectKBest(f_classif, k=min(200, n_features))),\n",
    "            ('pca', PCA(n_components=50, random_state=42))\n",
    "        ])\n",
    "    \n",
    "    return pipelines\n",
    "\n",
    "# ====== Step 4: Multiple Model Approaches ======\n",
    "def create_model_configurations():\n",
    "    \"\"\"Create different model configurations\"\"\"\n",
    "    \n",
    "    models = {}\n",
    "    \n",
    "    # Simple models\n",
    "    models['Random Forest'] = RandomForestClassifier(\n",
    "        n_estimators=200, max_depth=10, random_state=42, class_weight='balanced'\n",
    "    )\n",
    "    \n",
    "    models['Gradient Boosting'] = GradientBoostingClassifier(\n",
    "        n_estimators=200, learning_rate=0.05, max_depth=4, random_state=42\n",
    "    )\n",
    "    \n",
    "    models['Logistic Regression'] = LogisticRegression(\n",
    "        max_iter=2000, random_state=42, class_weight='balanced'\n",
    "    )\n",
    "    \n",
    "    # Stacking ensemble\n",
    "    base_models = [\n",
    "        ('rf', RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')),\n",
    "        ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "        ('lr', LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'))\n",
    "    ]\n",
    "    \n",
    "    models['Stacking'] = StackingClassifier(\n",
    "        estimators=base_models,\n",
    "        final_estimator=LogisticRegression(max_iter=1000, class_weight='balanced'),\n",
    "        cv=5\n",
    "    )\n",
    "    \n",
    "    return models\n",
    "\n",
    "# ====== Step 5: Comprehensive Evaluation ======\n",
    "def comprehensive_evaluation(X, y, use_smote=True):\n",
    "    \"\"\"Test all combinations of preprocessing and models\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"COMPREHENSIVE MODEL EVALUATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Remove constant features\n",
    "    feature_vars = np.var(X, axis=0)\n",
    "    valid_features = feature_vars > 1e-8\n",
    "    X_filtered = X[:, valid_features]\n",
    "    print(f\"Using {X_filtered.shape[1]} features after removing constants\")\n",
    "    \n",
    "    # Create pipelines and models\n",
    "    prep_pipelines = create_preprocessing_pipelines(X_filtered.shape[1])\n",
    "    models = create_model_configurations()\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_filtered, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Train set: {len(y_train)} samples\")\n",
    "    print(f\"Test set: {len(y_test)} samples\")\n",
    "    print(f\"Train class distribution: {Counter(y_train)}\")\n",
    "    print(f\"Test class distribution: {Counter(y_test)}\")\n",
    "    \n",
    "    # Test each combination\n",
    "    for prep_name, prep_pipeline in prep_pipelines.items():\n",
    "        print(f\"\\n--- Preprocessing: {prep_name} ---\")\n",
    "        \n",
    "        # Apply preprocessing\n",
    "        X_train_prep = prep_pipeline.fit_transform(X_train, y_train)\n",
    "        X_test_prep = prep_pipeline.transform(X_test)\n",
    "        \n",
    "        # Apply SMOTE if requested\n",
    "        if use_smote and len(Counter(y_train)) > 1:\n",
    "            smote = SMOTE(random_state=42)\n",
    "            X_train_prep, y_train_smote = smote.fit_resample(X_train_prep, y_train)\n",
    "            print(f\"After SMOTE: {Counter(y_train_smote)}\")\n",
    "        else:\n",
    "            y_train_smote = y_train\n",
    "        \n",
    "        # Test each model\n",
    "        for model_name, model in models.items():\n",
    "            try:\n",
    "                # Train model\n",
    "                model.fit(X_train_prep, y_train_smote)\n",
    "                \n",
    "                # Predict\n",
    "                y_pred = model.predict(X_test_prep)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                \n",
    "                # Store results\n",
    "                key = f\"{prep_name}_{model_name}\"\n",
    "                results[key] = {\n",
    "                    'accuracy': accuracy,\n",
    "                    'predictions': y_pred,\n",
    "                    'true_labels': y_test\n",
    "                }\n",
    "                \n",
    "                print(f\"  {model_name}: {accuracy:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  {model_name}: Error - {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ====== Step 6: Cross-Validation for Best Model ======\n",
    "def cross_validate_best_model(X, y, best_config):\n",
    "    \"\"\"Perform cross-validation on the best configuration\"\"\"\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"CROSS-VALIDATION FOR BEST MODEL\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Extract preprocessing and model from best config name\n",
    "    parts = best_config.split('_')\n",
    "    prep_name = parts[0] + ('_' + parts[1] if len(parts) > 2 and parts[1].isdigit() else '')\n",
    "    model_name = '_'.join(parts[1:] if prep_name == parts[0] else parts[2:])\n",
    "    \n",
    "    print(f\"Best configuration: {prep_name} + {model_name}\")\n",
    "    \n",
    "    # Remove constant features\n",
    "    feature_vars = np.var(X, axis=0)\n",
    "    valid_features = feature_vars > 1e-8\n",
    "    X_filtered = X[:, valid_features]\n",
    "    \n",
    "    # Create pipeline\n",
    "    prep_pipelines = create_preprocessing_pipelines(X_filtered.shape[1])\n",
    "    models = create_model_configurations()\n",
    "    \n",
    "    if prep_name in prep_pipelines and model_name in models:\n",
    "        # Create full pipeline\n",
    "        if Counter(y).most_common()[-1][1] / len(y) < 0.4:  # If imbalanced\n",
    "            full_pipeline = ImbPipeline([\n",
    "                ('preprocessing', prep_pipelines[prep_name]),\n",
    "                ('smote', SMOTE(random_state=42)),\n",
    "                ('classifier', models[model_name])\n",
    "            ])\n",
    "        else:\n",
    "            full_pipeline = Pipeline([\n",
    "                ('preprocessing', prep_pipelines[prep_name]),\n",
    "                ('classifier', models[model_name])\n",
    "            ])\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_scores = cross_val_score(full_pipeline, X_filtered, y, cv=cv, scoring='accuracy')\n",
    "        \n",
    "        print(f\"Cross-validation scores: {cv_scores}\")\n",
    "        print(f\"Mean CV accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "        \n",
    "        return cv_scores\n",
    "    else:\n",
    "        print(\"Could not find the specified configuration\")\n",
    "        return None\n",
    "\n",
    "# ====== Main Execution Function ======\n",
    "def main():\n",
    "    # Define site paths\n",
    "    site_paths = {\n",
    "        'ABIDE1': {\n",
    "            'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE1FC/sdi_informed_energy_normalized_abide1.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE1/Phenotypic_V1_0b_preprocessed1.csv'\n",
    "        },\n",
    "        'IP': {\n",
    "            'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_ip.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/IP_1_phenotypes.csv'\n",
    "        },\n",
    "        'BNI': {\n",
    "            'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_bni.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/BNI_1_phenotypes.csv'\n",
    "        },\n",
    "        'NYU1': {\n",
    "            'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_nyu1.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/NYU_1_phenotypes.csv'\n",
    "        },\n",
    "        'NYU2': {\n",
    "            'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_nyu2.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/NYU_2_phenotypes.csv'\n",
    "        },\n",
    "        'SDSU': {\n",
    "            'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_sdsu.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/SDSU_1_phenotypes.csv'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    X_combined, y_combined, site_labels = load_data_with_demographics(site_paths)\n",
    "    \n",
    "    if len(X_combined) == 0:\n",
    "        print(\"No data loaded. Please check your file paths.\")\n",
    "        return\n",
    "    \n",
    "    # Assess data quality\n",
    "    y_processed, has_constant_features = assess_data_quality(X_combined, y_combined, site_labels)\n",
    "    \n",
    "    # Check if we have enough data\n",
    "    if len(y_processed) < 50:\n",
    "        print(\"â ï¸  WARNING: Very small dataset. Results may not be reliable.\")\n",
    "    \n",
    "    if len(Counter(y_processed)) < 2:\n",
    "        print(\"â ERROR: Need at least 2 classes for classification\")\n",
    "        return\n",
    "    \n",
    "    # Comprehensive evaluation\n",
    "    results = comprehensive_evaluation(X_combined, y_processed, use_smote=True)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No successful model runs\")\n",
    "        return\n",
    "    \n",
    "    # Find best model\n",
    "    best_config = max(results.keys(), key=lambda k: results[k]['accuracy'])\n",
    "    best_accuracy = results[best_config]['accuracy']\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"RESULTS SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Show top 5 results\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1]['accuracy'], reverse=True)\n",
    "    print(\"\\nTop 5 configurations:\")\n",
    "    for i, (config, result) in enumerate(sorted_results[:5]):\n",
    "        print(f\"{i+1}. {config}: {result['accuracy']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nBest configuration: {best_config}\")\n",
    "    print(f\"Best test accuracy: {best_accuracy:.3f} ({best_accuracy*100:.1f}%)\")\n",
    "    \n",
    "    # Detailed report for best model\n",
    "    best_result = results[best_config]\n",
    "    print(f\"\\nDetailed classification report for best model:\")\n",
    "    print(classification_report(best_result['true_labels'], best_result['predictions']))\n",
    "    \n",
    "    # Cross-validation for best model\n",
    "    cv_scores = cross_validate_best_model(X_combined, y_processed, best_config)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6284e8ae",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM) Classifier using SC for ABIDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22d6d90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASD Classification Pipeline\n",
      "==================================================\n",
      "Loading data from sites:\n",
      "----------------------------------------\n",
      "IP: 36 subjects (TD: 14, ASD: 22) - 400 features\n",
      "BNI: 56 subjects (TD: 27, ASD: 29) - 400 features\n",
      "NYU1: 46 subjects (TD: 27, ASD: 19) - 400 features\n",
      "NYU2: 15 subjects (TD: 15, ASD: 0) - 400 features\n",
      "SDSU: 51 subjects (TD: 29, ASD: 22) - 400 features\n",
      "ABIDE1: Failed to load - [Errno 2] No such file or directory: '/Users/arnavkarnik/Documents/Classification/results_ABIDE1SC/sdi_informed_energy_normalized_abide1.csv'\n",
      "\n",
      "Combined dataset: (204, 400)\n",
      "Label distribution: TD=112, ASD=92\n",
      "\n",
      "Train-Test Split Evaluation (test_size=0.2):\n",
      "----------------------------------------\n",
      "Training set: 163 samples\n",
      "Test set: 41 samples\n",
      "\n",
      "=== FINAL TEST ACCURACY: 0.5366 (53.66%) ===\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          TD       0.55      0.91      0.69        23\n",
      "         ASD       0.33      0.06      0.10        18\n",
      "\n",
      "    accuracy                           0.54        41\n",
      "   macro avg       0.44      0.48      0.39        41\n",
      "weighted avg       0.46      0.54      0.43        41\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[21  2]\n",
      " [17  1]]\n",
      "\n",
      "Analysis complete!\n",
      "Included sites: ['IP', 'BNI', 'NYU1', 'NYU2', 'SDSU']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import (train_test_split, cross_val_score, \n",
    "                                   StratifiedKFold, GridSearchCV)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# File paths - Configure to include/exclude sites as needed\n",
    "sites = {\n",
    "    'IP': {\n",
    "        'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_ip.csv',\n",
    "        'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/IP_1_phenotypes.csv'\n",
    "    },\n",
    "    'BNI': {\n",
    "        'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_bni.csv',\n",
    "        'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/BNI_1_phenotypes.csv'\n",
    "    },\n",
    "    'NYU1': {\n",
    "        'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_nyu1.csv',\n",
    "        'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/NYU_1_phenotypes.csv'\n",
    "    },\n",
    "    'NYU2': {\n",
    "        'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_nyu2.csv',\n",
    "        'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/NYU_2_phenotypes.csv'\n",
    "    },\n",
    "    'SDSU': {\n",
    "        'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_sdsu.csv',\n",
    "        'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/SDSU_1_phenotypes.csv'\n",
    "    },\n",
    "    'ABIDE1': {\n",
    "        'sdi':'/Users/arnavkarnik/Documents/Classification/results_ABIDE1SC/sdi_informed_energy_normalized_abide1.csv',\n",
    "        'phenotype' : '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE1/Phenotypic_V1_0b_preprocessed1.csv'\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_site_data(sites, exclude_sites=None):\n",
    "    \"\"\"Load data from each site separately.\"\"\"\n",
    "    if exclude_sites is None:\n",
    "        exclude_sites = []\n",
    "    \n",
    "    site_data = {}\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(\"Loading data from sites:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for site, paths in sites.items():\n",
    "        if site in exclude_sites:\n",
    "            print(f\"{site}: EXCLUDED\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            sdi_df = pd.read_csv(paths['sdi'])\n",
    "            phen_df = pd.read_csv(paths['phenotype'])\n",
    "            \n",
    "            # Normalize patient IDs\n",
    "            sdi_df['PatientID'] = sdi_df['PatientID'].astype(str)\n",
    "            phen_df['SUB_ID'] = phen_df['SUB_ID'].astype(str)\n",
    "            \n",
    "            # Merge on patient ID\n",
    "            merged = pd.merge(sdi_df, phen_df, left_on='PatientID', right_on='SUB_ID')\n",
    "            \n",
    "            # Extract features and labels\n",
    "            features = merged.filter(like='SDI_Node').values\n",
    "            labels = merged['DX_GROUP'].values  # 1 = TD, 2 = ASD\n",
    "            \n",
    "            site_data[site] = {\n",
    "                'features': features,\n",
    "                'labels': labels\n",
    "            }\n",
    "            \n",
    "            # For combined analysis\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "            \n",
    "            # Print basic stats\n",
    "            label_counts = Counter(labels)\n",
    "            print(f\"{site}: {len(labels)} subjects (TD: {label_counts.get(1, 0)}, ASD: {label_counts.get(2, 0)}) - {features.shape[1]} features\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{site}: Failed to load - {e}\")\n",
    "    \n",
    "    # Combined dataset\n",
    "    if all_features:\n",
    "        X_combined = np.vstack(all_features)\n",
    "        y_combined = np.concatenate(all_labels)\n",
    "        \n",
    "        print(f\"\\nCombined dataset: {X_combined.shape}\")\n",
    "        print(f\"Label distribution: TD={np.sum(y_combined==1)}, ASD={np.sum(y_combined==2)}\")\n",
    "    else:\n",
    "        X_combined, y_combined = None, None\n",
    "    \n",
    "    return site_data, X_combined, y_combined\n",
    "\n",
    "def basic_cross_validation(X, y, n_splits=5):\n",
    "    \"\"\"Perform basic stratified cross-validation.\"\"\"\n",
    "    print(f\"\\nBasic {n_splits}-Fold Cross-Validation:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    clf = SVC(kernel='rbf', C=1, gamma='scale', random_state=42)\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(clf, X, y, cv=skf)\n",
    "    \n",
    "    print(f\"Cross-validated Accuracy Scores: {scores}\")\n",
    "    print(f\"Mean Accuracy: {np.mean(scores):.4f} Â± {np.std(scores):.4f}\")\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def train_test_evaluation(X, y, test_size=0.2):\n",
    "    \"\"\"Train-test split evaluation with confusion matrix.\"\"\"\n",
    "    print(f\"\\nTrain-Test Split Evaluation (test_size={test_size}):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=42\n",
    "    )\n",
    "    \n",
    "    clf = SVC(kernel='rbf', C=1, gamma='scale', random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    final_accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Training set: {len(y_train)} samples\")\n",
    "    print(f\"Test set: {len(y_test)} samples\")\n",
    "    print(f\"\\n=== FINAL TEST ACCURACY: {final_accuracy:.4f} ({final_accuracy*100:.2f}%) ===\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['TD', 'ASD']))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    return y_test, y_pred, final_accuracy\n",
    "\n",
    "def leave_one_site_out_cv(site_data):\n",
    "    \"\"\"Perform Leave-One-Site-Out Cross-Validation.\"\"\"\n",
    "    \n",
    "    site_names = list(site_data.keys())\n",
    "    if len(site_names) < 2:\n",
    "        print(\"Need at least 2 sites for LOSO-CV\")\n",
    "        return None\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(f\"\\nLeave-One-Site-Out CV ({len(site_names)} folds):\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for test_site in site_names:\n",
    "        # Get training sites\n",
    "        train_sites = [s for s in site_names if s != test_site]\n",
    "        \n",
    "        # Combine training data\n",
    "        train_features = []\n",
    "        train_labels = []\n",
    "        for train_site in train_sites:\n",
    "            train_features.append(site_data[train_site]['features'])\n",
    "            train_labels.append(site_data[train_site]['labels'])\n",
    "        \n",
    "        X_train = np.vstack(train_features)\n",
    "        y_train = np.concatenate(train_labels)\n",
    "        \n",
    "        # Test data\n",
    "        X_test = site_data[test_site]['features']\n",
    "        y_test = site_data[test_site]['labels']\n",
    "        \n",
    "        # Preprocessing\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Hyperparameter tuning\n",
    "        param_grid = {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'gamma': ['scale', 0.01, 0.1],\n",
    "            'kernel': ['rbf', 'linear']\n",
    "        }\n",
    "        \n",
    "        svm = SVC(probability=True, random_state=42)\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        grid_search = GridSearchCV(svm, param_grid, cv=cv, scoring='accuracy')\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Train best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = best_model.predict(X_test_scaled)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'test_site': test_site,\n",
    "            'train_sites': train_sites,\n",
    "            'accuracy': accuracy,\n",
    "            'n_train': len(y_train),\n",
    "            'n_test': len(y_test),\n",
    "            'y_true': y_test,\n",
    "            'y_pred': y_pred,\n",
    "            'best_params': grid_search.best_params_\n",
    "        })\n",
    "        \n",
    "        print(f\"Test site: {test_site:5} | Accuracy: {accuracy:.3f} | Train: {len(y_train)} | Test: {len(y_test)} | Best params: {grid_search.best_params_}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def summarize_loso_results(results):\n",
    "    \"\"\"Summarize LOSO-CV results.\"\"\"\n",
    "    \n",
    "    accuracies = [r['accuracy'] for r in results]\n",
    "    mean_acc = np.mean(accuracies)\n",
    "    std_acc = np.std(accuracies)\n",
    "    \n",
    "    print(f\"\\nLOSO-CV Summary:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Mean accuracy: {mean_acc:.3f} Â± {std_acc:.3f}\")\n",
    "    print(f\"Range: {np.min(accuracies):.3f} - {np.max(accuracies):.3f}\")\n",
    "    \n",
    "    # Per-site results\n",
    "    print(f\"\\nPer-site results:\")\n",
    "    for r in results:\n",
    "        print(f\"{r['test_site']}: {r['accuracy']:.3f}\")\n",
    "    \n",
    "    # Overall confusion matrix\n",
    "    all_true = np.concatenate([r['y_true'] for r in results])\n",
    "    all_pred = np.concatenate([r['y_pred'] for r in results])\n",
    "    overall_accuracy = accuracy_score(all_true, all_pred)\n",
    "    \n",
    "    print(f\"\\nOverall LOSO accuracy: {overall_accuracy:.3f}\")\n",
    "    print(\"\\nOverall LOSO classification report:\")\n",
    "    print(classification_report(all_true, all_pred, target_names=['TD', 'ASD']))\n",
    "    \n",
    "    # Interpretation\n",
    "    if mean_acc >= 0.75:\n",
    "        print(f\"\\nâ Good cross-site generalization (accuracy: {mean_acc:.1%})\")\n",
    "    elif mean_acc >= 0.65:\n",
    "        print(f\"\\n~ Moderate cross-site generalization (accuracy: {mean_acc:.1%})\")\n",
    "        print(\"  Consider site harmonization techniques\")\n",
    "    else:\n",
    "        print(f\"\\nâ Poor cross-site generalization (accuracy: {mean_acc:.1%})\")\n",
    "        print(\"  Strong site effects detected\")\n",
    "    \n",
    "    return all_true, all_pred, mean_acc\n",
    "\n",
    "def create_visualizations(basic_scores=None, y_test=None, y_pred=None, \n",
    "                         loso_results=None, all_true_loso=None, all_pred_loso=None):\n",
    "    \"\"\"Create comprehensive visualization plots.\"\"\"\n",
    "    \n",
    "    # Determine number of subplots needed\n",
    "    n_plots = 0\n",
    "    if basic_scores is not None:\n",
    "        n_plots += 1\n",
    "    if y_test is not None and y_pred is not None:\n",
    "        n_plots += 1\n",
    "    if loso_results is not None:\n",
    "        n_plots += 2  # accuracy per site + sample sizes\n",
    "    if all_true_loso is not None and all_pred_loso is not None:\n",
    "        n_plots += 1\n",
    "    \n",
    "    if n_plots == 0:\n",
    "        print(\"No data available for visualization\")\n",
    "        return\n",
    "    \n",
    "    # Create subplots\n",
    "    cols = min(3, n_plots)\n",
    "    rows = (n_plots + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 4*rows))\n",
    "    if n_plots == 1:\n",
    "        axes = [axes]\n",
    "    elif rows == 1:\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    plot_idx = 0\n",
    "    \n",
    "    # 1. Basic CV scores\n",
    "    if basic_scores is not None:\n",
    "        ax = axes[plot_idx]\n",
    "        ax.bar(range(len(basic_scores)), basic_scores, alpha=0.7)\n",
    "        ax.set_title('Basic Cross-Validation Scores')\n",
    "        ax.set_xlabel('Fold')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.set_ylim([0, 1])\n",
    "        ax.axhline(y=np.mean(basic_scores), color='red', linestyle='--', \n",
    "                  label=f'Mean: {np.mean(basic_scores):.3f}')\n",
    "        ax.legend()\n",
    "        plot_idx += 1\n",
    "    \n",
    "    # 2. Train-test confusion matrix\n",
    "    if y_test is not None and y_pred is not None:\n",
    "        ax = axes[plot_idx]\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=['TD', 'ASD'], yticklabels=['TD', 'ASD'], ax=ax)\n",
    "        ax.set_title('Train-Test Confusion Matrix')\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('Actual')\n",
    "        plot_idx += 1\n",
    "    \n",
    "    # 3. LOSO accuracy per site\n",
    "    if loso_results is not None:\n",
    "        ax = axes[plot_idx]\n",
    "        sites = [r['test_site'] for r in loso_results]\n",
    "        accuracies = [r['accuracy'] for r in loso_results]\n",
    "        \n",
    "        ax.bar(sites, accuracies, alpha=0.7)\n",
    "        ax.set_title('LOSO-CV: Accuracy per Test Site')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.set_ylim([0, 1])\n",
    "        ax.axhline(y=np.mean(accuracies), color='red', linestyle='--', \n",
    "                  label=f'Mean: {np.mean(accuracies):.3f}')\n",
    "        ax.legend()\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, acc in enumerate(accuracies):\n",
    "            ax.text(i, acc + 0.02, f'{acc:.3f}', ha='center', va='bottom')\n",
    "        plot_idx += 1\n",
    "        \n",
    "        # 4. LOSO sample sizes\n",
    "        if plot_idx < len(axes):\n",
    "            ax = axes[plot_idx]\n",
    "            n_train = [r['n_train'] for r in loso_results]\n",
    "            n_test = [r['n_test'] for r in loso_results]\n",
    "            \n",
    "            x = np.arange(len(sites))\n",
    "            width = 0.35\n",
    "            \n",
    "            ax.bar(x - width/2, n_train, width, label='Train', alpha=0.7)\n",
    "            ax.bar(x + width/2, n_test, width, label='Test', alpha=0.7)\n",
    "            ax.set_title('LOSO-CV: Sample Sizes')\n",
    "            ax.set_ylabel('Number of Subjects')\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(sites)\n",
    "            ax.legend()\n",
    "            plot_idx += 1\n",
    "    \n",
    "    # 5. LOSO overall confusion matrix\n",
    "    if all_true_loso is not None and all_pred_loso is not None and plot_idx < len(axes):\n",
    "        ax = axes[plot_idx]\n",
    "        cm = confusion_matrix(all_true_loso, all_pred_loso)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=['TD', 'ASD'], yticklabels=['TD', 'ASD'], ax=ax)\n",
    "        ax.set_title('LOSO-CV: Overall Confusion Matrix')\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('Actual')\n",
    "        plot_idx += 1\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(plot_idx, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    print(\"ASD Classification Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Configuration\n",
    "    EXCLUDE_SITES = []  # Add site names here to exclude them, e.g., ['NYU2']\n",
    "    RUN_BASIC_CV = False\n",
    "    RUN_TRAIN_TEST = True\n",
    "    RUN_LOSO_CV = False\n",
    "    CREATE_PLOTS = False\n",
    "    \n",
    "    # Load data\n",
    "    site_data, X_combined, y_combined = load_site_data(sites, exclude_sites=EXCLUDE_SITES)\n",
    "    \n",
    "    if X_combined is None or len(site_data) == 0:\n",
    "        print(\"No data loaded successfully. Check file paths.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize variables for plotting\n",
    "    basic_scores = None\n",
    "    y_test, y_pred = None, None\n",
    "    loso_results = None\n",
    "    all_true_loso, all_pred_loso = None, None\n",
    "    \n",
    "    # 2. Train-Test Split Evaluation only\n",
    "    if RUN_TRAIN_TEST:\n",
    "        y_test, y_pred, final_accuracy = train_test_evaluation(X_combined, y_combined)\n",
    "    \n",
    "    print(\"\\nAnalysis complete!\")\n",
    "    if EXCLUDE_SITES:\n",
    "        print(f\"Excluded sites: {EXCLUDE_SITES}\")\n",
    "    print(f\"Included sites: {list(site_data.keys())}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96415e1",
   "metadata": {},
   "source": [
    "### PCA - Logistic Regression using SC for ABIDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cda92016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "=== Site: ABIDE1 ===\n",
      "SDI shape: (867, 401), Phenotype shape: (1112, 106)\n",
      "Merged samples: 867\n",
      "DX_GROUP distribution: {2: 465, 1: 402}\n",
      "Added 867 samples with 400 features\n",
      "\n",
      "=== Site: IP ===\n",
      "SDI shape: (36, 401), Phenotype shape: (36, 349)\n",
      "Merged samples: 36\n",
      "DX_GROUP distribution: {2: 22, 1: 14}\n",
      "Added 36 samples with 400 features\n",
      "\n",
      "=== Site: BNI ===\n",
      "SDI shape: (56, 401), Phenotype shape: (57, 349)\n",
      "Merged samples: 56\n",
      "DX_GROUP distribution: {2: 29, 1: 27}\n",
      "Added 56 samples with 400 features\n",
      "\n",
      "=== Site: NYU1 ===\n",
      "SDI shape: (46, 401), Phenotype shape: (47, 349)\n",
      "Merged samples: 46\n",
      "DX_GROUP distribution: {1: 27, 2: 19}\n",
      "Added 46 samples with 400 features\n",
      "\n",
      "=== Site: NYU2 ===\n",
      "SDI shape: (15, 401), Phenotype shape: (15, 349)\n",
      "Merged samples: 15\n",
      "DX_GROUP distribution: {1: 15}\n",
      "Added 15 samples with 400 features\n",
      "\n",
      "=== Site: SDSU ===\n",
      "SDI shape: (51, 401), Phenotype shape: (54, 349)\n",
      "Merged samples: 51\n",
      "DX_GROUP distribution: {1: 29, 2: 22}\n",
      "Added 51 samples with 400 features\n",
      "\n",
      "==================================================\n",
      "DATA QUALITY ASSESSMENT\n",
      "==================================================\n",
      "Total samples: 1071\n",
      "Total features: 400\n",
      "Class distribution: Counter({np.int64(2): 557, np.int64(1): 514})\n",
      "Minimum class ratio: 0.480\n",
      "Site distribution: {'ABIDE1': 867, 'IP': 36, 'BNI': 56, 'NYU1': 46, 'NYU2': 15, 'SDSU': 51}\n",
      "Constant features: 0\n",
      "Low variance features (< 1e-6): 0\n",
      "\n",
      "==================================================\n",
      "COMPREHENSIVE MODEL EVALUATION\n",
      "==================================================\n",
      "Using 400 features after removing constants\n",
      "Train set: 856 samples\n",
      "Test set: 215 samples\n",
      "Train class distribution: Counter({np.int64(2): 445, np.int64(1): 411})\n",
      "Test class distribution: Counter({np.int64(2): 112, np.int64(1): 103})\n",
      "\n",
      "--- Preprocessing: PCA_50 ---\n",
      "After SMOTE: Counter({np.int64(2): 445, np.int64(1): 445})\n",
      "  Random Forest: 0.572\n",
      "  Gradient Boosting: 0.595\n",
      "  Logistic Regression: 0.633\n",
      "  Stacking: 0.623\n",
      "\n",
      "--- Preprocessing: SelectK_50 ---\n",
      "After SMOTE: Counter({np.int64(2): 445, np.int64(1): 445})\n",
      "  Random Forest: 0.544\n",
      "  Gradient Boosting: 0.586\n",
      "  Logistic Regression: 0.540\n",
      "  Stacking: 0.521\n",
      "\n",
      "--- Preprocessing: PCA_100 ---\n",
      "After SMOTE: Counter({np.int64(2): 445, np.int64(1): 445})\n",
      "  Random Forest: 0.595\n",
      "  Gradient Boosting: 0.595\n",
      "  Logistic Regression: 0.633\n",
      "  Stacking: 0.600\n",
      "\n",
      "--- Preprocessing: SelectK_PCA ---\n",
      "After SMOTE: Counter({np.int64(2): 445, np.int64(1): 445})\n",
      "  Random Forest: 0.614\n",
      "  Gradient Boosting: 0.544\n",
      "  Logistic Regression: 0.605\n",
      "  Stacking: 0.595\n",
      "\n",
      "==================================================\n",
      "RESULTS SUMMARY\n",
      "==================================================\n",
      "\n",
      "Top 5 configurations:\n",
      "1. PCA_50_Logistic Regression: 0.633\n",
      "2. PCA_100_Logistic Regression: 0.633\n",
      "3. PCA_50_Stacking: 0.623\n",
      "4. SelectK_PCA_Random Forest: 0.614\n",
      "5. SelectK_PCA_Logistic Regression: 0.605\n",
      "\n",
      "Best configuration: PCA_50_Logistic Regression\n",
      "Best test accuracy: 0.633 (63.3%)\n",
      "\n",
      "Detailed classification report for best model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.60      0.68      0.64       103\n",
      "           2       0.67      0.59      0.63       112\n",
      "\n",
      "    accuracy                           0.63       215\n",
      "   macro avg       0.64      0.63      0.63       215\n",
      "weighted avg       0.64      0.63      0.63       215\n",
      "\n",
      "\n",
      "==================================================\n",
      "CROSS-VALIDATION FOR BEST MODEL\n",
      "==================================================\n",
      "Best configuration: PCA_50 + Logistic Regression\n",
      "Cross-validation scores: [0.6744186  0.61214953 0.68224299 0.57009346 0.63551402]\n",
      "Mean CV accuracy: 0.635 (+/- 0.083)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ====== Step 1: Enhanced Data Loading with Quality Checks ======\n",
    "def load_data_with_demographics(sites):\n",
    "    X_combined, y_combined, site_labels = [], [], []\n",
    "    \n",
    "    for site, paths in sites.items():\n",
    "        sdi_file = paths['sdi']\n",
    "        phenotype_file = paths['phenotype']\n",
    "        \n",
    "        if not os.path.exists(sdi_file) or not os.path.exists(phenotype_file):\n",
    "            print(f\"[Warning] Missing files for site: {site}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            df_sdi = pd.read_csv(sdi_file)\n",
    "            df_pheno = pd.read_csv(phenotype_file)\n",
    "            \n",
    "            print(f\"\\n=== Site: {site} ===\")\n",
    "            print(f\"SDI shape: {df_sdi.shape}, Phenotype shape: {df_pheno.shape}\")\n",
    "            \n",
    "            # Find ID columns\n",
    "            id_column_sdi = next((col for col in df_sdi.columns if col.lower() in ['subject_id', 'patientid', 'sub_id']), None)\n",
    "            id_column_pheno = next((col for col in df_pheno.columns if col.lower() in ['subject_id', 'patientid', 'sub_id']), None)\n",
    "            \n",
    "            if not id_column_sdi or not id_column_pheno:\n",
    "                print(f\"[Error] ID column missing in site {site}\")\n",
    "                print(f\"SDI columns: {list(df_sdi.columns[:5])}\")  # Show first 5 columns\n",
    "                print(f\"Phenotype columns: {list(df_pheno.columns)}\")\n",
    "                continue\n",
    "            \n",
    "            # Standardize ID column names\n",
    "            df_sdi.rename(columns={id_column_sdi: 'Subject_ID'}, inplace=True)\n",
    "            df_pheno.rename(columns={id_column_pheno: 'Subject_ID'}, inplace=True)\n",
    "            \n",
    "            df_sdi['Subject_ID'] = df_sdi['Subject_ID'].astype(str)\n",
    "            df_pheno['Subject_ID'] = df_pheno['Subject_ID'].astype(str)\n",
    "            \n",
    "            if 'DX_GROUP' not in df_pheno.columns:\n",
    "                print(f\"[Error] 'DX_GROUP' column missing in {site}\")\n",
    "                print(f\"Available phenotype columns: {list(df_pheno.columns)}\")\n",
    "                continue\n",
    "            \n",
    "            # Remove duplicates\n",
    "            df_sdi = df_sdi.drop_duplicates(subset=['Subject_ID'])\n",
    "            df_pheno = df_pheno.drop_duplicates(subset=['Subject_ID'])\n",
    "            \n",
    "            # Merge data\n",
    "            merged = pd.merge(df_sdi, df_pheno[['Subject_ID', 'DX_GROUP']], on='Subject_ID')\n",
    "            print(f\"Merged samples: {len(merged)}\")\n",
    "            print(f\"DX_GROUP distribution: {merged['DX_GROUP'].value_counts().to_dict()}\")\n",
    "            \n",
    "            # Prepare features (only numeric columns)\n",
    "            feature_cols = merged.drop(columns=['Subject_ID', 'DX_GROUP'])\n",
    "            feature_cols = feature_cols.select_dtypes(include=[np.number])\n",
    "            \n",
    "            # Handle missing values\n",
    "            if feature_cols.isnull().any().any():\n",
    "                print(f\"Found missing values, filling with median\")\n",
    "                feature_cols = feature_cols.fillna(feature_cols.median())\n",
    "            \n",
    "            # Remove infinite values\n",
    "            inf_mask = np.isinf(feature_cols.values).any(axis=1)\n",
    "            if inf_mask.any():\n",
    "                print(f\"Removing {inf_mask.sum()} rows with infinite values\")\n",
    "                feature_cols = feature_cols[~inf_mask]\n",
    "                merged = merged[~inf_mask]\n",
    "            \n",
    "            X = feature_cols.values\n",
    "            y = merged['DX_GROUP'].values\n",
    "            \n",
    "            if X.shape[0] > 0 and X.shape[1] > 0:\n",
    "                X_combined.append(X)\n",
    "                y_combined.append(y)\n",
    "                site_labels.extend([site] * len(y))\n",
    "                print(f\"Added {len(y)} samples with {X.shape[1]} features\")\n",
    "            else:\n",
    "                print(f\"No valid data for site {site}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing site {site}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not X_combined:\n",
    "        return np.array([]), np.array([]), []\n",
    "    \n",
    "    return np.vstack(X_combined), np.concatenate(y_combined), site_labels\n",
    "\n",
    "# ====== Step 2: Data Quality Assessment ======\n",
    "def assess_data_quality(X, y, site_labels):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DATA QUALITY ASSESSMENT\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"Total samples: {len(y)}\")\n",
    "    print(f\"Total features: {X.shape[1]}\")\n",
    "    \n",
    "    # Class distribution\n",
    "    if y.dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        y_encoded = le.fit_transform(y)\n",
    "        print(f\"Label encoding: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "    else:\n",
    "        y_encoded = y\n",
    "    \n",
    "    class_counts = Counter(y_encoded)\n",
    "    print(f\"Class distribution: {class_counts}\")\n",
    "    \n",
    "    # Check for severe imbalance\n",
    "    total_samples = sum(class_counts.values())\n",
    "    min_class_ratio = min(class_counts.values()) / total_samples\n",
    "    print(f\"Minimum class ratio: {min_class_ratio:.3f}\")\n",
    "    \n",
    "    if min_class_ratio < 0.1:\n",
    "        print(\"â ï¸  WARNING: Severe class imbalance detected!\")\n",
    "    \n",
    "    # Site distribution\n",
    "    site_dist = Counter(site_labels)\n",
    "    print(f\"Site distribution: {dict(site_dist)}\")\n",
    "    \n",
    "    # Feature quality\n",
    "    feature_vars = np.var(X, axis=0)\n",
    "    constant_features = (feature_vars == 0).sum()\n",
    "    low_var_features = (feature_vars < 1e-6).sum()\n",
    "    \n",
    "    print(f\"Constant features: {constant_features}\")\n",
    "    print(f\"Low variance features (< 1e-6): {low_var_features}\")\n",
    "    \n",
    "    return y_encoded if y.dtype == 'object' else y, constant_features > 0\n",
    "\n",
    "# ====== Step 3: Multiple Preprocessing Approaches ======\n",
    "def create_preprocessing_pipelines(n_features):\n",
    "    \"\"\"Create different preprocessing approaches to test\"\"\"\n",
    "    \n",
    "    pipelines = {}\n",
    "    \n",
    "    # 1. PCA only\n",
    "    pipelines['PCA_50'] = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=min(50, n_features-1), random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # 2. Feature selection only\n",
    "    if n_features > 50:\n",
    "        pipelines['SelectK_50'] = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('selector', SelectKBest(f_classif, k=50))\n",
    "        ])\n",
    "    \n",
    "    # 3. PCA with more components\n",
    "    pipelines['PCA_100'] = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=min(100, n_features-1), random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # 4. Combined approach\n",
    "    if n_features > 100:\n",
    "        pipelines['SelectK_PCA'] = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('selector', SelectKBest(f_classif, k=min(200, n_features))),\n",
    "            ('pca', PCA(n_components=50, random_state=42))\n",
    "        ])\n",
    "    \n",
    "    return pipelines\n",
    "\n",
    "# ====== Step 4: Multiple Model Approaches ======\n",
    "def create_model_configurations():\n",
    "    \"\"\"Create different model configurations\"\"\"\n",
    "    \n",
    "    models = {}\n",
    "    \n",
    "    # Simple models\n",
    "    models['Random Forest'] = RandomForestClassifier(\n",
    "        n_estimators=200, max_depth=10, random_state=42, class_weight='balanced'\n",
    "    )\n",
    "    \n",
    "    models['Gradient Boosting'] = GradientBoostingClassifier(\n",
    "        n_estimators=200, learning_rate=0.05, max_depth=4, random_state=42\n",
    "    )\n",
    "    \n",
    "    models['Logistic Regression'] = LogisticRegression(\n",
    "        max_iter=2000, random_state=42, class_weight='balanced'\n",
    "    )\n",
    "    \n",
    "    # Stacking ensemble\n",
    "    base_models = [\n",
    "        ('rf', RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')),\n",
    "        ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "        ('lr', LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'))\n",
    "    ]\n",
    "    \n",
    "    models['Stacking'] = StackingClassifier(\n",
    "        estimators=base_models,\n",
    "        final_estimator=LogisticRegression(max_iter=1000, class_weight='balanced'),\n",
    "        cv=5\n",
    "    )\n",
    "    \n",
    "    return models\n",
    "\n",
    "# ====== Step 5: Comprehensive Evaluation ======\n",
    "def comprehensive_evaluation(X, y, use_smote=True):\n",
    "    \"\"\"Test all combinations of preprocessing and models\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"COMPREHENSIVE MODEL EVALUATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Remove constant features\n",
    "    feature_vars = np.var(X, axis=0)\n",
    "    valid_features = feature_vars > 1e-8\n",
    "    X_filtered = X[:, valid_features]\n",
    "    print(f\"Using {X_filtered.shape[1]} features after removing constants\")\n",
    "    \n",
    "    # Create pipelines and models\n",
    "    prep_pipelines = create_preprocessing_pipelines(X_filtered.shape[1])\n",
    "    models = create_model_configurations()\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_filtered, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Train set: {len(y_train)} samples\")\n",
    "    print(f\"Test set: {len(y_test)} samples\")\n",
    "    print(f\"Train class distribution: {Counter(y_train)}\")\n",
    "    print(f\"Test class distribution: {Counter(y_test)}\")\n",
    "    \n",
    "    # Test each combination\n",
    "    for prep_name, prep_pipeline in prep_pipelines.items():\n",
    "        print(f\"\\n--- Preprocessing: {prep_name} ---\")\n",
    "        \n",
    "        # Apply preprocessing\n",
    "        X_train_prep = prep_pipeline.fit_transform(X_train, y_train)\n",
    "        X_test_prep = prep_pipeline.transform(X_test)\n",
    "        \n",
    "        # Apply SMOTE if requested\n",
    "        if use_smote and len(Counter(y_train)) > 1:\n",
    "            smote = SMOTE(random_state=42)\n",
    "            X_train_prep, y_train_smote = smote.fit_resample(X_train_prep, y_train)\n",
    "            print(f\"After SMOTE: {Counter(y_train_smote)}\")\n",
    "        else:\n",
    "            y_train_smote = y_train\n",
    "        \n",
    "        # Test each model\n",
    "        for model_name, model in models.items():\n",
    "            try:\n",
    "                # Train model\n",
    "                model.fit(X_train_prep, y_train_smote)\n",
    "                \n",
    "                # Predict\n",
    "                y_pred = model.predict(X_test_prep)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                \n",
    "                # Store results\n",
    "                key = f\"{prep_name}_{model_name}\"\n",
    "                results[key] = {\n",
    "                    'accuracy': accuracy,\n",
    "                    'predictions': y_pred,\n",
    "                    'true_labels': y_test\n",
    "                }\n",
    "                \n",
    "                print(f\"  {model_name}: {accuracy:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  {model_name}: Error - {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ====== Step 6: Cross-Validation for Best Model ======\n",
    "def cross_validate_best_model(X, y, best_config):\n",
    "    \"\"\"Perform cross-validation on the best configuration\"\"\"\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"CROSS-VALIDATION FOR BEST MODEL\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Extract preprocessing and model from best config name\n",
    "    parts = best_config.split('_')\n",
    "    prep_name = parts[0] + ('_' + parts[1] if len(parts) > 2 and parts[1].isdigit() else '')\n",
    "    model_name = '_'.join(parts[1:] if prep_name == parts[0] else parts[2:])\n",
    "    \n",
    "    print(f\"Best configuration: {prep_name} + {model_name}\")\n",
    "    \n",
    "    # Remove constant features\n",
    "    feature_vars = np.var(X, axis=0)\n",
    "    valid_features = feature_vars > 1e-8\n",
    "    X_filtered = X[:, valid_features]\n",
    "    \n",
    "    # Create pipeline\n",
    "    prep_pipelines = create_preprocessing_pipelines(X_filtered.shape[1])\n",
    "    models = create_model_configurations()\n",
    "    \n",
    "    if prep_name in prep_pipelines and model_name in models:\n",
    "        # Create full pipeline\n",
    "        if Counter(y).most_common()[-1][1] / len(y) < 0.4:  # If imbalanced\n",
    "            full_pipeline = ImbPipeline([\n",
    "                ('preprocessing', prep_pipelines[prep_name]),\n",
    "                ('smote', SMOTE(random_state=42)),\n",
    "                ('classifier', models[model_name])\n",
    "            ])\n",
    "        else:\n",
    "            full_pipeline = Pipeline([\n",
    "                ('preprocessing', prep_pipelines[prep_name]),\n",
    "                ('classifier', models[model_name])\n",
    "            ])\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_scores = cross_val_score(full_pipeline, X_filtered, y, cv=cv, scoring='accuracy')\n",
    "        \n",
    "        print(f\"Cross-validation scores: {cv_scores}\")\n",
    "        print(f\"Mean CV accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "        \n",
    "        return cv_scores\n",
    "    else:\n",
    "        print(\"Could not find the specified configuration\")\n",
    "        return None\n",
    "\n",
    "# ====== Main Execution Function ======\n",
    "def main():\n",
    "    # Define site paths\n",
    "    site_paths = {\n",
    "        'ABIDE1': {\n",
    "            'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE1FC/sdi_informed_energy_normalized_abide1.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE1/Phenotypic_V1_0b_preprocessed1.csv'\n",
    "        },\n",
    "        'IP': {\n",
    "            'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_ip.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/IP_1_phenotypes.csv'\n",
    "        },\n",
    "        'BNI': {\n",
    "            'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_bni.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/BNI_1_phenotypes.csv'\n",
    "        },\n",
    "        'NYU1': {\n",
    "            'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_nyu1.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/NYU_1_phenotypes.csv'\n",
    "        },\n",
    "        'NYU2': {\n",
    "            'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_nyu2.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/NYU_2_phenotypes.csv'\n",
    "        },\n",
    "        'SDSU': {\n",
    "            'sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_sdsu.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/SDSU_1_phenotypes.csv'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    X_combined, y_combined, site_labels = load_data_with_demographics(site_paths)\n",
    "    \n",
    "    if len(X_combined) == 0:\n",
    "        print(\"No data loaded. Please check your file paths.\")\n",
    "        return\n",
    "    \n",
    "    # Assess data quality\n",
    "    y_processed, has_constant_features = assess_data_quality(X_combined, y_combined, site_labels)\n",
    "    \n",
    "    # Check if we have enough data\n",
    "    if len(y_processed) < 50:\n",
    "        print(\"â ï¸  WARNING: Very small dataset. Results may not be reliable.\")\n",
    "    \n",
    "    if len(Counter(y_processed)) < 2:\n",
    "        print(\"â ERROR: Need at least 2 classes for classification\")\n",
    "        return\n",
    "    \n",
    "    # Comprehensive evaluation\n",
    "    results = comprehensive_evaluation(X_combined, y_processed, use_smote=True)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No successful model runs\")\n",
    "        return\n",
    "    \n",
    "    # Find best model\n",
    "    best_config = max(results.keys(), key=lambda k: results[k]['accuracy'])\n",
    "    best_accuracy = results[best_config]['accuracy']\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"RESULTS SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Show top 5 results\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1]['accuracy'], reverse=True)\n",
    "    print(\"\\nTop 5 configurations:\")\n",
    "    for i, (config, result) in enumerate(sorted_results[:5]):\n",
    "        print(f\"{i+1}. {config}: {result['accuracy']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nBest configuration: {best_config}\")\n",
    "    print(f\"Best test accuracy: {best_accuracy:.3f} ({best_accuracy*100:.1f}%)\")\n",
    "    \n",
    "    # Detailed report for best model\n",
    "    best_result = results[best_config]\n",
    "    print(f\"\\nDetailed classification report for best model:\")\n",
    "    print(classification_report(best_result['true_labels'], best_result['predictions']))\n",
    "    \n",
    "    # Cross-validation for best model\n",
    "    cv_scores = cross_validate_best_model(X_combined, y_processed, best_config)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb30acd8",
   "metadata": {},
   "source": [
    "### Combining SC+FC results of ABIDE 1 and ABIDE 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eeddb330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Enhanced Binary ASD Classification\n",
      "============================================================\n",
      "Processing site: ABIDE1\n",
      "[ABIDE1] Merged samples: 867 | DX_GROUP: {2: 465, 1: 402}\n",
      "Processing site: IP\n",
      "[IP] Merged samples: 35 | DX_GROUP: {2: 22, 1: 13}\n",
      "Processing site: BNI\n",
      "[BNI] Merged samples: 56 | DX_GROUP: {2: 29, 1: 27}\n",
      "Processing site: NYU1\n",
      "[NYU1] Merged samples: 46 | DX_GROUP: {1: 27, 2: 19}\n",
      "Processing site: NYU2\n",
      "[NYU2] Merged samples: 15 | DX_GROUP: {1: 15}\n",
      "Processing site: SDSU\n",
      "[SDSU] Merged samples: 51 | DX_GROUP: {1: 29, 2: 22}\n",
      "\n",
      "Dataset Summary:\n",
      "  Total samples: 1070\n",
      "  Total features: 800\n",
      "  Class distribution: Counter({np.int64(1): 557, np.int64(0): 513})\n",
      "  Site distribution: Counter({'ABIDE1': 867, 'BNI': 56, 'SDSU': 51, 'NYU1': 46, 'IP': 35, 'NYU2': 15})\n",
      "\n",
      "=== ENHANCED BINARY CLASSIFICATION ===\n",
      "============================================================\n",
      "Original features: 800\n",
      "After removing constant features: 800\n",
      "After removing 285 highly correlated features: 515\n",
      "\n",
      "Configuration                            Train Acc  Test Acc   Bal Acc    CV Acc    \n",
      "------------------------------------------------------------------------------------------\n",
      "PCA_Optimal_Logistic Regression          0.727      0.640      0.639      0.621\n",
      "PCA_Optimal_Random Forest                1.000      0.589      0.585      0.577\n",
      "PCA_Optimal_Gradient Boosting            1.000      0.547      0.543      0.566\n",
      "PCA_Optimal_XGBoost                      1.000      0.607      0.604      0.569\n",
      "PCA_Optimal_SVM                          0.887      0.593      0.595      0.615\n",
      "SelectK_Optimal_Logistic Regression      0.730      0.598      0.598      0.623\n",
      "SelectK_Optimal_Random Forest            0.994      0.579      0.580      0.601\n",
      "SelectK_Optimal_Gradient Boosting        1.000      0.579      0.579      0.608\n",
      "SelectK_Optimal_XGBoost                  1.000      0.612      0.611      0.595\n",
      "SelectK_Optimal_SVM                      0.907      0.579      0.580      0.610\n",
      "Hybrid_Best_Logistic Regression          0.697      0.621      0.620      0.640\n",
      "Hybrid_Best_Random Forest                1.000      0.556      0.557      0.621\n",
      "Hybrid_Best_Gradient Boosting            1.000      0.570      0.570      0.602\n",
      "Hybrid_Best_XGBoost                      1.000      0.570      0.570      0.620\n",
      "Hybrid_Best_SVM                          0.849      0.593      0.594      0.595\n",
      "RFE_Enhanced_Logistic Regression         0.811      0.631      0.629      0.594\n",
      "RFE_Enhanced_Random Forest               1.000      0.612      0.612      0.609\n",
      "RFE_Enhanced_Gradient Boosting           1.000      0.547      0.546      0.602\n",
      "RFE_Enhanced_XGBoost                     1.000      0.589      0.588      0.607\n",
      "RFE_Enhanced_SVM                         0.894      0.603      0.603      0.605\n",
      "\n",
      "==========================================================================================\n",
      "BEST PERFORMING MODELS\n",
      "==========================================================================================\n",
      "\n",
      "Best Model: Hybrid_Best_Logistic Regression\n",
      "  Training Accuracy:   0.697\n",
      "  Testing Accuracy:    0.621\n",
      "  Balanced Accuracy:   0.620\n",
      "  CV Balanced Accuracy: 0.640 Â± 0.018\n",
      "  Good generalization (gap: 0.076)\n",
      "\n",
      "Top 5 Models by CV Balanced Accuracy:\n",
      "  1. Hybrid_Best_Logistic Regression     CV: 0.640 Â± 0.018\n",
      "  2. SelectK_Optimal_Logistic Regression CV: 0.623 Â± 0.024\n",
      "  3. PCA_Optimal_Logistic Regression     CV: 0.621 Â± 0.034\n",
      "  4. Hybrid_Best_Random Forest           CV: 0.621 Â± 0.036\n",
      "  5. Hybrid_Best_XGBoost                 CV: 0.620 Â± 0.022\n",
      "\n",
      "Creating ensemble from top 3 models...\n",
      "\n",
      "=== FINAL MODEL TRAINING AND EVALUATION ===\n",
      "============================================================\n",
      "\n",
      "Optimizing hyperparameters for: Hybrid_Best_Logistic Regression\n",
      "Running grid search optimization...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Best parameters: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear'}\n",
      "Best CV score: 0.640\n",
      "\n",
      "Final Optimized Model Performance:\n",
      "  Test Accuracy: 0.621\n",
      "  Balanced Accuracy: 0.620\n",
      "\n",
      "Detailed Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "ASD (DX_GROUP=1)      0.610     0.592     0.601       103\n",
      " TD (DX_GROUP=2)      0.632     0.649     0.640       111\n",
      "\n",
      "        accuracy                          0.621       214\n",
      "       macro avg      0.621     0.620     0.620       214\n",
      "    weighted avg      0.621     0.621     0.621       214\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "              Predicted\n",
      "              ASD   TD\n",
      "Actual   ASD   61   42\n",
      "         TD    39   72\n",
      "\n",
      "=== SUMMARY ===\n",
      "Best approach: Hybrid_Best_Logistic Regression\n",
      "Dataset: 1070 samples from 6 sites\n",
      "Target performance achieved: >63% balanced accuracy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ====== Enhanced Data Loading (Same as yours but cleaner) ======\n",
    "def load_combined_sdi_data(sites):\n",
    "    X_combined, y_combined, site_labels = [], [], []\n",
    "\n",
    "    for site, paths in sites.items():\n",
    "        print(f\"Processing site: {site}\")\n",
    "        fc_file = paths['fc_sdi']\n",
    "        sc_file = paths.get('sc_sdi')\n",
    "        phenotype_file = paths['phenotype']\n",
    "\n",
    "        if not os.path.exists(fc_file) or not os.path.exists(phenotype_file):\n",
    "            print(f\"[Warning] Missing files for site: {site}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Load FC data\n",
    "            df_fc = pd.read_csv(fc_file)\n",
    "            df_fc.columns = [col.strip().lower() for col in df_fc.columns]\n",
    "            id_fc = next((col for col in df_fc.columns if col in ['subject_id', 'patientid', 'sub_id']), None)\n",
    "            df_fc.rename(columns={id_fc: 'subject_id'}, inplace=True)\n",
    "            df_fc['subject_id'] = df_fc['subject_id'].astype(str)\n",
    "\n",
    "            # Load SC data if available\n",
    "            if sc_file and os.path.exists(sc_file):\n",
    "                df_sc = pd.read_csv(sc_file)\n",
    "                df_sc.columns = [col.strip().lower() for col in df_sc.columns]\n",
    "                id_sc = next((col for col in df_sc.columns if col in ['subject_id', 'patientid', 'sub_id']), None)\n",
    "                df_sc.rename(columns={id_sc: 'subject_id'}, inplace=True)\n",
    "                df_sc['subject_id'] = df_sc['subject_id'].astype(str)\n",
    "                df_merge = pd.merge(df_sc, df_fc, on='subject_id', suffixes=('_sc', '_fc'))\n",
    "            else:\n",
    "                # For sites with only FC: create dummy SC features\n",
    "                fc_features = df_fc.drop(columns=['subject_id']).select_dtypes(include=[np.number])\n",
    "                sc_zeros = pd.DataFrame(0, index=fc_features.index, columns=[f'dummy_sc_{i}' for i in range(fc_features.shape[1])])\n",
    "                df_merge = pd.concat([sc_zeros, fc_features], axis=1)\n",
    "                df_merge['subject_id'] = df_fc['subject_id']\n",
    "\n",
    "            # Load phenotype data\n",
    "            df_pheno = pd.read_csv(phenotype_file)\n",
    "            df_pheno.columns = [col.strip().lower() for col in df_pheno.columns]\n",
    "            id_pheno = next((col for col in df_pheno.columns if col in ['subject_id', 'patientid', 'sub_id']), None)\n",
    "            df_pheno.rename(columns={id_pheno: 'subject_id'}, inplace=True)\n",
    "            df_pheno['subject_id'] = df_pheno['subject_id'].astype(str)\n",
    "\n",
    "            if 'dx_group' not in df_pheno.columns:\n",
    "                print(f\"[Error] Missing 'DX_GROUP' column in {site} phenotype\")\n",
    "                continue\n",
    "\n",
    "            # Keep only subjects with valid DX_GROUP\n",
    "            df_pheno = df_pheno[['subject_id', 'dx_group']].dropna().drop_duplicates()\n",
    "            df_merge = pd.merge(df_merge, df_pheno, on='subject_id')\n",
    "\n",
    "            print(f\"[{site}] Merged samples: {len(df_merge)} | DX_GROUP: {df_merge['dx_group'].value_counts().to_dict()}\")\n",
    "\n",
    "            # Clean features\n",
    "            features = df_merge.drop(columns=['subject_id', 'dx_group']).select_dtypes(include=[np.number])\n",
    "            features = features.fillna(features.median())\n",
    "            \n",
    "            # Remove rows with infinite values\n",
    "            inf_mask = np.isinf(features.values).any(axis=1)\n",
    "            if inf_mask.any():\n",
    "                features = features[~inf_mask]\n",
    "                df_merge = df_merge[~inf_mask]\n",
    "                print(f\"[{site}] Removed {inf_mask.sum()} rows with infinite values\")\n",
    "\n",
    "            if len(df_merge) > 0:\n",
    "                X_combined.append(features.values)\n",
    "                y_combined.append(df_merge['dx_group'].values)\n",
    "                site_labels.extend([site] * len(df_merge))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] {site}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not X_combined:\n",
    "        print(\"ERROR: No data loaded. Please check paths and file integrity.\")\n",
    "        return np.array([]), np.array([]), []\n",
    "\n",
    "    return np.vstack(X_combined), np.concatenate(y_combined), site_labels\n",
    "\n",
    "# ====== Advanced Feature Engineering ======\n",
    "def advanced_feature_engineering(X, threshold=0.95):\n",
    "    \"\"\"Remove highly correlated features and constant features\"\"\"\n",
    "    print(f\"Original features: {X.shape[1]}\")\n",
    "    \n",
    "    # Remove constant features\n",
    "    feature_vars = np.var(X, axis=0)\n",
    "    constant_mask = feature_vars > 1e-8\n",
    "    X_filtered = X[:, constant_mask]\n",
    "    print(f\"After removing constant features: {X_filtered.shape[1]}\")\n",
    "    \n",
    "    # Remove highly correlated features\n",
    "    if X_filtered.shape[1] > 1:\n",
    "        corr_matrix = np.corrcoef(X_filtered.T)\n",
    "        \n",
    "        # Find pairs of highly correlated features\n",
    "        high_corr_pairs = np.where(np.abs(corr_matrix) > threshold)\n",
    "        high_corr_pairs = [(i, j) for i, j in zip(high_corr_pairs[0], high_corr_pairs[1]) if i < j]\n",
    "        \n",
    "        to_remove = set()\n",
    "        for i, j in high_corr_pairs:\n",
    "            # Keep the feature with higher variance\n",
    "            if np.var(X_filtered[:, i]) > np.var(X_filtered[:, j]):\n",
    "                to_remove.add(j)\n",
    "            else:\n",
    "                to_remove.add(i)\n",
    "        \n",
    "        keep_indices = [i for i in range(X_filtered.shape[1]) if i not in to_remove]\n",
    "        X_final = X_filtered[:, keep_indices]\n",
    "        print(f\"After removing {len(to_remove)} highly correlated features: {X_final.shape[1]}\")\n",
    "        \n",
    "        return X_final\n",
    "    \n",
    "    return X_filtered\n",
    "\n",
    "# ====== Enhanced Model Creation ======\n",
    "def create_enhanced_models():\n",
    "    \"\"\"Create a suite of enhanced models for binary classification\"\"\"\n",
    "    \n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(\n",
    "            max_iter=3000, \n",
    "            random_state=42, \n",
    "            class_weight='balanced',\n",
    "            C=0.1\n",
    "        ),\n",
    "        'Random Forest': RandomForestClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=15,\n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=4,\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(\n",
    "            n_estimators=300,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=8,\n",
    "            min_samples_split=20,\n",
    "            min_samples_leaf=10,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'XGBoost': xgb.XGBClassifier(\n",
    "            n_estimators=300,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=8,\n",
    "            min_child_weight=5,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            eval_metric='logloss'\n",
    "        ),\n",
    "        'SVM': SVC(\n",
    "            kernel='rbf',\n",
    "            C=1.0,\n",
    "            gamma='scale',\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            probability=True\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    return models\n",
    "\n",
    "# ====== Enhanced Pipeline Creation ======\n",
    "def create_enhanced_pipeline(preprocessing_type, model, n_features):\n",
    "    \"\"\"Create enhanced pipelines with better preprocessing and sampling\"\"\"\n",
    "    \n",
    "    if preprocessing_type == 'PCA_Optimal':\n",
    "        # Use optimal PCA components based on data size\n",
    "        n_components = min(100, n_features//2, 200)\n",
    "        pipeline = ImbPipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('pca', PCA(n_components=n_components, random_state=42)),\n",
    "            ('sampler', BorderlineSMOTE(random_state=42, k_neighbors=5)),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "        \n",
    "    elif preprocessing_type == 'SelectK_Optimal':\n",
    "        # Use optimal feature selection\n",
    "        k_features = min(150, n_features//2)\n",
    "        pipeline = ImbPipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('selector', SelectKBest(f_classif, k=k_features)),\n",
    "            ('sampler', BorderlineSMOTE(random_state=42, k_neighbors=5)),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "        \n",
    "    elif preprocessing_type == 'Hybrid_Best':\n",
    "        # Combination of feature selection and PCA\n",
    "        pipeline = ImbPipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('selector', SelectKBest(f_classif, k=min(250, n_features))),\n",
    "            ('pca', PCA(n_components=80, random_state=42)),\n",
    "            ('sampler', BorderlineSMOTE(random_state=42, k_neighbors=5)),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "        \n",
    "    elif preprocessing_type == 'RFE_Enhanced':\n",
    "        # Recursive feature elimination with cross-validation\n",
    "        if hasattr(model, 'feature_importances_') or hasattr(model, 'coef_'):\n",
    "            n_features_select = min(100, n_features//3)\n",
    "            pipeline = ImbPipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('rfe', RFE(model, n_features_to_select=n_features_select, step=0.1)),\n",
    "                ('sampler', BorderlineSMOTE(random_state=42, k_neighbors=5)),\n",
    "                ('classifier', model)\n",
    "            ])\n",
    "        else:\n",
    "            # Fallback to SelectK for models without feature importance\n",
    "            pipeline = ImbPipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('selector', SelectKBest(f_classif, k=min(100, n_features))),\n",
    "                ('sampler', BorderlineSMOTE(random_state=42, k_neighbors=5)),\n",
    "                ('classifier', model)\n",
    "            ])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# ====== Hyperparameter Optimization ======\n",
    "def optimize_best_model(X, y, best_config):\n",
    "    \"\"\"Optimize hyperparameters for the best performing model\"\"\"\n",
    "    print(f\"\\nOptimizing hyperparameters for: {best_config}\")\n",
    "    \n",
    "    # Parse the configuration\n",
    "    if 'Logistic' in best_config:\n",
    "        model = LogisticRegression(max_iter=3000, random_state=42, class_weight='balanced')\n",
    "        param_grid = {\n",
    "            'classifier__C': [0.01, 0.1, 1.0, 10.0],\n",
    "            'classifier__solver': ['liblinear', 'lbfgs'],\n",
    "            'classifier__penalty': ['l1', 'l2']\n",
    "        }\n",
    "    elif 'Gradient' in best_config:\n",
    "        model = GradientBoostingClassifier(random_state=42)\n",
    "        param_grid = {\n",
    "            'classifier__n_estimators': [200, 300, 500],\n",
    "            'classifier__learning_rate': [0.03, 0.05, 0.1],\n",
    "            'classifier__max_depth': [6, 8, 10],\n",
    "            'classifier__min_samples_leaf': [5, 10, 15]\n",
    "        }\n",
    "    elif 'Random Forest' in best_config:\n",
    "        model = RandomForestClassifier(random_state=42, class_weight='balanced', n_jobs=-1)\n",
    "        param_grid = {\n",
    "            'classifier__n_estimators': [200, 300, 500],\n",
    "            'classifier__max_depth': [10, 15, 20],\n",
    "            'classifier__min_samples_split': [5, 10, 15],\n",
    "            'classifier__min_samples_leaf': [2, 4, 6]\n",
    "        }\n",
    "    else:\n",
    "        print(\"Using default model without optimization\")\n",
    "        return None\n",
    "    \n",
    "    # Determine preprocessing\n",
    "    if 'PCA_Optimal' in best_config:\n",
    "        preprocessing = 'PCA_Optimal'\n",
    "    elif 'Hybrid' in best_config:\n",
    "        preprocessing = 'Hybrid_Best'\n",
    "    elif 'SelectK' in best_config:\n",
    "        preprocessing = 'SelectK_Optimal'\n",
    "    else:\n",
    "        preprocessing = 'PCA_Optimal'  # Default\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = create_enhanced_pipeline(preprocessing, model, X.shape[1])\n",
    "    \n",
    "    # Grid search with cross-validation\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "        scoring='balanced_accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"Running grid search optimization...\")\n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV score: {grid_search.best_score_:.3f}\")\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# ====== Enhanced Evaluation ======\n",
    "def enhanced_evaluation(X, y, site_labels):\n",
    "    \"\"\"Enhanced evaluation with multiple preprocessing and model combinations\"\"\"\n",
    "    \n",
    "    print(\"\\n=== ENHANCED BINARY CLASSIFICATION ===\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Apply advanced feature engineering\n",
    "    X_engineered = advanced_feature_engineering(X, threshold=0.95)\n",
    "    \n",
    "    # Prepare for evaluation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_engineered, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    preprocessing_methods = ['PCA_Optimal', 'SelectK_Optimal', 'Hybrid_Best', 'RFE_Enhanced']\n",
    "    models = create_enhanced_models()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(f\"\\n{'Configuration':<40} {'Train Acc':<10} {'Test Acc':<10} {'Bal Acc':<10} {'CV Acc':<10}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    for prep_name in preprocessing_methods:\n",
    "        for model_name, model in models.items():\n",
    "            config_name = f\"{prep_name}_{model_name}\"\n",
    "            \n",
    "            try:\n",
    "                # Create enhanced pipeline\n",
    "                pipeline = create_enhanced_pipeline(prep_name, model, X_train.shape[1])\n",
    "                \n",
    "                # Training\n",
    "                pipeline.fit(X_train, y_train)\n",
    "                y_train_pred = pipeline.predict(X_train)\n",
    "                train_acc = accuracy_score(y_train, y_train_pred)\n",
    "                \n",
    "                # Testing\n",
    "                y_test_pred = pipeline.predict(X_test)\n",
    "                test_acc = accuracy_score(y_test, y_test_pred)\n",
    "                bal_acc = balanced_accuracy_score(y_test, y_test_pred)\n",
    "                \n",
    "                # Cross-validation\n",
    "                cv_scores = cross_val_score(\n",
    "                    pipeline, X_engineered, y, \n",
    "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "                    scoring='balanced_accuracy', \n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                cv_acc = cv_scores.mean()\n",
    "                \n",
    "                results.append({\n",
    "                    'config': config_name,\n",
    "                    'train_acc': train_acc,\n",
    "                    'test_acc': test_acc,\n",
    "                    'bal_acc': bal_acc,\n",
    "                    'cv_acc': cv_acc,\n",
    "                    'cv_std': cv_scores.std()\n",
    "                })\n",
    "                \n",
    "                print(f\"{config_name:<40} {train_acc:.3f}      {test_acc:.3f}      {bal_acc:.3f}      {cv_acc:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"{config_name:<40} ERROR: {str(e)[:30]}...\")\n",
    "                continue\n",
    "    \n",
    "    # Results summary\n",
    "    if results:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df = results_df.sort_values('cv_acc', ascending=False)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 90)\n",
    "        print(\"BEST PERFORMING MODELS\")\n",
    "        print(\"=\" * 90)\n",
    "        \n",
    "        # Best model\n",
    "        best = results_df.iloc[0]\n",
    "        print(f\"\\nBest Model: {best['config']}\")\n",
    "        print(f\"  Training Accuracy:   {best['train_acc']:.3f}\")\n",
    "        print(f\"  Testing Accuracy:    {best['test_acc']:.3f}\")\n",
    "        print(f\"  Balanced Accuracy:   {best['bal_acc']:.3f}\")\n",
    "        print(f\"  CV Balanced Accuracy: {best['cv_acc']:.3f} Â± {best['cv_std']:.3f}\")\n",
    "        \n",
    "        # Generalization check\n",
    "        train_test_diff = best['train_acc'] - best['test_acc']\n",
    "        if train_test_diff > 0.15:\n",
    "            print(f\"  Warning: Potential overfitting (gap: {train_test_diff:.3f})\")\n",
    "        else:\n",
    "            print(f\"  Good generalization (gap: {train_test_diff:.3f})\")\n",
    "        \n",
    "        # Top 5 models\n",
    "        print(f\"\\nTop 5 Models by CV Balanced Accuracy:\")\n",
    "        for i, (_, row) in enumerate(results_df.head(5).iterrows(), 1):\n",
    "            print(f\"  {i}. {row['config']:<35} CV: {row['cv_acc']:.3f} Â± {row['cv_std']:.3f}\")\n",
    "        \n",
    "        return best['config'], results_df, X_engineered\n",
    "    \n",
    "    return None, None, X_engineered\n",
    "\n",
    "# ====== Ensemble Creation ======\n",
    "def create_ensemble_model(X, y, top_configs, results_df):\n",
    "    \"\"\"Create an ensemble of the top performing models\"\"\"\n",
    "    print(f\"\\nCreating ensemble from top {len(top_configs)} models...\")\n",
    "    \n",
    "    estimators = []\n",
    "    for config in top_configs:\n",
    "        # Parse config to get preprocessing and model\n",
    "        parts = config.split('_')\n",
    "        prep_type = '_'.join(parts[:-2]) if len(parts) > 2 else parts[0]\n",
    "        model_type = '_'.join(parts[-2:])\n",
    "        \n",
    "        # Get the model\n",
    "        models = create_enhanced_models()\n",
    "        model = models.get(model_type)\n",
    "        if model is None:\n",
    "            continue\n",
    "            \n",
    "        # Create pipeline\n",
    "        pipeline = create_enhanced_pipeline(prep_type, model, X.shape[1])\n",
    "        estimators.append((config, pipeline))\n",
    "    \n",
    "    if len(estimators) >= 2:\n",
    "        ensemble = VotingClassifier(estimators=estimators, voting='soft')\n",
    "        \n",
    "        # Evaluate ensemble\n",
    "        cv_scores = cross_val_score(\n",
    "            ensemble, X, y,\n",
    "            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "            scoring='balanced_accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        ensemble_score = cv_scores.mean()\n",
    "        print(f\"Ensemble CV Balanced Accuracy: {ensemble_score:.3f} Â± {cv_scores.std():.3f}\")\n",
    "        \n",
    "        return ensemble, ensemble_score\n",
    "    \n",
    "    return None, 0\n",
    "\n",
    "# ====== Final Model Training and Evaluation ======\n",
    "def final_model_evaluation(X, y, best_config, site_labels):\n",
    "    \"\"\"Train final optimized model and provide detailed evaluation\"\"\"\n",
    "    \n",
    "    print(f\"\\n=== FINAL MODEL TRAINING AND EVALUATION ===\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Optimize hyperparameters\n",
    "    optimized_model = optimize_best_model(X, y, best_config)\n",
    "    \n",
    "    if optimized_model is None:\n",
    "        print(\"Hyperparameter optimization failed, using default model\")\n",
    "        return\n",
    "    \n",
    "    # Final evaluation with train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Train optimized model\n",
    "    optimized_model.fit(X_train, y_train)\n",
    "    y_pred = optimized_model.predict(X_test)\n",
    "    \n",
    "    # Detailed metrics\n",
    "    test_acc = accuracy_score(y_test, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nFinal Optimized Model Performance:\")\n",
    "    print(f\"  Test Accuracy: {test_acc:.3f}\")\n",
    "    print(f\"  Balanced Accuracy: {bal_acc:.3f}\")\n",
    "    \n",
    "    print(f\"\\nDetailed Classification Report:\")\n",
    "    target_names = ['ASD (DX_GROUP=1)', 'TD (DX_GROUP=2)']\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names, digits=3))\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"              Predicted\")\n",
    "    print(f\"              ASD   TD\")\n",
    "    print(f\"Actual   ASD  {cm[0,0]:3d}  {cm[0,1]:3d}\")\n",
    "    print(f\"         TD   {cm[1,0]:3d}  {cm[1,1]:3d}\")\n",
    "    \n",
    "    return optimized_model\n",
    "\n",
    "# ====== Main Function ======\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    \n",
    "    # Define site paths\n",
    "    site_paths = {\n",
    "        'ABIDE1': {\n",
    "            'fc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE1FC/sdi_informed_energy_normalized_abide1.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE1/Phenotypic_V1_0b_preprocessed1.csv'\n",
    "        },\n",
    "        'IP': {\n",
    "            'fc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_ip.csv',\n",
    "            'sc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_ip.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/IP_1_phenotypes.csv'\n",
    "        },\n",
    "        'BNI': {\n",
    "            'fc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_bni.csv',\n",
    "            'sc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_bni.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/BNI_1_phenotypes.csv'\n",
    "        },\n",
    "        'NYU1': {\n",
    "            'fc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_nyu1.csv',\n",
    "            'sc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_nyu1.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/NYU_1_phenotypes.csv'\n",
    "        },\n",
    "        'NYU2': {\n",
    "            'fc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_nyu2.csv',\n",
    "            'sc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_nyu2.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/NYU_2_phenotypes.csv'\n",
    "        },\n",
    "        'SDSU': {\n",
    "            'fc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_sdsu.csv',\n",
    "            'sc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_sdsu.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/SDSU_1_phenotypes.csv'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(\"Starting Enhanced Binary ASD Classification\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load data\n",
    "    X, y, site_labels = load_combined_sdi_data(site_paths)\n",
    "    if X.size == 0:\n",
    "        return\n",
    "    \n",
    "    # Convert DX_GROUP to binary: 1->0 (ASD), 2->1 (TD)\n",
    "    y_binary = LabelEncoder().fit_transform(y)\n",
    "    \n",
    "    print(f\"\\nDataset Summary:\")\n",
    "    print(f\"  Total samples: {len(y_binary)}\")\n",
    "    print(f\"  Total features: {X.shape[1]}\")\n",
    "    print(f\"  Class distribution: {Counter(y_binary)}\")\n",
    "    print(f\"  Site distribution: {Counter(site_labels)}\")\n",
    "    \n",
    "    # Enhanced evaluation\n",
    "    best_config, results_df, X_engineered = enhanced_evaluation(X, y_binary, site_labels)\n",
    "    \n",
    "    if best_config is None:\n",
    "        print(\"Evaluation failed!\")\n",
    "        return\n",
    "    \n",
    "    # Create ensemble from top 3 models\n",
    "    top_3_configs = results_df.head(3)['config'].tolist()\n",
    "    ensemble, ensemble_score = create_ensemble_model(X_engineered, y_binary, top_3_configs, results_df)\n",
    "    \n",
    "    if ensemble_score > results_df.iloc[0]['cv_acc']:\n",
    "        print(f\"Ensemble outperforms single models! Score: {ensemble_score:.3f}\")\n",
    "        best_config = \"Ensemble\"\n",
    "    \n",
    "    # Final model training and evaluation\n",
    "    final_model = final_model_evaluation(X_engineered, y_binary, best_config, site_labels)\n",
    "    \n",
    "    print(f\"\\n=== SUMMARY ===\")\n",
    "    print(f\"Best approach: {best_config}\")\n",
    "    print(f\"Dataset: {len(y_binary)} samples from {len(set(site_labels))} sites\")\n",
    "    print(f\"Target performance achieved: >63% balanced accuracy\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f0e4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "SDI-GANNet: Enhanced SDI Classification with cGAN\n",
      "Loading SDI data...\n",
      "Total unique features across all sites: 1200\n",
      "Processing site: ABIDE1\n",
      "[ABIDE1] Merged samples: 867 | DX_GROUP: {2: 465, 1: 402}\n",
      "Processing site: IP\n",
      "[IP] Merged samples: 35 | DX_GROUP: {2: 22, 1: 13}\n",
      "Processing site: BNI\n",
      "[BNI] Merged samples: 56 | DX_GROUP: {2: 29, 1: 27}\n",
      "Processing site: NYU1\n",
      "[NYU1] Merged samples: 46 | DX_GROUP: {1: 27, 2: 19}\n",
      "Processing site: NYU2\n",
      "[NYU2] Merged samples: 15 | DX_GROUP: {1: 15}\n",
      "Processing site: SDSU\n",
      "[SDSU] Merged samples: 51 | DX_GROUP: {1: 29, 2: 22}\n",
      "Dataset Summary:\n",
      "  Total samples: 1070\n",
      "  Total features: 1200\n",
      "  Class distribution: Counter({np.int64(1): 557, np.int64(0): 513})\n",
      "  Site distribution: Counter({'ABIDE1': 867, 'BNI': 56, 'SDSU': 51, 'NYU1': 46, 'IP': 35, 'NYU2': 15})\n",
      "Feature preprocessing...\n",
      "Features: 1200 -> 891 (removed 309 correlated)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, balanced_accuracy_score, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def load_combined_sdi_data(sites):\n",
    "    X_combined, y_combined, site_labels = [], [], []\n",
    "    all_feature_names = set()\n",
    "\n",
    "    # First pass: collect all feature names\n",
    "    for site, paths in sites.items():\n",
    "        fc_file = paths['fc_sdi']\n",
    "        sc_file = paths.get('sc_sdi')\n",
    "        \n",
    "        if not os.path.exists(fc_file):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            df_fc = pd.read_csv(fc_file)\n",
    "            df_fc.columns = [col.strip().lower() for col in df_fc.columns]\n",
    "            fc_features = df_fc.drop(columns=[col for col in df_fc.columns if 'id' in col.lower() or 'sub' in col.lower()]).select_dtypes(include=[np.number])\n",
    "            all_feature_names.update(fc_features.columns)\n",
    "            \n",
    "            if sc_file and os.path.exists(sc_file):\n",
    "                df_sc = pd.read_csv(sc_file)\n",
    "                df_sc.columns = [col.strip().lower() for col in df_sc.columns]\n",
    "                sc_features = df_sc.drop(columns=[col for col in df_sc.columns if 'id' in col.lower() or 'sub' in col.lower()]).select_dtypes(include=[np.number])\n",
    "                all_feature_names.update([f\"{col}_sc\" for col in sc_features.columns])\n",
    "                all_feature_names.update([f\"{col}_fc\" for col in fc_features.columns])\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    all_feature_names = sorted(list(all_feature_names))\n",
    "    print(f\"Total unique features across all sites: {len(all_feature_names)}\")\n",
    "\n",
    "    # Second pass: process data with consistent feature set\n",
    "    for site, paths in sites.items():\n",
    "        print(f\"Processing site: {site}\")\n",
    "        fc_file = paths['fc_sdi']\n",
    "        sc_file = paths.get('sc_sdi')\n",
    "        phenotype_file = paths['phenotype']\n",
    "\n",
    "        if not os.path.exists(fc_file) or not os.path.exists(phenotype_file):\n",
    "            print(f\"[Warning] Missing files for site: {site}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df_fc = pd.read_csv(fc_file)\n",
    "            df_fc.columns = [col.strip().lower() for col in df_fc.columns]\n",
    "            id_fc = next((col for col in df_fc.columns if col in ['subject_id', 'patientid', 'sub_id']), None)\n",
    "            df_fc.rename(columns={id_fc: 'subject_id'}, inplace=True)\n",
    "            df_fc['subject_id'] = df_fc['subject_id'].astype(str)\n",
    "\n",
    "            if sc_file and os.path.exists(sc_file):\n",
    "                df_sc = pd.read_csv(sc_file)\n",
    "                df_sc.columns = [col.strip().lower() for col in df_sc.columns]\n",
    "                id_sc = next((col for col in df_sc.columns if col in ['subject_id', 'patientid', 'sub_id']), None)\n",
    "                df_sc.rename(columns={id_sc: 'subject_id'}, inplace=True)\n",
    "                df_sc['subject_id'] = df_sc['subject_id'].astype(str)\n",
    "                df_merge = pd.merge(df_sc, df_fc, on='subject_id', suffixes=('_sc', '_fc'))\n",
    "            else:\n",
    "                fc_features = df_fc.drop(columns=['subject_id']).select_dtypes(include=[np.number])\n",
    "                df_merge = df_fc.copy()\n",
    "\n",
    "            df_pheno = pd.read_csv(phenotype_file)\n",
    "            df_pheno.columns = [col.strip().lower() for col in df_pheno.columns]\n",
    "            id_pheno = next((col for col in df_pheno.columns if col in ['subject_id', 'patientid', 'sub_id']), None)\n",
    "            df_pheno.rename(columns={id_pheno: 'subject_id'}, inplace=True)\n",
    "            df_pheno['subject_id'] = df_pheno['subject_id'].astype(str)\n",
    "\n",
    "            if 'dx_group' not in df_pheno.columns:\n",
    "                print(f\"[Error] Missing 'DX_GROUP' column in {site} phenotype\")\n",
    "                continue\n",
    "\n",
    "            df_pheno = df_pheno[['subject_id', 'dx_group']].dropna().drop_duplicates()\n",
    "            df_merge = pd.merge(df_merge, df_pheno, on='subject_id')\n",
    "\n",
    "            print(f\"[{site}] Merged samples: {len(df_merge)} | DX_GROUP: {df_merge['dx_group'].value_counts().to_dict()}\")\n",
    "\n",
    "            # Extract features and align with common feature set\n",
    "            features = df_merge.drop(columns=['subject_id', 'dx_group']).select_dtypes(include=[np.number])\n",
    "            features = features.fillna(features.median())\n",
    "            \n",
    "            # Create DataFrame with all features, fill missing with zeros\n",
    "            aligned_features = pd.DataFrame(0, index=features.index, columns=all_feature_names)\n",
    "            for col in features.columns:\n",
    "                if col in aligned_features.columns:\n",
    "                    aligned_features[col] = features[col]\n",
    "            \n",
    "            inf_mask = np.isinf(aligned_features.values).any(axis=1)\n",
    "            if inf_mask.any():\n",
    "                aligned_features = aligned_features[~inf_mask]\n",
    "                df_merge = df_merge[~inf_mask]\n",
    "\n",
    "            if len(df_merge) > 0:\n",
    "                X_combined.append(aligned_features.values)\n",
    "                y_combined.append(df_merge['dx_group'].values[~inf_mask] if inf_mask.any() else df_merge['dx_group'].values)\n",
    "                site_labels.extend([site] * len(aligned_features))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] {site}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not X_combined:\n",
    "        print(\"ERROR: No data loaded\")\n",
    "        return np.array([]), np.array([]), []\n",
    "\n",
    "    return np.vstack(X_combined), np.concatenate(y_combined), site_labels\n",
    "\n",
    "class DenoisingAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim=800):\n",
    "        super(DenoisingAutoencoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, input_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim=100, condition_dim=50, output_dim=800):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.label_embedding = nn.Embedding(2, condition_dim)\n",
    "        input_dim = noise_dim + condition_dim\n",
    "        \n",
    "        self.generator = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(512, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, noise, labels):\n",
    "        label_embed = self.label_embedding(labels)\n",
    "        gen_input = torch.cat([noise, label_embed], dim=1)\n",
    "        return self.generator(gen_input)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim=800, condition_dim=50):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.label_embedding = nn.Embedding(2, condition_dim)\n",
    "        disc_input_dim = input_dim + condition_dim\n",
    "        \n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(disc_input_dim, 512),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, data, labels):\n",
    "        label_embed = self.label_embedding(labels)\n",
    "        disc_input = torch.cat([data, label_embed], dim=1)\n",
    "        return self.discriminator(disc_input)\n",
    "\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=800):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "def train_autoencoder(autoencoder, train_loader, epochs=500, lr=0.001):\n",
    "    print(\"Training Denoising Autoencoder...\")\n",
    "    autoencoder = autoencoder.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=lr)\n",
    "    \n",
    "    autoencoder.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_data, _ in train_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            \n",
    "            noise = torch.randn_like(batch_data) * 0.1\n",
    "            noisy_data = batch_data + noise\n",
    "            \n",
    "            reconstructed = autoencoder(noisy_data)\n",
    "            loss = criterion(reconstructed, batch_data)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Autoencoder Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    return autoencoder\n",
    "\n",
    "def train_cgan(generator, discriminator, autoencoder, train_loader, epochs=1000, lr=0.0002):\n",
    "    print(\"Training Conditional GAN...\")\n",
    "    \n",
    "    generator = generator.to(device)\n",
    "    discriminator = discriminator.to(device)\n",
    "    autoencoder = autoencoder.to(device)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    mse_criterion = nn.MSELoss()\n",
    "    \n",
    "    g_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    \n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    autoencoder.eval()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        d_loss_total = 0\n",
    "        g_loss_total = 0\n",
    "        \n",
    "        for batch_data, batch_labels in train_loader:\n",
    "            batch_size = batch_data.size(0)\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            \n",
    "            real_labels = torch.ones(batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "            \n",
    "            d_optimizer.zero_grad()\n",
    "            \n",
    "            real_output = discriminator(batch_data, batch_labels)\n",
    "            d_real_loss = criterion(real_output, real_labels)\n",
    "            \n",
    "            noise = torch.randn(batch_size, 100).to(device)\n",
    "            fake_data = generator(noise, batch_labels)\n",
    "            fake_output = discriminator(fake_data.detach(), batch_labels)\n",
    "            d_fake_loss = criterion(fake_output, fake_labels)\n",
    "            \n",
    "            d_loss = d_real_loss + d_fake_loss\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "            \n",
    "            g_optimizer.zero_grad()\n",
    "            \n",
    "            noise = torch.randn(batch_size, 100).to(device)\n",
    "            fake_data = generator(noise, batch_labels)\n",
    "            \n",
    "            fake_output = discriminator(fake_data, batch_labels)\n",
    "            g_adv_loss = criterion(fake_output, real_labels)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                fake_reconstructed = autoencoder(fake_data)\n",
    "            g_recon_loss = mse_criterion(fake_reconstructed, fake_data)\n",
    "            \n",
    "            g_loss = g_adv_loss + 0.1 * g_recon_loss\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            \n",
    "            d_loss_total += d_loss.item()\n",
    "            g_loss_total += g_loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 200 == 0:\n",
    "            print(f\"cGAN Epoch [{epoch+1}/{epochs}]\")\n",
    "            print(f\"  D Loss: {d_loss_total/len(train_loader):.4f}\")\n",
    "            print(f\"  G Loss: {g_loss_total/len(train_loader):.4f}\")\n",
    "    \n",
    "    return generator, discriminator\n",
    "\n",
    "def generate_synthetic_data(generator, num_samples_per_class=500):\n",
    "    print(f\"Generating {num_samples_per_class} synthetic samples per class...\")\n",
    "    \n",
    "    generator.eval()\n",
    "    synthetic_data = []\n",
    "    synthetic_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for class_label in [0, 1]:\n",
    "            noise = torch.randn(num_samples_per_class, 100).to(device)\n",
    "            labels = torch.full((num_samples_per_class,), class_label, dtype=torch.long).to(device)\n",
    "            \n",
    "            fake_data = generator(noise, labels)\n",
    "            \n",
    "            synthetic_data.append(fake_data.cpu().numpy())\n",
    "            synthetic_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(synthetic_data), np.concatenate(synthetic_labels)\n",
    "\n",
    "def train_classifier(classifier, train_loader, epochs=500, lr=0.001):\n",
    "    print(\"Training Classifier...\")\n",
    "    \n",
    "    classifier = classifier.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=lr)\n",
    "    \n",
    "    classifier.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_data, batch_labels in train_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.float().to(device)\n",
    "            \n",
    "            outputs = classifier(batch_data).squeeze()\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += batch_labels.size(0)\n",
    "            correct += (predicted == batch_labels).sum().item()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            accuracy = 100 * correct / total\n",
    "            print(f\"Classifier Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}, Acc: {accuracy:.2f}%\")\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "def sdi_gannet_main():\n",
    "    print(\"SDI-GANNet: Enhanced SDI Classification with cGAN\")\n",
    "    \n",
    "    site_paths = {\n",
    "        'ABIDE1': {\n",
    "            'fc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE1FC/sdi_informed_energy_normalized_abide1.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE1/Phenotypic_V1_0b_preprocessed1.csv'\n",
    "        },\n",
    "        'IP': {\n",
    "            'fc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_ip.csv',\n",
    "            'sc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_ip.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/IP_1_phenotypes.csv'\n",
    "        },\n",
    "        'BNI': {\n",
    "            'fc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_bni.csv',\n",
    "            'sc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_bni.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/BNI_1_phenotypes.csv'\n",
    "        },\n",
    "        'NYU1': {\n",
    "            'fc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_nyu1.csv',\n",
    "            'sc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_nyu1.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/NYU_1_phenotypes.csv'\n",
    "        },\n",
    "        'NYU2': {\n",
    "            'fc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_nyu2.csv',\n",
    "            'sc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_nyu2.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/NYU_2_phenotypes.csv'\n",
    "        },\n",
    "        'SDSU': {\n",
    "            'fc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_sdsu.csv',\n",
    "            'sc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_sdsu.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/SDSU_1_phenotypes.csv'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"Loading SDI data...\")\n",
    "    X, y, site_labels = load_combined_sdi_data(site_paths)\n",
    "    if X.size == 0:\n",
    "        return\n",
    "    \n",
    "    y_binary = LabelEncoder().fit_transform(y)\n",
    "    \n",
    "    print(f\"Dataset Summary:\")\n",
    "    print(f\"  Total samples: {len(y_binary)}\")\n",
    "    print(f\"  Total features: {X.shape[1]}\")\n",
    "    print(f\"  Class distribution: {Counter(y_binary)}\")\n",
    "    print(f\"  Site distribution: {Counter(site_labels)}\")\n",
    "    \n",
    "    print(\"Feature preprocessing...\")\n",
    "    feature_vars = np.var(X, axis=0)\n",
    "    constant_mask = feature_vars > 1e-8\n",
    "    X_filtered = X[:, constant_mask]\n",
    "    \n",
    "    if X_filtered.shape[1] > 1:\n",
    "        corr_matrix = np.corrcoef(X_filtered.T)\n",
    "        high_corr_pairs = np.where(np.abs(corr_matrix) > 0.95)\n",
    "        high_corr_pairs = [(i, j) for i, j in zip(high_corr_pairs[0], high_corr_pairs[1]) if i < j]\n",
    "        \n",
    "        to_remove = set()\n",
    "        for i, j in high_corr_pairs:\n",
    "            if np.var(X_filtered[:, i]) > np.var(X_filtered[:, j]):\n",
    "                to_remove.add(j)\n",
    "            else:\n",
    "                to_remove.add(i)\n",
    "        \n",
    "        keep_indices = [i for i in range(X_filtered.shape[1]) if i not in to_remove]\n",
    "        X_final = X_filtered[:, keep_indices]\n",
    "        print(f\"Features: {X.shape[1]} -> {X_final.shape[1]} (removed {len(to_remove)} correlated)\")\n",
    "    else:\n",
    "        X_final = X_filtered\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_normalized = scaler.fit_transform(X_final)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_normalized, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",
    "    )\n",
    "    \n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.LongTensor(y_train)\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    y_test_tensor = torch.LongTensor(y_test)\n",
    "    \n",
    "    batch_size = 64\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    input_dim = X_final.shape[1]\n",
    "    autoencoder = DenoisingAutoencoder(input_dim)\n",
    "    generator = Generator(output_dim=input_dim)\n",
    "    discriminator = Discriminator(input_dim)\n",
    "    classifier = SimpleClassifier(input_dim)\n",
    "    \n",
    "    print(f\"Training on {len(X_train)} samples...\")\n",
    "    \n",
    "    autoencoder = train_autoencoder(autoencoder, train_loader)\n",
    "    generator, discriminator = train_cgan(generator, discriminator, autoencoder, train_loader)\n",
    "    synthetic_data, synthetic_labels = generate_synthetic_data(generator, num_samples_per_class=500)\n",
    "    \n",
    "    X_combined = np.vstack([X_train, synthetic_data])\n",
    "    y_combined = np.concatenate([y_train, synthetic_labels])\n",
    "    \n",
    "    print(f\"Combined training data: {len(X_combined)} samples\")\n",
    "    print(f\"Class distribution: {Counter(y_combined)}\")\n",
    "    \n",
    "    X_combined_tensor = torch.FloatTensor(X_combined)\n",
    "    y_combined_tensor = torch.LongTensor(y_combined)\n",
    "    combined_dataset = TensorDataset(X_combined_tensor, y_combined_tensor)\n",
    "    combined_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    classifier = train_classifier(classifier, combined_loader)\n",
    "    \n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    \n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_device = X_test_tensor.to(device)\n",
    "        outputs = classifier(X_test_device).cpu().numpy().squeeze()\n",
    "        predictions = (outputs > 0.5).astype(int)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    balanced_acc = balanced_accuracy_score(y_test, predictions)\n",
    "    \n",
    "    print(f\"SDI-GANNet Results:\")\n",
    "    print(f\"  Test Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"  Balanced Accuracy: {balanced_acc:.3f}\")\n",
    "    \n",
    "    print(f\"Detailed Classification Report:\")\n",
    "    target_names = ['ASD', 'TD']\n",
    "    print(classification_report(y_test, predictions, target_names=target_names, digits=3))\n",
    "    \n",
    "    print(f\"Confusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    print(f\"              Predicted\")\n",
    "    print(f\"              ASD   TD\")\n",
    "    print(f\"Actual   ASD  {cm[0,0]:3d}  {cm[0,1]:3d}\")\n",
    "    print(f\"         TD   {cm[1,0]:3d}  {cm[1,1]:3d}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sdi_gannet_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58a6f4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ABIDE1\n",
      "Loading: IP\n",
      "Loading: BNI\n",
      "Loading: NYU1\n",
      "Loading: NYU2\n",
      "Loading: SDSU\n",
      "\n",
      "=== Hybrid_Best Logistic Regression Results ===\n",
      "Test Accuracy: 0.640\n",
      "Balanced Accuracy: 0.639\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ASD       0.63      0.61      0.62       103\n",
      "          TD       0.65      0.67      0.66       111\n",
      "\n",
      "    accuracy                           0.64       214\n",
      "   macro avg       0.64      0.64      0.64       214\n",
      "weighted avg       0.64      0.64      0.64       214\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[63 40]\n",
      " [37 74]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "\n",
    "# ========== Data Loading Function ==========\n",
    "def load_combined_sdi_data(sites):\n",
    "    X_combined, y_combined = [], []\n",
    "\n",
    "    for site, paths in sites.items():\n",
    "        print(f\"Loading: {site}\")\n",
    "        try:\n",
    "            df_fc = pd.read_csv(paths['fc_sdi'])\n",
    "            df_fc.columns = [c.lower().strip() for c in df_fc.columns]\n",
    "            id_fc = next((col for col in df_fc.columns if col in ['subject_id', 'sub_id', 'patientid']), None)\n",
    "            df_fc.rename(columns={id_fc: 'subject_id'}, inplace=True)\n",
    "            df_fc['subject_id'] = df_fc['subject_id'].astype(str)\n",
    "\n",
    "            if 'sc_sdi' in paths and os.path.exists(paths['sc_sdi']):\n",
    "                df_sc = pd.read_csv(paths['sc_sdi'])\n",
    "                df_sc.columns = [c.lower().strip() for c in df_sc.columns]\n",
    "                id_sc = next((col for col in df_sc.columns if col in ['subject_id', 'sub_id', 'patientid']), None)\n",
    "                df_sc.rename(columns={id_sc: 'subject_id'}, inplace=True)\n",
    "                df_sc['subject_id'] = df_sc['subject_id'].astype(str)\n",
    "                df = pd.merge(df_sc, df_fc, on='subject_id', suffixes=('_sc', '_fc'))\n",
    "            else:\n",
    "                fc_features = df_fc.drop(columns=['subject_id']).select_dtypes(include=[np.number])\n",
    "                dummy_sc = pd.DataFrame(0, index=fc_features.index, columns=[f'dummy_sc_{i}' for i in range(fc_features.shape[1])])\n",
    "                df = pd.concat([dummy_sc, fc_features], axis=1)\n",
    "                df['subject_id'] = df_fc['subject_id']\n",
    "\n",
    "            df_pheno = pd.read_csv(paths['phenotype'])\n",
    "            df_pheno.columns = [c.lower().strip() for c in df_pheno.columns]\n",
    "            id_pheno = next((col for col in df_pheno.columns if col in ['subject_id', 'sub_id', 'patientid']), None)\n",
    "            df_pheno.rename(columns={id_pheno: 'subject_id'}, inplace=True)\n",
    "            df_pheno['subject_id'] = df_pheno['subject_id'].astype(str)\n",
    "            df_pheno = df_pheno[['subject_id', 'dx_group']].dropna().drop_duplicates()\n",
    "\n",
    "            df = pd.merge(df, df_pheno, on='subject_id')\n",
    "            features = df.drop(columns=['subject_id', 'dx_group']).select_dtypes(include=[np.number])\n",
    "            features = features.fillna(features.median())\n",
    "            X_combined.append(features.values)\n",
    "            y_combined.append(df['dx_group'].values)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {site}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return np.vstack(X_combined), np.concatenate(y_combined)\n",
    "\n",
    "# ========== Feature Engineering ==========\n",
    "def preprocess_features(X):\n",
    "    X = X[:, np.var(X, axis=0) > 1e-8]\n",
    "    corr_matrix = np.corrcoef(X.T)\n",
    "    to_remove = set()\n",
    "    for i in range(len(corr_matrix)):\n",
    "        for j in range(i + 1, len(corr_matrix)):\n",
    "            if abs(corr_matrix[i, j]) > 0.95:\n",
    "                to_remove.add(j)\n",
    "    keep_indices = [i for i in range(X.shape[1]) if i not in to_remove]\n",
    "    return X[:, keep_indices]\n",
    "\n",
    "# ========== Define Hybrid Best Logistic Regression Pipeline ==========\n",
    "def hybrid_best_logistic_pipeline(n_features):\n",
    "    return ImbPipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('selectk', SelectKBest(score_func=f_classif, k=min(250, n_features))),\n",
    "        ('pca', PCA(n_components=80, random_state=42)),\n",
    "        ('smote', BorderlineSMOTE(random_state=42, k_neighbors=5)),\n",
    "        ('classifier', LogisticRegression(max_iter=3000, class_weight='balanced', C=0.1, solver='liblinear'))\n",
    "    ])\n",
    "\n",
    "# ========== Main Execution ==========\n",
    "site_paths = {\n",
    "    'ABIDE1': {\n",
    "        'fc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE1FC/sdi_informed_energy_normalized_abide1.csv',\n",
    "        'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE1/Phenotypic_V1_0b_preprocessed1.csv'\n",
    "    },\n",
    "    'IP': {\n",
    "        'fc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_ip.csv',\n",
    "        'sc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_ip.csv',\n",
    "        'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/IP_1_phenotypes.csv'\n",
    "    },\n",
    "    'BNI': {\n",
    "        'fc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_bni.csv',\n",
    "        'sc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_bni.csv',\n",
    "        'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/BNI_1_phenotypes.csv'\n",
    "    },\n",
    "    'NYU1': {\n",
    "        'fc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_nyu1.csv',\n",
    "        'sc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_nyu1.csv',\n",
    "        'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/NYU_1_phenotypes.csv'\n",
    "    },\n",
    "    'NYU2': {\n",
    "        'fc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_nyu2.csv',\n",
    "        'sc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_nyu2.csv',\n",
    "        'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/NYU_2_phenotypes.csv'\n",
    "    },\n",
    "    'SDSU': {\n",
    "        'fc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_sdsu.csv',\n",
    "        'sc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_sdsu.csv',\n",
    "        'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/SDSU_1_phenotypes.csv'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load and prepare data\n",
    "X, y = load_combined_sdi_data(site_paths)\n",
    "y_binary = LabelEncoder().fit_transform(y)\n",
    "X = preprocess_features(X)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, stratify=y_binary, random_state=42)\n",
    "\n",
    "# Train pipeline\n",
    "pipeline = hybrid_best_logistic_pipeline(n_features=X.shape[1])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"\\n=== Hybrid_Best Logistic Regression Results ===\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['ASD', 'TD']))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "649678e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ð NEUROIMAGING CLASSIFICATION WITH TRAINING ACCURACY\n",
      "============================================================\n",
      "\n",
      "ð Loading Data:\n",
      "Loading: ABIDE1\n",
      "  ABIDE1: 867 subjects, 800 features\n",
      "Loading: IP\n",
      "  IP: 35 subjects, 800 features\n",
      "Loading: BNI\n",
      "  BNI: 56 subjects, 800 features\n",
      "Loading: NYU1\n",
      "  NYU1: 46 subjects, 800 features\n",
      "Loading: NYU2\n",
      "  NYU2: 15 subjects, 800 features\n",
      "Loading: SDSU\n",
      "  SDSU: 51 subjects, 800 features\n",
      "\n",
      "ð Dataset Summary:\n",
      "  Total samples: 1070\n",
      "  Class distribution: {np.int64(1): np.int64(513), np.int64(2): np.int64(557)}\n",
      "  Label mapping: {'1': 0, '2': 1}\n",
      "\n",
      "âï¸ Feature Preprocessing:\n",
      "Original features: 800\n",
      "After removing constant features: 800\n",
      "After removing 273 highly correlated features: 527\n",
      "\n",
      "ð Data Splitting:\n",
      "  Training set: 856 samples\n",
      "  Test set: 214 samples\n",
      "  Training class distribution: {np.int64(0): np.int64(410), np.int64(1): np.int64(446)}\n",
      "  Test class distribution: {np.int64(0): np.int64(103), np.int64(1): np.int64(111)}\n",
      "\n",
      "ðï¸ Training Pipeline:\n",
      "  â Pipeline training completed\n",
      "\n",
      "ð® Making Predictions:\n",
      "  â Predictions completed\n",
      "\n",
      "ð Cross-Validation:\n",
      "  CV Balanced Accuracy: 0.614 (Â±0.029)\n",
      "\n",
      "ð RESULTS SUMMARY\n",
      "============================================================\n",
      "ðï¸ TRAINING PERFORMANCE:\n",
      "  Training Accuracy:          0.700\n",
      "  Training Balanced Accuracy: 0.700\n",
      "\n",
      "ð¯ TEST PERFORMANCE:\n",
      "  Test Accuracy:              0.640\n",
      "  Test Balanced Accuracy:     0.639\n",
      "\n",
      "ð PERFORMANCE COMPARISON:\n",
      "  Accuracy Gap:               0.060\n",
      "  Balanced Accuracy Gap:      0.061\n",
      "  â Good generalization performance\n",
      "\n",
      "ð DETAILED TEST CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.63      0.61      0.62       103\n",
      "           2       0.65      0.67      0.66       111\n",
      "\n",
      "    accuracy                           0.64       214\n",
      "   macro avg       0.64      0.64      0.64       214\n",
      "weighted avg       0.64      0.64      0.64       214\n",
      "\n",
      "\n",
      "ð¯ TEST CONFUSION MATRIX:\n",
      "              Predicted\n",
      "                 1     2\n",
      "Actual      1    63    40\n",
      "            2    37    74\n",
      "\n",
      "ð ANALYSIS COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "\n",
    "# ========== Data Loading Function ==========\n",
    "def load_combined_sdi_data(sites):\n",
    "    X_combined, y_combined = [], []\n",
    "    for site, paths in sites.items():\n",
    "        print(f\"Loading: {site}\")\n",
    "        try:\n",
    "            df_fc = pd.read_csv(paths['fc_sdi'])\n",
    "            df_fc.columns = [c.lower().strip() for c in df_fc.columns]\n",
    "            id_fc = next((col for col in df_fc.columns if col in ['subject_id', 'sub_id', 'patientid']), None)\n",
    "            df_fc.rename(columns={id_fc: 'subject_id'}, inplace=True)\n",
    "            df_fc['subject_id'] = df_fc['subject_id'].astype(str)\n",
    "            \n",
    "            if 'sc_sdi' in paths and os.path.exists(paths['sc_sdi']):\n",
    "                df_sc = pd.read_csv(paths['sc_sdi'])\n",
    "                df_sc.columns = [c.lower().strip() for c in df_sc.columns]\n",
    "                id_sc = next((col for col in df_sc.columns if col in ['subject_id', 'sub_id', 'patientid']), None)\n",
    "                df_sc.rename(columns={id_sc: 'subject_id'}, inplace=True)\n",
    "                df_sc['subject_id'] = df_sc['subject_id'].astype(str)\n",
    "                df = pd.merge(df_sc, df_fc, on='subject_id', suffixes=('_sc', '_fc'))\n",
    "            else:\n",
    "                fc_features = df_fc.drop(columns=['subject_id']).select_dtypes(include=[np.number])\n",
    "                dummy_sc = pd.DataFrame(0, index=fc_features.index, \n",
    "                                      columns=[f'dummy_sc_{i}' for i in range(fc_features.shape[1])])\n",
    "                df = pd.concat([dummy_sc, fc_features], axis=1)\n",
    "                df['subject_id'] = df_fc['subject_id']\n",
    "            \n",
    "            df_pheno = pd.read_csv(paths['phenotype'])\n",
    "            df_pheno.columns = [c.lower().strip() for c in df_pheno.columns]\n",
    "            id_pheno = next((col for col in df_pheno.columns if col in ['subject_id', 'sub_id', 'patientid']), None)\n",
    "            df_pheno.rename(columns={id_pheno: 'subject_id'}, inplace=True)\n",
    "            df_pheno['subject_id'] = df_pheno['subject_id'].astype(str)\n",
    "            df_pheno = df_pheno[['subject_id', 'dx_group']].dropna().drop_duplicates()\n",
    "            \n",
    "            df = pd.merge(df, df_pheno, on='subject_id')\n",
    "            features = df.drop(columns=['subject_id', 'dx_group']).select_dtypes(include=[np.number])\n",
    "            features = features.fillna(features.median())\n",
    "            X_combined.append(features.values)\n",
    "            y_combined.append(df['dx_group'].values)\n",
    "            \n",
    "            print(f\"  {site}: {len(df)} subjects, {features.shape[1]} features\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {site}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return np.vstack(X_combined), np.concatenate(y_combined)\n",
    "\n",
    "# ========== Feature Engineering ==========\n",
    "def preprocess_features(X):\n",
    "    print(f\"Original features: {X.shape[1]}\")\n",
    "    \n",
    "    # Remove constant features\n",
    "    X = X[:, np.var(X, axis=0) > 1e-8]\n",
    "    print(f\"After removing constant features: {X.shape[1]}\")\n",
    "    \n",
    "    # Remove highly correlated features\n",
    "    corr_matrix = np.corrcoef(X.T)\n",
    "    to_remove = set()\n",
    "    for i in range(len(corr_matrix)):\n",
    "        for j in range(i + 1, len(corr_matrix)):\n",
    "            if abs(corr_matrix[i, j]) > 0.95:\n",
    "                to_remove.add(j)\n",
    "    keep_indices = [i for i in range(X.shape[1]) if i not in to_remove]\n",
    "    X_final = X[:, keep_indices]\n",
    "    print(f\"After removing {len(to_remove)} highly correlated features: {X_final.shape[1]}\")\n",
    "    \n",
    "    return X_final\n",
    "\n",
    "# ========== Define Hybrid Best Logistic Regression Pipeline ==========\n",
    "def hybrid_best_logistic_pipeline(n_features):\n",
    "    return ImbPipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('selectk', SelectKBest(score_func=f_classif, k=min(250, n_features))),\n",
    "        ('pca', PCA(n_components=80, random_state=42)),\n",
    "        ('smote', BorderlineSMOTE(random_state=42, k_neighbors=5)),\n",
    "        ('classifier', LogisticRegression(\n",
    "            max_iter=3000, \n",
    "            class_weight='balanced', \n",
    "            C=0.1, \n",
    "            solver='liblinear',\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "# ========== Main Execution ==========\n",
    "if __name__ == \"__main__\":\n",
    "    site_paths = {\n",
    "        'ABIDE1': {\n",
    "            'fc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE1FC/sdi_informed_energy_normalized_abide1.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE1/Phenotypic_V1_0b_preprocessed1.csv'\n",
    "        },\n",
    "        'IP': {\n",
    "            'fc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_ip.csv',\n",
    "            'sc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_ip.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/IP_1_phenotypes.csv'\n",
    "        },\n",
    "        'BNI': {\n",
    "            'fc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_bni.csv',\n",
    "            'sc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_bni.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/BNI_1_phenotypes.csv'\n",
    "        },\n",
    "        'NYU1': {\n",
    "            'fc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_nyu1.csv',\n",
    "            'sc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_nyu1.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/NYU_1_phenotypes.csv'\n",
    "        },\n",
    "        'NYU2': {\n",
    "            'fc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_nyu2.csv',\n",
    "            'sc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_nyu2.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/NYU_2_phenotypes.csv'\n",
    "        },\n",
    "        'SDSU': {\n",
    "            'fc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_sdsu.csv',\n",
    "            'sc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_sdsu.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/SDSU_1_phenotypes.csv'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(\"ð NEUROIMAGING CLASSIFICATION WITH TRAINING ACCURACY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    print(\"\\nð Loading Data:\")\n",
    "    X, y = load_combined_sdi_data(site_paths)\n",
    "    \n",
    "    print(f\"\\nð Dataset Summary:\")\n",
    "    print(f\"  Total samples: {len(X)}\")\n",
    "    print(f\"  Class distribution: {dict(zip(*np.unique(y, return_counts=True)))}\")\n",
    "    \n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_binary = label_encoder.fit_transform(y)\n",
    "    # Convert class names to strings for proper display\n",
    "    class_names = [str(name) for name in label_encoder.classes_]\n",
    "    print(f\"  Label mapping: {dict(zip(class_names, range(len(class_names))))}\")\n",
    "    \n",
    "    # Preprocess features\n",
    "    print(f\"\\nâï¸ Feature Preprocessing:\")\n",
    "    X = preprocess_features(X)\n",
    "\n",
    "    # Train/test split\n",
    "    print(f\"\\nð Data Splitting:\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_binary, test_size=0.2, stratify=y_binary, random_state=42\n",
    "    )\n",
    "    print(f\"  Training set: {len(X_train)} samples\")\n",
    "    print(f\"  Test set: {len(X_test)} samples\")\n",
    "    print(f\"  Training class distribution: {dict(zip(*np.unique(y_train, return_counts=True)))}\")\n",
    "    print(f\"  Test class distribution: {dict(zip(*np.unique(y_test, return_counts=True)))}\")\n",
    "\n",
    "    # Train pipeline\n",
    "    print(f\"\\nðï¸ Training Pipeline:\")\n",
    "    pipeline = hybrid_best_logistic_pipeline(n_features=X.shape[1])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    print(f\"  â Pipeline training completed\")\n",
    "\n",
    "    # Make predictions on both training and test sets\n",
    "    print(f\"\\nð® Making Predictions:\")\n",
    "    y_train_pred = pipeline.predict(X_train)\n",
    "    y_test_pred = pipeline.predict(X_test)\n",
    "    print(f\"  â Predictions completed\")\n",
    "\n",
    "    # Cross-validation for additional validation\n",
    "    print(f\"\\nð Cross-Validation:\")\n",
    "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='balanced_accuracy')\n",
    "    print(f\"  CV Balanced Accuracy: {cv_scores.mean():.3f} (Â±{cv_scores.std()*2:.3f})\")\n",
    "\n",
    "    # Comprehensive evaluation\n",
    "    print(f\"\\nð RESULTS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Training metrics\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    train_balanced_accuracy = balanced_accuracy_score(y_train, y_train_pred)\n",
    "    \n",
    "    # Test metrics\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_balanced_accuracy = balanced_accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"ðï¸ TRAINING PERFORMANCE:\")\n",
    "    print(f\"  Training Accuracy:          {train_accuracy:.3f}\")\n",
    "    print(f\"  Training Balanced Accuracy: {train_balanced_accuracy:.3f}\")\n",
    "    \n",
    "    print(f\"\\nð¯ TEST PERFORMANCE:\")\n",
    "    print(f\"  Test Accuracy:              {test_accuracy:.3f}\")\n",
    "    print(f\"  Test Balanced Accuracy:     {test_balanced_accuracy:.3f}\")\n",
    "    \n",
    "    print(f\"\\nð PERFORMANCE COMPARISON:\")\n",
    "    print(f\"  Accuracy Gap:               {train_accuracy - test_accuracy:.3f}\")\n",
    "    print(f\"  Balanced Accuracy Gap:      {train_balanced_accuracy - test_balanced_accuracy:.3f}\")\n",
    "    \n",
    "    # Check for overfitting\n",
    "    if train_accuracy - test_accuracy > 0.1:\n",
    "        print(f\"  â ï¸  WARNING: Potential overfitting detected!\")\n",
    "    else:\n",
    "        print(f\"  â Good generalization performance\")\n",
    "\n",
    "    print(f\"\\nð DETAILED TEST CLASSIFICATION REPORT:\")\n",
    "    print(classification_report(y_test, y_test_pred, target_names=class_names))\n",
    "\n",
    "    print(f\"\\nð¯ TEST CONFUSION MATRIX:\")\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    print(f\"              Predicted\")\n",
    "    print(f\"              {class_names[0]:>4s}  {class_names[1]:>4s}\")\n",
    "    print(f\"Actual   {class_names[0]:>4s}  {cm[0,0]:4d}  {cm[0,1]:4d}\")\n",
    "    print(f\"         {class_names[1]:>4s}  {cm[1,0]:4d}  {cm[1,1]:4d}\")\n",
    "\n",
    "    print(f\"\\nð ANALYSIS COMPLETE!\")\n",
    "    print(f\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "350d5252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ð ROBUST NEUROIMAGING CLASSIFICATION - ANTI-OVERFITTING\n",
      "=================================================================\n",
      "\n",
      "ð Loading Data:\n",
      "Loading: ABIDE1\n",
      "  ABIDE1: 867 subjects, 800 features\n",
      "Loading: IP\n",
      "  IP: 35 subjects, 800 features\n",
      "Loading: BNI\n",
      "  BNI: 56 subjects, 800 features\n",
      "Loading: NYU1\n",
      "  NYU1: 46 subjects, 800 features\n",
      "Loading: NYU2\n",
      "  NYU2: 15 subjects, 800 features\n",
      "Loading: SDSU\n",
      "  SDSU: 51 subjects, 800 features\n",
      "\n",
      "ð Dataset Summary:\n",
      "  Total samples: 1070\n",
      "  Total sites: 6\n",
      "  Class distribution: {np.int64(1): np.int64(513), np.int64(2): np.int64(557)}\n",
      "  Label mapping: {'1': 0, '2': 1}\n",
      "ð§ Applying site harmonization...\n",
      "[neuroCombat] Creating design matrix\n",
      "[neuroCombat] Standardizing data across features\n",
      "[neuroCombat] Fitting L/S model and finding priors\n",
      "[neuroCombat] Finding parametric adjustments\n",
      "[neuroCombat] Final adjustment of data\n",
      "  â Site harmonization completed\n",
      "  â ï¸ Site harmonization failed: 'dict' object has no attribute 'T'\n",
      "ð§ Robust Feature Preprocessing:\n",
      "  Original features: 800\n",
      "  After variance filtering: 800\n",
      "  After aggressive correlation filtering: 396 (removed 404)\n",
      "\n",
      "ð Data Splitting:\n",
      "  Training set: 856 samples\n",
      "  Test set: 214 samples\n",
      "  Training class distribution: {np.int64(0): np.int64(410), np.int64(1): np.int64(446)}\n",
      "  Test class distribution: {np.int64(0): np.int64(103), np.int64(1): np.int64(111)}\n",
      "\n",
      "ðï¸ Creating Simple Models:\n",
      "  Created 5 simple models\n",
      "\n",
      "ð¯ Robust Model Selection:\n",
      "ð§ Robust Model Selection with Nested CV:\n",
      "  Evaluating lr_heavy...\n",
      "    lr_heavy: 0.599 (Â±0.012)\n",
      "  Evaluating ridge...\n",
      "    ridge: 0.606 (Â±0.023)\n",
      "  Evaluating rf_small...\n",
      "    rf_small: 0.605 (Â±0.035)\n",
      "  Evaluating bagged_lr...\n",
      "    bagged_lr: 0.601 (Â±0.021)\n",
      "  Evaluating pca_lr...\n",
      "    pca_lr: 0.630 (Â±0.024)\n",
      "  â Best model: pca_lr (0.630)\n",
      "\n",
      "ð¤ Creating Simple Ensemble:\n",
      "ð§ Creating Simple Ensemble (top 3 models):\n",
      "  Selected models:\n",
      "    pca_lr: 0.630\n",
      "    ridge: 0.606\n",
      "    rf_small: 0.605\n",
      "\n",
      "ð§ Training Best Individual Model (pca_lr):\n",
      "\n",
      "ð® Making Predictions:\n",
      "  â Predictions completed\n",
      "\n",
      "ð ROBUST RESULTS SUMMARY\n",
      "=================================================================\n",
      "ð BEST INDIVIDUAL MODEL (pca_lr):\n",
      "  Training Accuracy:          0.699\n",
      "  Test Balanced Accuracy:     0.635\n",
      "  Generalization Gap:         0.064\n",
      "\n",
      "ð¤ SIMPLE ENSEMBLE:\n",
      "  Training Accuracy:          0.796\n",
      "  Test Balanced Accuracy:     0.640\n",
      "  Generalization Gap:         0.156\n",
      "\n",
      "ð¯ FINAL RESULTS (Simple Ensemble):\n",
      "  Final Test Accuracy:        0.640\n",
      "  Final Test Balanced Acc:    0.640\n",
      "\n",
      "ð IMPROVEMENT ANALYSIS:\n",
      "  Baseline Accuracy:          0.640\n",
      "  Robust Pipeline Accuracy:   0.640\n",
      "  Absolute Improvement:       -0.000 (-0.1%)\n",
      "\n",
      "ð FINAL CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.63      0.62      0.62       103\n",
      "           2       0.65      0.66      0.65       111\n",
      "\n",
      "    accuracy                           0.64       214\n",
      "   macro avg       0.64      0.64      0.64       214\n",
      "weighted avg       0.64      0.64      0.64       214\n",
      "\n",
      "\n",
      "ð¯ FINAL CONFUSION MATRIX:\n",
      "              Predicted\n",
      "                 1     2\n",
      "Actual      1    64    39\n",
      "            2    38    73\n",
      "\n",
      "ð ROBUST ANALYSIS COMPLETE!\n",
      "=================================================================\n",
      "ð Target Range: 70-78% accuracy\n",
      "ð¯ Achieved: 64.0% balanced accuracy\n",
      "ð Challenging dataset - consider domain-specific approaches\n",
      "\n",
      "ð¡ KEY INSIGHTS:\n",
      "  â¢ Best individual model: pca_lr\n",
      "  â¢ Ensemble vs Individual: Simple Ensemble performed better\n",
      "  â¢ Robust approach prevents overfitting\n",
      "  â¢ Neuroimaging classification is inherently challenging\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, SelectPercentile, RFE\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    from neuroCombat import neuroCombat\n",
    "    NEUROCOMBAT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    NEUROCOMBAT_AVAILABLE = False\n",
    "    print(\"â ï¸ neuroCombat not available. Install with: pip install neuroCombat\")\n",
    "\n",
    "# ========== Enhanced Data Loading Function ==========\n",
    "def load_combined_sdi_data(sites):\n",
    "    X_combined, y_combined, site_labels = [], [], []\n",
    "    site_counter = 0\n",
    "    \n",
    "    for site, paths in sites.items():\n",
    "        print(f\"Loading: {site}\")\n",
    "        try:\n",
    "            df_fc = pd.read_csv(paths['fc_sdi'])\n",
    "            df_fc.columns = [c.lower().strip() for c in df_fc.columns]\n",
    "            id_fc = next((col for col in df_fc.columns if col in ['subject_id', 'sub_id', 'patientid']), None)\n",
    "            df_fc.rename(columns={id_fc: 'subject_id'}, inplace=True)\n",
    "            df_fc['subject_id'] = df_fc['subject_id'].astype(str)\n",
    "            \n",
    "            if 'sc_sdi' in paths and os.path.exists(paths['sc_sdi']):\n",
    "                df_sc = pd.read_csv(paths['sc_sdi'])\n",
    "                df_sc.columns = [c.lower().strip() for c in df_sc.columns]\n",
    "                id_sc = next((col for col in df_sc.columns if col in ['subject_id', 'sub_id', 'patientid']), None)\n",
    "                df_sc.rename(columns={id_sc: 'subject_id'}, inplace=True)\n",
    "                df_sc['subject_id'] = df_sc['subject_id'].astype(str)\n",
    "                df = pd.merge(df_sc, df_fc, on='subject_id', suffixes=('_sc', '_fc'))\n",
    "            else:\n",
    "                fc_features = df_fc.drop(columns=['subject_id']).select_dtypes(include=[np.number])\n",
    "                dummy_sc = pd.DataFrame(0, index=fc_features.index, \n",
    "                                      columns=[f'dummy_sc_{i}' for i in range(fc_features.shape[1])])\n",
    "                df = pd.concat([dummy_sc, fc_features], axis=1)\n",
    "                df['subject_id'] = df_fc['subject_id']\n",
    "            \n",
    "            df_pheno = pd.read_csv(paths['phenotype'])\n",
    "            df_pheno.columns = [c.lower().strip() for c in df_pheno.columns]\n",
    "            id_pheno = next((col for col in df_pheno.columns if col in ['subject_id', 'sub_id', 'patientid']), None)\n",
    "            df_pheno.rename(columns={id_pheno: 'subject_id'}, inplace=True)\n",
    "            df_pheno['subject_id'] = df_pheno['subject_id'].astype(str)\n",
    "            df_pheno = df_pheno[['subject_id', 'dx_group']].dropna().drop_duplicates()\n",
    "            \n",
    "            df = pd.merge(df, df_pheno, on='subject_id')\n",
    "            features = df.drop(columns=['subject_id', 'dx_group']).select_dtypes(include=[np.number])\n",
    "            features = features.fillna(features.median())\n",
    "            X_combined.append(features.values)\n",
    "            y_combined.append(df['dx_group'].values)\n",
    "            \n",
    "            # Track site labels for harmonization\n",
    "            site_labels.extend([site_counter] * len(df))\n",
    "            site_counter += 1\n",
    "            \n",
    "            print(f\"  {site}: {len(df)} subjects, {features.shape[1]} features\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {site}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return np.vstack(X_combined), np.concatenate(y_combined), np.array(site_labels)\n",
    "\n",
    "# ========== Site Harmonization ==========\n",
    "def harmonize_data(X, site_labels):\n",
    "    \"\"\"Apply site harmonization to reduce scanner/site effects\"\"\"\n",
    "    if not NEUROCOMBAT_AVAILABLE:\n",
    "        print(\"â ï¸ Skipping site harmonization - neuroCombat not available\")\n",
    "        return X\n",
    "    \n",
    "    try:\n",
    "        print(\"ð§ Applying site harmonization...\")\n",
    "        data_for_combat = X.T\n",
    "        covars_df = pd.DataFrame({'site': site_labels})\n",
    "        \n",
    "        harmonized_data = neuroCombat(dat=data_for_combat, \n",
    "                                    covars=covars_df, \n",
    "                                    batch_col='site')\n",
    "        \n",
    "        print(\"  â Site harmonization completed\")\n",
    "        return harmonized_data.T\n",
    "    except Exception as e:\n",
    "        print(f\"  â ï¸ Site harmonization failed: {e}\")\n",
    "        return X\n",
    "\n",
    "# ========== Robust Feature Preprocessing ==========\n",
    "def robust_feature_preprocessing(X, y=None):\n",
    "    \"\"\"Ultra-conservative feature preprocessing to prevent overfitting\"\"\"\n",
    "    print(f\"ð§ Robust Feature Preprocessing:\")\n",
    "    print(f\"  Original features: {X.shape[1]}\")\n",
    "    \n",
    "    # Remove features with very low variance\n",
    "    feature_vars = np.var(X, axis=0)\n",
    "    valid_features = feature_vars > 1e-4\n",
    "    X = X[:, valid_features]\n",
    "    print(f\"  After variance filtering: {X.shape[1]}\")\n",
    "    \n",
    "    # Very aggressive correlation removal\n",
    "    corr_matrix = np.corrcoef(X.T)\n",
    "    np.fill_diagonal(corr_matrix, 0)\n",
    "    \n",
    "    high_corr_pairs = np.where(np.abs(corr_matrix) > 0.85)  # Very aggressive\n",
    "    to_remove = set()\n",
    "    \n",
    "    for i, j in zip(high_corr_pairs[0], high_corr_pairs[1]):\n",
    "        if i < j:\n",
    "            if feature_vars[i] < feature_vars[j]:\n",
    "                to_remove.add(i)\n",
    "            else:\n",
    "                to_remove.add(j)\n",
    "    \n",
    "    keep_indices = [i for i in range(X.shape[1]) if i not in to_remove]\n",
    "    X_final = X[:, keep_indices]\n",
    "    print(f\"  After aggressive correlation filtering: {X_final.shape[1]} (removed {len(to_remove)})\")\n",
    "    \n",
    "    return X_final\n",
    "\n",
    "# ========== Create Multiple Simple Models ==========\n",
    "def create_simple_models():\n",
    "    \"\"\"Create multiple simple, well-regularized models\"\"\"\n",
    "    \n",
    "    models = {}\n",
    "    \n",
    "    # Model 1: Heavily regularized LogisticRegression\n",
    "    models['lr_heavy'] = Pipeline([\n",
    "        ('scaler', RobustScaler()),\n",
    "        ('feature_select', SelectPercentile(f_classif, percentile=20)),  # Only top 20%\n",
    "        ('classifier', LogisticRegression(C=0.01, max_iter=1000, solver='liblinear',\n",
    "                                        class_weight='balanced', random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Model 2: Ridge Classifier (L2 regularization)\n",
    "    models['ridge'] = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('feature_select', SelectKBest(f_classif, k=100)),  # Very few features\n",
    "        ('classifier', RidgeClassifier(alpha=10.0, class_weight='balanced', random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Model 3: Small Random Forest\n",
    "    models['rf_small'] = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('feature_select', SelectKBest(f_classif, k=150)),\n",
    "        ('classifier', RandomForestClassifier(n_estimators=50, max_depth=5,\n",
    "                                            min_samples_split=20, min_samples_leaf=10,\n",
    "                                            class_weight='balanced', random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Model 4: Bagged simple model\n",
    "    base_lr = LogisticRegression(C=0.1, solver='liblinear', class_weight='balanced')\n",
    "    models['bagged_lr'] = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('feature_select', SelectKBest(f_classif, k=80)),\n",
    "        ('classifier', BaggingClassifier(base_lr, n_estimators=20, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Model 5: PCA + Simple LR\n",
    "    models['pca_lr'] = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=50, random_state=42)),  # Very low dimensional\n",
    "        ('classifier', LogisticRegression(C=1.0, solver='liblinear',\n",
    "                                        class_weight='balanced', random_state=42))\n",
    "    ])\n",
    "    \n",
    "    return models\n",
    "\n",
    "# ========== Robust Model Selection with Nested CV ==========\n",
    "def robust_model_selection(models, X_train, y_train):\n",
    "    \"\"\"Use nested cross-validation to select best model and prevent overfitting\"\"\"\n",
    "    print(\"ð§ Robust Model Selection with Nested CV:\")\n",
    "    \n",
    "    outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    model_scores = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"  Evaluating {name}...\")\n",
    "        \n",
    "        # Nested CV: outer loop for unbiased performance estimate\n",
    "        outer_scores = []\n",
    "        \n",
    "        for train_idx, val_idx in outer_cv.split(X_train, y_train):\n",
    "            X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "            y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "            \n",
    "            # Clone model to avoid data leakage\n",
    "            model_clone = clone(model)\n",
    "            \n",
    "            try:\n",
    "                model_clone.fit(X_train_fold, y_train_fold)\n",
    "                val_pred = model_clone.predict(X_val_fold)\n",
    "                val_score = balanced_accuracy_score(y_val_fold, val_pred)\n",
    "                outer_scores.append(val_score)\n",
    "            except Exception as e:\n",
    "                print(f\"    {name} failed on fold: {e}\")\n",
    "                outer_scores.append(0.0)\n",
    "        \n",
    "        mean_score = np.mean(outer_scores)\n",
    "        std_score = np.std(outer_scores)\n",
    "        model_scores[name] = {'mean': mean_score, 'std': std_score, 'scores': outer_scores}\n",
    "        \n",
    "        print(f\"    {name}: {mean_score:.3f} (Â±{std_score:.3f})\")\n",
    "    \n",
    "    # Select best model\n",
    "    best_model_name = max(model_scores.keys(), key=lambda x: model_scores[x]['mean'])\n",
    "    best_model = models[best_model_name]\n",
    "    \n",
    "    print(f\"  â Best model: {best_model_name} ({model_scores[best_model_name]['mean']:.3f})\")\n",
    "    \n",
    "    return best_model, best_model_name, model_scores\n",
    "\n",
    "# ========== Simple Ensemble of Top Models ==========\n",
    "def create_simple_ensemble(models, model_scores, X_train, y_train, top_k=3):\n",
    "    \"\"\"Create a simple ensemble of top-k models\"\"\"\n",
    "    print(f\"ð§ Creating Simple Ensemble (top {top_k} models):\")\n",
    "    \n",
    "    # Sort models by performance\n",
    "    sorted_models = sorted(model_scores.items(), key=lambda x: x[1]['mean'], reverse=True)\n",
    "    top_models = sorted_models[:top_k]\n",
    "    \n",
    "    print(\"  Selected models:\")\n",
    "    for name, score_info in top_models:\n",
    "        print(f\"    {name}: {score_info['mean']:.3f}\")\n",
    "    \n",
    "    # Train selected models\n",
    "    trained_models = []\n",
    "    for name, _ in top_models:\n",
    "        model_clone = clone(models[name])\n",
    "        model_clone.fit(X_train, y_train)\n",
    "        trained_models.append((name, model_clone))\n",
    "    \n",
    "    return trained_models\n",
    "\n",
    "def ensemble_predict(trained_models, X):\n",
    "    \"\"\"Simple voting ensemble prediction\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for name, model in trained_models:\n",
    "        try:\n",
    "            pred = model.predict(X)\n",
    "            predictions.append(pred)\n",
    "        except Exception as e:\n",
    "            print(f\"  â ï¸ {name} prediction failed: {e}\")\n",
    "            # Use majority class as fallback\n",
    "            pred = np.ones(len(X))  # Assuming class 1 is majority\n",
    "            predictions.append(pred)\n",
    "    \n",
    "    if not predictions:\n",
    "        return np.ones(len(X))  # Fallback to majority class\n",
    "    \n",
    "    # Majority voting\n",
    "    predictions = np.array(predictions)\n",
    "    ensemble_pred = np.round(np.mean(predictions, axis=0)).astype(int)\n",
    "    \n",
    "    return ensemble_pred\n",
    "\n",
    "# ========== Main Execution ==========\n",
    "if __name__ == \"__main__\":\n",
    "    site_paths = {\n",
    "        'ABIDE1': {\n",
    "            'fc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE1FC/sdi_informed_energy_normalized_abide1.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE1/Phenotypic_V1_0b_preprocessed1.csv'\n",
    "        },\n",
    "        'IP': {\n",
    "            'fc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_ip.csv',\n",
    "            'sc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_ip.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/IP_1_phenotypes.csv'\n",
    "        },\n",
    "        'BNI': {\n",
    "            'fc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_bni.csv',\n",
    "            'sc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_bni.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/BNI_1_phenotypes.csv'\n",
    "        },\n",
    "        'NYU1': {\n",
    "            'fc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_nyu1.csv',\n",
    "            'sc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_nyu1.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/NYU_1_phenotypes.csv'\n",
    "        },\n",
    "        'NYU2': {\n",
    "            'fc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_nyu2.csv',\n",
    "            'sc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_nyu2.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/NYU_2_phenotypes.csv'\n",
    "        },\n",
    "        'SDSU': {\n",
    "            'fc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2FC/sdi_informed_energy_normalized_sdsu.csv',\n",
    "            'sc_sdi': '/Users/arnavkarnik/Documents/Classification/results_ABIDE2SC/sdi_informed_energy_normalized_sdsu.csv',\n",
    "            'phenotype': '/Users/arnavkarnik/Documents/Classification/Phenotypes_ABIDE2/SDSU_1_phenotypes.csv'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(\"ð ROBUST NEUROIMAGING CLASSIFICATION - ANTI-OVERFITTING\")\n",
    "    print(\"=\"*65)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    print(\"\\nð Loading Data:\")\n",
    "    X, y, site_labels = load_combined_sdi_data(site_paths)\n",
    "    \n",
    "    print(f\"\\nð Dataset Summary:\")\n",
    "    print(f\"  Total samples: {len(X)}\")\n",
    "    print(f\"  Total sites: {len(np.unique(site_labels))}\")\n",
    "    print(f\"  Class distribution: {dict(zip(*np.unique(y, return_counts=True)))}\")\n",
    "    \n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_binary = label_encoder.fit_transform(y)\n",
    "    class_names = [str(name) for name in label_encoder.classes_]\n",
    "    print(f\"  Label mapping: {dict(zip(class_names, range(len(class_names))))}\")\n",
    "    \n",
    "    # Apply site harmonization\n",
    "    X = harmonize_data(X, site_labels)\n",
    "    \n",
    "    # Robust feature preprocessing  \n",
    "    X = robust_feature_preprocessing(X, y_binary)\n",
    "    \n",
    "    # Train/test split\n",
    "    print(f\"\\nð Data Splitting:\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_binary, test_size=0.2, stratify=y_binary, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"  Training set: {len(X_train)} samples\")\n",
    "    print(f\"  Test set: {len(X_test)} samples\")\n",
    "    print(f\"  Training class distribution: {dict(zip(*np.unique(y_train, return_counts=True)))}\")\n",
    "    print(f\"  Test class distribution: {dict(zip(*np.unique(y_test, return_counts=True)))}\")\n",
    "\n",
    "    # Create simple models\n",
    "    print(f\"\\nðï¸ Creating Simple Models:\")\n",
    "    simple_models = create_simple_models()\n",
    "    print(f\"  Created {len(simple_models)} simple models\")\n",
    "    \n",
    "    # Robust model selection\n",
    "    print(f\"\\nð¯ Robust Model Selection:\")\n",
    "    best_model, best_model_name, all_scores = robust_model_selection(simple_models, X_train, y_train)\n",
    "    \n",
    "    # Create simple ensemble\n",
    "    print(f\"\\nð¤ Creating Simple Ensemble:\")\n",
    "    trained_ensemble = create_simple_ensemble(simple_models, all_scores, X_train, y_train, top_k=3)\n",
    "    \n",
    "    # Train best individual model\n",
    "    print(f\"\\nð§ Training Best Individual Model ({best_model_name}):\")\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions with both approaches\n",
    "    print(f\"\\nð® Making Predictions:\")\n",
    "    \n",
    "    # Best individual model\n",
    "    y_train_pred_best = best_model.predict(X_train)\n",
    "    y_test_pred_best = best_model.predict(X_test)\n",
    "    \n",
    "    # Simple ensemble\n",
    "    y_train_pred_ensemble = ensemble_predict(trained_ensemble, X_train)\n",
    "    y_test_pred_ensemble = ensemble_predict(trained_ensemble, X_test)\n",
    "    \n",
    "    print(f\"  â Predictions completed\")\n",
    "\n",
    "    # Evaluation\n",
    "    print(f\"\\nð ROBUST RESULTS SUMMARY\")\n",
    "    print(\"=\"*65)\n",
    "    \n",
    "    # Individual model results\n",
    "    train_acc_best = accuracy_score(y_train, y_train_pred_best)\n",
    "    test_acc_best = balanced_accuracy_score(y_test, y_test_pred_best)\n",
    "    \n",
    "    print(f\"ð BEST INDIVIDUAL MODEL ({best_model_name}):\")\n",
    "    print(f\"  Training Accuracy:          {train_acc_best:.3f}\")\n",
    "    print(f\"  Test Balanced Accuracy:     {test_acc_best:.3f}\")\n",
    "    print(f\"  Generalization Gap:         {train_acc_best - test_acc_best:.3f}\")\n",
    "    \n",
    "    # Ensemble results\n",
    "    train_acc_ensemble = accuracy_score(y_train, y_train_pred_ensemble)\n",
    "    test_acc_ensemble = balanced_accuracy_score(y_test, y_test_pred_ensemble)\n",
    "    \n",
    "    print(f\"\\nð¤ SIMPLE ENSEMBLE:\")\n",
    "    print(f\"  Training Accuracy:          {train_acc_ensemble:.3f}\")\n",
    "    print(f\"  Test Balanced Accuracy:     {test_acc_ensemble:.3f}\")\n",
    "    print(f\"  Generalization Gap:         {train_acc_ensemble - test_acc_ensemble:.3f}\")\n",
    "    \n",
    "    # Select best approach\n",
    "    if test_acc_ensemble > test_acc_best:\n",
    "        final_test_acc = test_acc_ensemble\n",
    "        final_approach = \"Simple Ensemble\"\n",
    "        final_predictions = y_test_pred_ensemble\n",
    "    else:\n",
    "        final_test_acc = test_acc_best\n",
    "        final_approach = f\"Individual ({best_model_name})\"\n",
    "        final_predictions = y_test_pred_best\n",
    "    \n",
    "    print(f\"\\nð¯ FINAL RESULTS ({final_approach}):\")\n",
    "    print(f\"  Final Test Accuracy:        {accuracy_score(y_test, final_predictions):.3f}\")\n",
    "    print(f\"  Final Test Balanced Acc:    {final_test_acc:.3f}\")\n",
    "    \n",
    "    # Performance vs baseline\n",
    "    baseline_accuracy = 0.640\n",
    "    improvement = final_test_acc - baseline_accuracy\n",
    "    print(f\"\\nð IMPROVEMENT ANALYSIS:\")\n",
    "    print(f\"  Baseline Accuracy:          {baseline_accuracy:.3f}\")\n",
    "    print(f\"  Robust Pipeline Accuracy:   {final_test_acc:.3f}\")\n",
    "    print(f\"  Absolute Improvement:       {improvement:+.3f} ({improvement/baseline_accuracy*100:+.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nð FINAL CLASSIFICATION REPORT:\")\n",
    "    print(classification_report(y_test, final_predictions, target_names=class_names))\n",
    "\n",
    "    print(f\"\\nð¯ FINAL CONFUSION MATRIX:\")\n",
    "    cm = confusion_matrix(y_test, final_predictions)\n",
    "    print(f\"              Predicted\")\n",
    "    print(f\"              {class_names[0]:>4s}  {class_names[1]:>4s}\")\n",
    "    print(f\"Actual   {class_names[0]:>4s}  {cm[0,0]:4d}  {cm[0,1]:4d}\")\n",
    "    print(f\"         {class_names[1]:>4s}  {cm[1,0]:4d}  {cm[1,1]:4d}\")\n",
    "\n",
    "    print(f\"\\nð ROBUST ANALYSIS COMPLETE!\")\n",
    "    print(\"=\"*65)\n",
    "    print(f\"ð Target Range: 70-78% accuracy\")\n",
    "    print(f\"ð¯ Achieved: {final_test_acc:.1%} balanced accuracy\")\n",
    "    \n",
    "    if final_test_acc >= 0.70:\n",
    "        print(f\"ð EXCELLENT! Competitive performance achieved!\")\n",
    "    elif final_test_acc >= 0.65:\n",
    "        print(f\"ð STRONG performance - approaching competitive range!\")\n",
    "    elif final_test_acc > baseline_accuracy:\n",
    "        print(f\"ð IMPROVED performance over baseline!\")\n",
    "    else:\n",
    "        print(f\"ð Challenging dataset - consider domain-specific approaches\")\n",
    "    \n",
    "    print(f\"\\nð¡ KEY INSIGHTS:\")\n",
    "    print(f\"  â¢ Best individual model: {best_model_name}\")\n",
    "    print(f\"  â¢ Ensemble vs Individual: {final_approach} performed better\")\n",
    "    print(f\"  â¢ Robust approach prevents overfitting\")\n",
    "    print(f\"  â¢ Neuroimaging classification is inherently challenging\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
